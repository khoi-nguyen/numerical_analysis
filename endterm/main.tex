\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage[outputdir=build,newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm}
\usepackage{mathtools}
\theoremstyle{definition}
\newtheorem{question}{Question}
\input{../macros.tex}
\begin{document}

\title{Numerical Analysis: Final exam (60 marks)}
\author{Urbain Vaes}
\maketitle

\begin{question}
    [8 marks]
    True or false?
    \begin{enumerate}
        \item
            Let $(\placeholder)_2$ denote base 3 representation.
            It holds that
            \[
                (120)_3 + (211)_3 = (1001)_3.
            \]
        \item
            Let $(\placeholder)_2$ denote binary representation.
            It holds that
            \[
                (1000)_2 \times (0.1\overline{01})_2 = (101.\overline{01})_2.
            \]

        \item In Julia, \julia{Float64(.25) == Float32(.25)} evaluates to \julia{true}.

        \item The spacing (in absolute value) between successive double-precision (\julia{Float64}) floating point numbers is constant.

        \item
            The machine epsilon is the smallest strictly positive number that can be represented in a floating point format.

        \item
            The following equality holds
            \[
                (0.\overline{101})_2 = \frac{7}{3}.
            \]

        \item
            Let $\floating_{64} \subset \real$ denote the set of double-precision floating point numbers.
            If $x \in \floating_{64}$, then the binary representation of~$x$ is finite,
            and so is the decimal representation of~$x$.

        \item
            In Julia, \julia{256.0 + 2.0*eps(Float64) == 16.0} evaluates to \julia{true}.

        \item
            The set~$\floating_{64}$ of double-precision floating point numbers contains twice as many real numbers
            as the set~$\floating_{32}$ of single-precision floating point numbers.
        % \item
        %     The computational cost of calculating the~$\mat L \mat U$ decomposition of a square nonsingular $\real^{n \times n}$ matrix scales as
        %     $\mathcal O(n^3)$ when $n \to \infty$,

        % \item
        %     The following Julia code never terminates:
        %     \begin{minted}{julia}
        % iter = 0
        % old_result = 0.0
        % new_result = 1.0
        % while new_result != old_result
        %     old_result = new_result
        %     new_result = old_result + 2^(-iter)
        %     iter += 1
        % end
        %     \end{minted}

    \end{enumerate}
    A correct (resp.~incorrect) answer leads to +1 mark (resp. -1 mark).
\end{question}

\newpage
\begin{question}
    Assume that $\mat A \in \real^{n \times n}$ is a nonsingular matrix and that $\vect b \in \real^n$.
    We wish to solve the linear system
    \[
        \mat A \vect x = \vect b
    \]
    using an iterative method where each iteration is of the form
    \[
        \mat M \vect x_{k+1} = \mat N \vect x_k + \vect b.
    \]
    Here $\mat A = \mat M - \mat N$ is a splitting of~$\mat A$ such that $\mat M$ is nonsingular,
    and $\vect x_k \in \real^n$ denotes the $k$-th iterate of the numerical scheme.

    \begin{enumerate}
        \item
            Let $\vect e_k := \vect x_k - \vect x$.
            Prove that
            \[
                \vect e_{k+1} = \mat M^{-1} \mat N \vect e_k.
            \]

        \item
            Let $L = \norm{\mat M^{-1} \mat N}_1$.
            Prove that
            \[
                \forall k \in \nat, \qquad
                \norm{\vect e_k}_1 \leq L^k \norm{\vect e_0}_1.
            \]

        \item
            Is the condition $\norm{\mat M^{-1} \mat N}_1 < 1$ necessary
            for convergence when $\vect e_0 \neq \vect 0$?

        \item
            Assume that $\mat A$ is column diagonally dominant, in the sense that
            \[
                \forall j \in \{1, \dotsc, n\}, \qquad
                \lvert a_{jj} \rvert \geq \sum_{i=1, i\neq j}^{n} \lvert a_{ij} \rvert.
            \]
            Show that in this case $\norm{\mat M^{-1} \mat N}_1 < 1$ for the Jacobi method,
            i.e.\ when $\mat M$ is the diagonal of~$\mat A$.
            You may take for granted the following expression for the 1-norm of a matrix:
            \[
                \norm{\mat A}_1 = \max_{1 \leq j \leq n} \sum_{i=1}^{m} \abs{a_{ij}}.
            \]
    \end{enumerate}
\end{question}

\newpage
\begin{question}
    Assume that $\vect x_*$ is a solution to the equation
    \[
        \vect F(\vect x) = \vect x
    \]
    and that $\vect F\colon \real^n \to \real^n$ satisfies the local Lipschitz condition
    \begin{equation}
        \label{eq:local_lipschitz}
        \forall \vect x \in B_{\delta}(\vect x_*), \qquad
        \norm[big]{\vect F(\vect x) - \vect F(\vect x_*)} \leq L \norm{\vect x - \vect x_*},
    \end{equation}
    with $0 \leq L < 1$ and $\delta > 0$.
    We consider the following fixed-point iterative method for approximating~$\vect x_*$:
    \begin{equation}
        \label{eq:fixed_point}
        \vect x_{k+1} = \vect F(\vect x_k).
    \end{equation}
    Show that if $\vect x_0 \in B_{\delta}(\vect x_*)$,
    then the following statements hold:
    \begin{itemize}
        \item All the iterates $(\vect x_k)_{k \in \nat}$ belong to $B_{\delta}(\vect x_*)$.
        \item The sequence $(\vect x_k)_{k \in \nat}$ converges exponentially to $\vect x_*$.
        \item There is no other fixed point of $\vect F$ inside $B_{\delta}(\vect x_*)$.
    \end{itemize}
    Explain with an example how the iterative scheme~\eqref{eq:fixed_point} can be employed for solving a nonlinear equation of the form
    \[
        \vect f(\vect x) = \vect 0.
    \]
\end{question}

% \newpage
% \begin{question}
%     Show that if $\mat A \in \real^{n \times n}$ is nonsingular,
%     then the solution to the equation $\mat A \vect x = \vect b$ belongs to the Krylov subspace
%     \[
%         \mathcal K_n(\mat A, \vect b)
%         = \Span \Bigl\{ \vect b, \mat A \vect b, \mat A^2 \vect b, \dotsc, \mat A^{n-1} \vect b \Bigr\}.
%     \]
%     You can take for granted that $\mathcal K_n(\mat A, \vect b)$ is an invariant subspace of~$\mat A$.
% \end{question}


\newpage
\begin{question}
    [Error estimate for eigenvalue problem]
    Let $\norm{\placeholder}$ denote the Euclidean norm,
    and assume that~$\mat A \in \complex^{n \times n}$ is Hermitian and nonsingular.

    \begin{itemize}
        \item
            Describe a simple numerical method for calculating the eigenvalue of $\mat A$ of smallest magnitude,
            as well as the corresponding eigenvector.


        \item
            Assume that $\widehat \lambda \in \complex$ and $\widehat{\vect v} \in \complex^n$ are such that
            \[
                \norm{\mat A \widehat{\vect v} - \widehat{\lambda} \widehat{\vect v}} = \varepsilon \norm{\widehat {\vect v}}.
            \]
            Prove that there exists an eigenvalue~$\lambda$ of~$\mat A$ such that
            \[
                \abs{\lambda - \widehat \lambda} \leq \varepsilon.
            \]
    \end{itemize}

\end{question}

\newpage
\begin{question}
    [Interpolation error]
    Let~$u$ denote the function
    \begin{align*}
        u\colon
        &[0, 1] \to \real; \\
        &x \mapsto \sin(x).
    \end{align*}
    Let $p_n \colon [0, 2 \pi] \to \real$ denote the interpolating polynomial of~$u$ through the nodes
    \[
        x_i = \frac{2 \pi i}{n}, \qquad 0, \dotsc, n.
    \]
    \begin{itemize}
        \item
            Using a method of your choice,
            calculate $p_n$ in the particular case where $n = 3$.
        \item
            Prove that
            \[
                E_n := \sup_{x \in [0, 1]} \abs{\widehat u(x) - u(x)}
                \leq \frac{1}{(n+1)!} \prod_{i=0}^{n} (x - x_i).
            \]
            Does the error $E_n$ tend to zero as $n \to \infty$?
    \end{itemize}
\end{question}

\newpage
\begin{question}
    [Numerical integration]
\end{question}

\end{document}
