\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage[outputdir=build,newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage[colorlinks=true]{hyperref}
\usepackage[capitalize,nameinlink]{cleveref}
\usepackage{amssymb}
\setlist[enumerate]{font=\bfseries}
\theoremstyle{definition}
\newtheorem{question}{Question}
\theoremstyle{remark}
\newtheorem*{solution}{Solution}
\input{../macros.tex}
% \usepackage{tikz-cd}
% \usetikzlibrary{graphs,graphdrawing}
% \usetikzlibrary{backgrounds}

\theoremstyle{plain}% default
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\crefname{figure}{Figure}{Figures}

\begin{document}

\title{Numerical Analysis: Final Exam \\
\small{(\textbf{50 marks}, only the 5 best questions count)}}
\author{Urbain Vaes}
\date{12 May 2022}
\maketitle

\noindent You are allowed to use a calculator, but not \emph{Julia} or \emph{Python}.

\section*{Academic integrity pledge}
\noindent $\square$ \textcolor{blue}{I certify that I will not give or receive any unauthorized help on this exam,
and that all work will be my own. (Tick $\checkmark$ or copy the sentence on your answer sheet)}.


\newpage
\begin{question}
    [Floating point arithmetic, \textbf{10 marks}]
    True or false? +1/0/-1
    \begin{enumerate}
        \item
            Let $(\placeholder)_3$ denote base 3 representation.
            It holds that
            \[
                (222,222)_3 + (1)_3 = (1,000,000)_3.
            \]

        \item
            Let $(\placeholder)_2$ denote base 2 representation.
            It holds that
            \[
                3 \times (0.0101)_2 = (0.1111)_2.
            \]

        \item
            The following equality holds
            \[
                (0.\overline{011})_2 = \frac{3}{4}.
            \]

        \item
            The number $x = (d_1 d_2 d_3)_3$ for $d_1, d_2, d_3 \in \{0, 1, 2\}$ is a multiple of~$3$ if and only if~$d_3 = 0$.

        \item
            In Julia, \julia{Float64(0.375) == Float32(0.375)} evaluates to \julia{true}.

        \item
            The value of the machine epsilon is the same for the single precision~($\floating_{32}$) and the double precision ($\floating_{64}$) formats.

        \item
            The spacing (in absolute value) between successive double-precision (\julia{Float64}) floating point numbers is equal to the machine epsilon.

        \item
            All the natural numbers can be represented exactly in the double precision floating point format~$\floating_{64}$.

        \item
            Machine addition in the $\floating_{64}$ format is associative but not commutative.

        \item
             In Julia \julia{exp(eps()) == 1 + eps()} evaluates to \julia{true}.
             (Remember that, by default, rounding is to the nearest representable number).

        \item
             In Julia \julia{sqrt(1 + eps()) == 1 + eps()} evaluates to \julia{true}.

        \item
            Let $x$ and $y$ be two numbers in $\floating_{64}$.
            The result of the machine multiplication $x \mtimes y$ is sometimes exact and sometimes not,
            depending on the values of $x$ and $y$.

        \item
            In Julia, let \julia{f(x) = (x == x/100.0) ? x : f(x/100.0)}
            \footnote{%
                In Python, let \mintinline{python}{f = lambda x: x if x == x/100.0 else f(x/100.0)}
            }.
            Then \julia{f(3.0)} returns \julia{0.0}.


        % \item
        %     In a fixed point format with~$p$ bits, the set of real numbers that can be represented is of the form
        %     \begin{align}
        %         \Bigl\{ i \delta: i \in \nat \text{ and } -2^{p-1} \leq i \leq 2^{p-1} + 1 \Bigr\},
        %     \end{align}
        %     for some parameter $\delta \in \real$.
        %     In such a format, the absolute spacing between successive numbers is constant.

    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Interpolation, \textbf{10 marks}]
    Let~$u\colon [-1, 1] \to \real$ be given by
    \begin{align*}
        u(x) = x^3.
    \end{align*}
    Let $p \colon [-1, 1] \to \real$ denote the interpolating polynomial of~$u$ at nodes $x_0 < x_1 < x_2$,
    all contained in the interval $[-1, 1]$.
    \begin{enumerate}
        \itemsep0pt
        \item
            \mymarks{2}
            Let $e(x) := u(x) - p(x)$.
            Prove, without assuming any result shown in class,
            that the interpolation error satisfies
            \[
                \forall x \in [0, 1], \qquad
                e(x) = (x-x_0) (x-x_1) (x-x_2).
            \]

        \item
            \mymarks{2}
            Using a method of your choice,
            calculate the interpolating polynomial $p$ in the particular case where
            \begin{equation}
                \label{eq:particular_case}
                x_0 = -1, \qquad x_1 = 0, \qquad x_2 = 1.
            \end{equation}

        \item \mymarks{2}
            We denote the maximum absolute value of the error by
            \begin{equation}
                \label{eq:max_error}
                E := \max_{x \in [-1, 1]} \abs[\big]{e(x)}.
            \end{equation}
            Calculate the value of $E$ in the particular case~\eqref{eq:particular_case}.

        \item
            \mymarks{2}
            We denote by~$T_3 \colon [-1, 1] \to \real$ the Chebyshev polynomial given by
            \[
                T_3(x) := \cos\bigl(3 \arccos(x)\bigr).
            \]
            Show that
            \[
                T_3(x) = 4x^3 - 3x
            \]
            and calculate the roots~$z_0, z_1, z_2$ of $T_3$.

            \textbf{Hint:} Note that $\cos (3 \theta) = \Re \left( \e^{\i 3\theta} \right) = \Re \bigl( \left(\e^{\i \theta} \right)^3 \bigr)$, where $\e^{i \theta} = \cos (\theta) + \i \sin (\theta)$.

        \item
            \mymarks{2}
            Find the expression of the error~$e(x)$ and the maximum absolute error~$E$ given in~\eqref{eq:max_error}
            in the case where the interpolation nodes~$x_0, x_1, x_2$ are given by~$z_0, z_1, z_2$.

        \item
            *\mybonus{2}
            Show that the maximum absolute error~\eqref{eq:max_error},
            viewed as a function of the interpolation nodes~$x_1, x_2, x_3$,
            is minimized when $x_i = z_i$ for $i \in \{0, 1, 2\}$.

            \textbf{Hint:} Reason by contradiction and notice that
            \[
                \bigl\lvert T_3(y) \bigr\rvert = 1
                \qquad \text{ for $y \in \left\{-1, -\frac{1}{2}, \frac{1}{2}, 1 \right\}$  }.
            \]
    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Numerical integration, \textbf{10 marks}]
    Let $u \colon [0, 1] \to \real$ be a function we wish to integrate and
    \[
        I := \int_{0}^{1} u(x) \, \d x.
    \]
    \begin{enumerate}
        \item
        \mymarks{3}
        Consider the following integration rule:
        \begin{equation}
            \label{eq:newton_cotes}
            I\approx w_1 u(0) + w_2 u(1).
        \end{equation}
        Find the weights $w_1, w_2 \in \real$ so that this integration rule has the highest possible degree of precision.
        What is the degree of precision of the rule constructed?


        \item
        \mymarks{3}
            Let $x_i = i/n$ for $i = 0, \dotsc, n$.
            The composite trapezoidal rule is given by
            \begin{equation}
                \label{eq:composite_rule}
                I
                \approx \frac{1}{2n} \bigl( u(x_0) + 2 u(x_1) + 2 u(x_2) + \dotsb + 2 u(x_{n-2}) + 2 u(x_{n-1}) + u(x_n) \bigr) =: \widehat I_n.
            \end{equation}
            Explain how this rule can be obtained by applying a generalization of the integration rule~\eqref{eq:newton_cotes} in each interval $[x_i, x_{i+1}]$.
            % (more precisely
            % from a generalization of the rule~\eqref{eq:newton_cotes} to any interval $[x_i, x_{i+1}]$).

        \item
        \mymarks{3}
            Assume that $u \in C^2\bigl([0, 1]\bigr)$.
            Show that, for all $n \in \nat_{>0}$,
            \begin{equation}
                \label{eq:error_bound_trapezoidal}
                \bigl\lvert I - \widehat I_n \bigr\rvert
                \leq \frac{C_2}{12 n^2},
                \qquad
                C_2 := \sup_{\xi \in [0, 1]} \abs*{u''(\xi)}.
            \end{equation}
            You may use~\cref{proposition:interpolation_error} at the end of this document for the interpolation error.
        \item
        \mymark
        In this part of the question, we assume that $u$ is a quadratic polynomial.
        It is possible to show that, in this case,
        \[
             I - \widehat I_n = - \frac{u''(0)}{12 n^2}.
        \]
        Explain how, given two approximations $\widehat I_n$ and $\widehat I_{2n}$ obtained with~\eqref{eq:composite_rule},
        a better approximation of the integral~$I$ can be obtained by a linear combination of the form
        \[
            \alpha_1 \widehat I_n + \alpha_2 \widehat I_{2n}.
        \]

        \item
        *\mybonus{2}
        Instead of~\eqref{eq:newton_cotes},
        consider a more general integration rule of the form
        \begin{equation}
            \label{eq:gauss_legendre}
            \int_{0}^{1} u(x) \, \d x \approx w_1 u(x_1) + w_2 u(x_2).
        \end{equation}
        Find the weights $w_1, w_2 \in \real$ and the nodes $x_1, x_2 \in [0, 1]$ so that this integration rule has the highest possible degree of precision.
        What is the degree of precision obtained?

    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Iterative method for linear systems, \textbf{10 marks}]
    Assume that $\mat A \in \real^{n \times n}$ is a \emph{symmetric positive definite} matrix and that $\vect b \in \real^n$.
    We wish to solve the linear system
    \begin{equation}
        \label{eq:linear_system}
        \mat A \vect x = \vect b.
    \end{equation}
    To this end we consider an iterative method where each iteration is of the form
    \begin{equation}
        \label{eq:iterative_scheme}
        \mat M \vect x_{k+1} = \mat N \vect x_k + \vect b.
    \end{equation}
    Here $\mat A = \mat M - \mat N$ is a splitting of~$\mat A$ such that $\mat M$ is nonsingular,
    and $\vect x_k \in \real^n$ denotes the $k$-th iterate of the numerical scheme.

    \begin{enumerate}
        \itemsep0pt
        \item
            \mymarks{3}
            Let $\vect e_k := \vect x_k - \vect x_*$,
            where $\vect x_*$ is the exact solution to~\eqref{eq:linear_system}.
            Prove that
            \[
                \forall k \in \nat, \qquad
                \vect e_{k+1} = \mat M^{-1} \mat N \vect e_k.
            \]

        \item
            \mymarks{2}
            We denote by $\norm{\placeholder}_{\mat A}$ the vector norm
            \begin{equation}
                \norm{\vect x}_{\mat A} := \sqrt{\vect x^\t \mat A \vect x},
            \end{equation}
            and we use the same notation for the induced matrix norm.
            Prove that
            \begin{equation}
                \label{eq:inequality_interpolation}
                \forall k \in \nat, \qquad
                \norm{\vect e_k}_{\mat A} \leq L^k \norm{\vect e_0}_{\mat A},
                \qquad L := \norm{\mat M^{-1} \mat N}_{\mat A}.
            \end{equation}

        \item
            \mymark
            Is the condition~$\norm{\mat M^{-1} \mat N}_{\mat A} < 1$ sufficient to ensure convergence for all~$\vect x_0$?

        \item
            *\mymarks{3}
            Show that
            \begin{equation}
                \label{eq:norm}
                \norm{\mat M^{-1} \mat N \vect x}_{\mat A}^2
                = \norm{\vect x}_{\mat A}^2 - \vect y^\t (\mat M^\t + \mat N) \vect y,
                \qquad \vect y :=  \mat M^{-1} \mat A \vect x.
            \end{equation}

            \textbf{Hint: } Eliminate~$\mat N$ from both sides of the equation by rewriting~$\mat N = \mat M - \mat A$.
            Then substitute the expression of~$\vect y$ and expand both sides.
            Remember that a scalar quantity transposed is equal to itself.

        \item
            \mymark
            Show that, for the Gauss--Seidel method,
            i.e.\ when $\mat M = \mat L + \mat D$ contains just the lower triangular and diagonal parts of~$\mat A$,
            it holds that
            \begin{equation}
                \label{eq:gauss_seidel}
                \mat M^\t + \mat N = \mat D.
            \end{equation}

        \item
            \mybonus{2}
            Deduce from~\eqref{eq:norm} and~\eqref{eq:gauss_seidel} that,
            for the Gauss--Seidel method,
            \[
                \norm{\mat M^{-1} \mat N}_{\mat A} < 1.
            \]
    \end{enumerate}
\end{question}

\newpage
\begin{question}
    [Nonlinear equations, \textbf{10 marks}]
    We consider the following iterative method for calculating~$\sqrt[3]{2}$:
    \begin{equation}
        \label{eq:iteration}
        x_{k+1} = F(x_k) :=  \omega x_k + (1 - \omega) \frac{2}{x_k^2},
    \end{equation}
    with $\omega \in (0, 1)$ a fixed parameter.
    \begin{enumerate}
        \item
            \mymark
            Show that $x_* := \sqrt[3]{2}$ is a fixed point of the iteration~\eqref{eq:iteration}.

        \item
            \mymarks{2}
            Write down in pseudocode a computer program based on the iteration~\eqref{eq:iteration} for calculating~$\sqrt[3]{2}$.
            Use an appropriate stopping criterion that does not require to know the value of~$\sqrt[3]{2}$.

        \item
            \mymarks{2}
            Prove that if~$\omega \in \left(\frac{1}{3}, 1\right)$,
            then~$x_*$ is locally exponentially stable.
            You may take for granted~\cref{proposition:local_convergence} at the end of this document.

        \item
            \mymark
            Do you expect faster convergence of~\eqref{eq:iteration} with $\omega = \frac{1}{2}$ or with $\omega = \frac{2}{3}$?

        \item
            \mymarks{2}
            % Write down, in the form
            % \[
            %     x_{k+1} = \dots
            % \]
            Show that, in the particular case where $\omega = \frac{2}{3}$,
            the iterative scheme~\eqref{eq:iteration} coincides with the Newton--Raphson method applied to
            the nonlinear equation
            \begin{equation}
                \label{eq:nonlinear_equation}
                f(x) = 0,
            \end{equation}
            for an appropriate function $f \colon \real \to \real$.
            % Write explicitly the definition of~$f(x)$.

        \item
            \mymarks{2}
            Illustrate graphically a few iterations of the Newton--Raphson method for solving~\eqref{eq:nonlinear_equation} when starting from~$x_0 = 2$.
            You may either create your own figure or write on \cref{fig:nr} at the end of this document.

        \item
            *\mybonus{2}
            Prove~\cref{proposition:local_convergence} in the appendix.
            More precisely, show that the assumptions of the proposition imply that there is $\delta > 0$ and $L < 1$ such that the following local Lipschitz condition is satisfied:
            \begin{equation}
                \label{eq:local_lipschitz}
                \forall x \in [x_* - \delta, x_* + \delta],
                \qquad
                \lvert F(x) - F(x_*) \rvert \leq L \abs{x - x_*}.
            \end{equation}
            For completeness, one should then show that~\eqref{eq:local_lipschitz} is sufficient to guarantee local exponential stability,
            but this is taken for granted here; you do not need to prove this.
    \end{enumerate}
\end{question}

\newpage

\newpage

\newpage
\begin{question}
    [Iterative methods for eigenvalue problems, \textbf{10 marks}]
    Let $\norm{\placeholder}$ denote both the Euclidean norm on vectors and the induced matrix norm.
    Assume that~$\mat A \in \real^{n \times n}$ is symmetric and nonsingular,
    and that all the eigenvalues of~$\mat A$ have different moduli:
    \[
        \lvert \lambda_1 \rvert
        >
        \lvert \lambda_2 \rvert
        >
        \dots
        >
        \lvert \lambda_n \rvert.
    \]

    \begin{enumerate}
        \item
            \mymarks{5}
            Describe with words and pseudocode a simple numerical method for calculating the eigenvalue of $\mat A$ of smallest modulus
            as well as the corresponding eigenvector.

        \item
            \mymarks{2}
            Suppose that we have calculated the smallest eigenvalue in modulus $\lambda_n$,
            as well as the associated normalized eigenvector~$\vect v_n$.
            We let
            \[
                \mat B := \mat A^{-1} - \frac{1}{\lambda_n} \vect v_n \vect v_n^\t.
            \]
            % Write down an expression for the eigenvalues of~$\mat B$ in terms of the eigenvalues of~$\mat A$.
            If we apply the power iteration to this matrix,
            what convergence can we expect?
            Justify your answer.


        \item
            *\mymarks{3}
            The aim of this part is to provide an answer to the following question:
            given an approximate eigenpair $(\widehat {\vect v}, \widehat \lambda)$,
            what is the smallest perturbation~$\mat E$ that we need to apply to $\mat A$ in order to guarantee that
            $(\widehat {\vect v}, \widehat \lambda)$ is an exact eigenpair, i.e.\ that
            \[
                (\mat A + \mat E) \widehat {\vect v} = \widehat \lambda \widehat {\vect v} \,?
            \]
            Assume that $\widehat {\vect v}$ is normalized and
            let $\mathcal E = \Bigl\{\mat E \in \complex^{n \times n}: (\mat A + \mat E) \widehat {\vect v} = \widehat \lambda \widehat {\vect v} \Bigr\}$.
            Prove that
            \begin{equation}
                \label{eq:kahan_parlett_jiang}
                \min_{\mat E \in \mathcal E} \norm{\mat E} = \norm{\vect r}, \qquad \vect r := \mat A \widehat {\vect v} - \widehat \lambda \widehat {\vect v}.
            \end{equation}
            \textbf{Hint:} You may find it useful to proceed as follows:
            \begin{itemize}
                \item
                    Show first that $\mat E \in \mathcal E$ if and only if $\mat E \widehat {\vect v} = - \vect r$.

                \item
                    Deduce from the previous item that
                    \[
                        \forall \mat E \in \mathcal E, \qquad
                        \norm{\mat E} \geq \norm{\vect r}.
                    \]

                \item
                    Find a rank one matrix $\mat E_* \in \mathcal E$ such that
                    \(
                    \norm{\mat E_*} = \norm{\vect r},
                    \)
                    and then conclude.
                    Recall that any rank 1 matrix can be written in the form $\mat E_* = \vect u \vect w^*$,
                    with norm $\norm{\vect u} \norm{\vect w}$.
            \end{itemize}

        \item
            \mybonus{2}
            Suppose that we have calculated~$\lambda_n$ and $\lambda_{n-1}$ together with the associated normalized eigenvectors.
            Propose a method for calculating the third smallest eigenvalue in modulus, i.e.\ $\lambda_{n-2}$.
    \end{enumerate}
\end{question}



\newpage

\section*{Auxiliary results}
\begin{proposition}
    \label{proposition:interpolation_error}
    Assume that~$f\colon [a, b] \to \real$ is a function in $C^{2}([a, b])$ and let $\widehat f$ denote
    the interpolation of~$f$ at two distinct interpolation nodes~$y_1, y_2$.
    Then there exists~$\xi\colon [a, b] \to [a, b]$ such that
    \[
        \forall y \in [a, b], \qquad
        f(y) - \widehat f(y) = \frac{f''\bigl(\xi(y)\bigr)}{2} (y-y_1) (y - y_2).
    \]
\end{proposition}

\begin{proposition}
    \label{proposition:local_convergence}
    Assume that $F\colon (0, \infty) \to (0, \infty)$ is continuously differentiable,
    and suppose that~$x_* \in (0, \infty)$ is a fixed point of the iteration
    \(
        x_{k+1} = F(x_k).
    \)
    If
    \[
        \abs{F'(x_*)} < 1,
    \]
    then the fixed point $x_*$ is locally exponentially stable.
\end{proposition}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{newton-raphson.pdf}
    \caption{You can use this figure to illustrate the Newton--Raphson method.}%
    \label{fig:nr}
\end{figure}

\end{document}
