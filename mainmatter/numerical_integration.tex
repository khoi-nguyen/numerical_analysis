\chapter{Numerical integration}
\label{cha:quadrature}

\minitoc

\section*{Introduction}
Integrals are ubiquitous in science and mathematics.
In this chapter,
we are concerned with the problem of calculating numerically integrals of the form
\begin{equation}
    \label{eq:integral}
    I = \int_{\Omega} u(\vect x) \, \d \vect x,
\end{equation}
Perhaps somewhat surprisingly,
the numerical calculation of such integrals when $n \gg 1$ is still a very active area of research today.
In this chapter,
we will focus for simplicity on the one-dimensional setting where $\Omega = [a, b] \subset \real$.
We assume throughout this chapter that the function~$u$ is Riemann-integrable.
This means that
\[
    I = \lim_{h \to 0}
    \sum_{i=0}^{n-1} u(t_i) (z_{i+1} - z_i),
\]
where $a = z_0 < \dotsb < z_n = b$ is a partition of the interval~$[a, b]$ such that the maximum spacing between successive~$x$ values is equal to~$h$,
and with $t_i \in [x_i, x_{i+1}]$ for all $i \in \{0, \dotsc, n-1\}$.

All the numerical integration formulas that we present in this chapter are based on a deterministic approximation of the form
\begin{equation}
    \label{eq:deterministic_integration}
    \widehat I = \sum_{i=0}^{m} w_i u(x_i),
\end{equation}
where~$x_0 < \dotsc < x_n$ are the \emph{integration nodes} and $w_0, \dotsc, w_n$ are the \emph{integration weights}.
In many cases, integration formulas contain a small parameter that can be changed to improve the accuracy of the approximation.
In methods based on equidistant interpolation nodes,
for example, this parameter encodes the distance between nodes and is typically denoted by~$h$,
and we often use the notation~$\widehat I_h$ to emphasize the dependence of the approximation on~$h$.
The difference~$E_h = I - I_h$ is called the \emph{integration error} or \emph{discretization error},
and the \emph{degree of precision} of an integration method is the smallest integer number~$d$ such that
the integration error is zero for all the polynomials of degree less than or equal to~$d$.

We observe that,
without loss of generality,
we can consider that the integration interval is equal to~$[-1, 1]$.
Indeed, using the change of variable
\begin{align}
    \notag
    \zeta\colon &[-1, 1] \to [a, b]; \\
    \label{eq:change_of_variable_integration}
                &y \mapsto \frac{b+a}{2} + \frac{(b-a)}{2} y,
\end{align}
we have
\begin{equation}
    \label{eq:change_of_variable_equivalence}
    \int_{a}^{b} u(x) \, \d x
    = \int_{-1}^{1} u\bigl(\zeta(y)\bigr) \, \zeta'(y) \, \d y
    = \frac{b-a}{2}\int_{-1}^{1} u \circ \zeta (y) \, \d y,
\end{equation}
and the right-hand side is the integral of $u \circ \zeta$ over the interval~$[-1, 1]$.

\section{The Newton--Cotes method}
\label{sec:newton_cotes}

Given a set of equidistant points $-1 = x_0 < \dotsb < x_m = 1$,
a natural method for approximating the integral~\eqref{eq:integral} of a function~$u\colon [-1, 1] \to \real$
is to first construct the interpolating polynomial~$\widehat u$ through the nodes,
and then calculate the integral of this polynomial.
By construction, this method is exact for polynomials of degree up to~$m$,
and so the degree of precision is equal to \emph{at least}~$m$.
Let $\varphi_0, \dotsc, \varphi_m$ denote the Lagrange polynomials associated with the integration nodes.
Then we have
\[
    I \approx \int_{-1}^{1} \widehat u(x) \, \d x
    = \int_{-1}^{1} \sum_{i=0}^{n} u(x_i) \varphi_i u(x) \, \d x
    = \sum_{i=0}^{n} u(x_i) \underbrace{\int_{-1}^{1}  \varphi_i u(x)  \, \d x}_{w_i}.
\]
The weights are independent of the function~$u$,
and so they can be calculated a priori.
The class of integration methods obtained using this approach are known as \emph{Newton--Cotes methods}.
We present a few particular cases:
\begin{itemize}
    \item
        $m = 1$, $d = 1$ (trapezoidal rule):
        \begin{equation}
            \label{eq:trapezoidal_rule}
            \int_{-1}^{1} u(x) \, dx
            \approx u(-1) + u(1).
        \end{equation}

    \item
        $m = 2$, $d = 3$ (Simpson's rule):
        \begin{equation}
            \label{eq:simpsons}
            \int_{-1}^{1} u(x) \, dx
            \approx \frac{1}{3} u(-1) + \frac{4}{3} u(0) + \frac{1}{3} u(1).
        \end{equation}

    \item
        $m = 3$, $d = 3$ (Simpson's $\frac{3}{8}$ rule):
        \[
            \int_{-1}^{1} u(x) \, dx
            \approx \frac{1}{4} u(-1) + \frac{3}{4} u(-1/3) + \frac{3}{4} u(1/3) + \frac{1}{4} u(1).
        \]

    \item
        $m = 4$, $d = 5$ (Bode's rule):
        \[
            \int_{-1}^{1} u(x) \, dx
            \approx \frac{7}{45} u(-1) + \frac{32}{45} u\left(-\frac{1}{2}\right) + \frac{12}{45} u\left(0\right) + \frac{32}{45} u\left(\frac{1}{2}\right) + \frac{7}{45} u(1).
        \]
\end{itemize}
In principle,
this approach could be employed in order to construct integration rules of arbitrary high degree of precision.
In practice, however, the weights become more and more imbalanced as the number of interpolation points increases,
with some of them becoming negative.
As a result, roundoff errors become increasingly detrimental to accuracy as the degree of precision increases.
In addition, in cases where the interpolating polynomial does not converge to~$u$,
for example if~$u$ is Runge's function,
the approximate integral may not even converge to the correct value in the limit as~$m \to \infty$ in exact arithmetic!

Note that,
although it is based on a quadratic polynomial interpolation,
Simpson's rule~\eqref{eq:simpsons} has a degree of precision equal to~$3$.
This is because any integration rule with symmetric nodes and weights on either side of $x=0$ is exact for odd functions,
in particular $x^3$.
Likewise, the degree of precision of Bode's rule is equal to 5.

\section{Composite methods with equidistant nodes}
\label{sec:composite_methods}
A natural alternative to the approach presented in~\cref{sec:newton_cotes}
is to construct an integration rule using piecewise polynomial interpolation,
which we studied in~\cref{sub:piecewise_interpolation}.
After partitioning the integration interval in a number of subintervals,
the integral can be approximated by using one of the rules presented in~\eqref{sec:newton_cotes} within each subinterval.

\paragraph{Composite trapezoidal rule.}
Let us illustrate the composite approach with an example.
To this end,
we introduce a partition $a = x_0 < \dotsb < x_m = b$ of the interval $[a, b]$ and
assume that the nodes are equidistant with $x_{i+1} - x_i = h$.
Using~\eqref{eq:change_of_variable_equivalence},
we first generalize~\eqref{eq:trapezoidal_rule} to an interval~$[x_i, x_{i+1}]$ as follows:
\[
    \int_{x_i}^{x_{i+1}} u(x) \, \d x
    = \int_{-1}^{1} u \circ \zeta (y) \, \d y
    \approx u \circ \zeta(-1)  + u \circ \zeta(1)
    = \frac{h}{2} \bigl(u(x_i) + u(x_{i+1})\bigr),
\]
where $\approx$ in this equation indicates approximation using the trapezoidal rule.
Applying this approximation to each subinterval of the partition,
we obtain the composite trapezoidal rule:
\begin{align}
    \notag
    \int_{a}^{b} u(x) \, \d x
    &= \sum_{i=0}^{n-1} \int_{x_i}^{x_{i+1}} u(x) \, \d x
    \approx
    \frac{h}{2}\sum_{i=0}^{n-1} \bigl( u(x_i) + u(x_{i+1}) \bigr) \\
    \label{eq:composite_trapezoidal_rule}
    &= h \bigl( u(x_0) + 2 u(x_1) + 2 u(x_2) + \dotsb + 2 u(x_{n-2}) + 2 u(x_{n-1}) + u(x_n) \bigr).
\end{align}
Like the trapezoidal rule~\eqref{eq:trapezoidal_rule},
the composite trapezoidal rule~\eqref{eq:composite_trapezoidal_rule} has a degree of precision equal to 1.
However,
the accuracy of the method depends on the parameter~$h$,
which represents the width of each subinterval:
for very small~$h$,
equation~\eqref{eq:composite_trapezoidal_rule} is expected to provide a good approximation of the integral.
An error estimate can be obtained directly from the formula in~\cref{theorem:interpolation_error} for interpolation error,
provided that we assume that~$u \in C^2([a, b])$.
Denoting by $\widehat I_h$ the approximate integral calculated using~\eqref{eq:composite_trapezoidal_rule},
and by~$\widehat u_h$ the piecewise linear interpolation of~$u$,
we have
\[
    \int_{x_{i}}^{x_{i+1}} u(x) - \widehat u(x) \, \d x
    = \frac{1}{2} \int_{x_{i}}^{x_{i+1}} u''\bigl(\xi(x)\bigr) (x - x_{i}) (x - x_{i+1}) \, \d x.
\]
Since $(x - x_i) (x - x_{i+1})$ is nonnegative over the interval $[x_i, x_{i+1}]$,
we deduce that
\[
    \abs*{\int_{x_{i}}^{x_{i+1}} u(x) - \widehat u(x) \, \d x}
    \leq \left( \sup_{\xi \in [a, b]} \abs*{u''(\xi)} \right) \int_{x_{i}}^{x_{i+1}} (x - x_{i}) (x - x_{i+1}) \, \d x
    = C_2 \frac{h^3}{12},
\]
where we introduced
\[
    C_2 =  \sup_{\xi \in [a, b]} \abs*{u''(\xi)} .
\]
Summing the contributions of all the intervals,
we obtain
\[
    \abs{I - \widehat I}
    \leq \sum_{i=0}^{n-1} \abs*{\int_{x_i}^{x_{i+1}} u(x) - \widehat u(x) \, \d x }
    \leq n \times C_2 \frac{h^3}{12} = \frac{b-a}{12} C_2 h^2.
\]
The integration error therefore scales as~$\mathcal O(h^2)$.
(Strictly speaking, we have shown only that the integration error admits an upper bound that scales at $\mathcal O(h^2)$,
but it turns out that the dependence on~$h$ of this bound is optimal).

\paragraph{Composite Simpson rule.}
The composite Simpson rule is derived in~\cref{exercise:composite_simpson}.
Given an odd number $n+1$ of equidistant points $a = x_0 < x_1 < \dotsb < x_n = b$,
it is given by
\begin{equation}
    \label{eq:composite_simpson}
    \widehat I_h = \frac{h}{3} \Bigl( u(x_0) + 4 u(x_1) + 2 u(x_2) + 4 u(x_3) + 2 u_(x_4) + \dotsb + 2 u(x_{n-2}) + 4 u(x_{n-1}) + u(x_n)\Bigr).
\end{equation}
This approximation is obtained by integrating the piecewise quadratic interpolant
over a partition of the integration interval into $n/2$ subintervals of equal width.
Obtaining an optimal error estimate,
in terms of the dependence on~$h$,
for this integration formula is slightly more involved.
% because it requires to employ the fact that the degree of precision for Simpson's rule is 4.
For a given subinterval~$[x_{2i}, x_{2i+2}]$,
let us denote by~$\widehat u_2(x)$ the quadratic interpolating polynomial at $x_{2i}, x_{2i+1}, x_{2i+2}$,
and by $\widehat u_3(x)$ a cubic interpolating polynomial relative to the nodes $x_{2i}, x_{2i+1}, x_{2i+2}, x_{\alpha}$,
for some $\alpha \in [x_{2i}, x_{2i+1}]$.
We have
\begin{align}
    \label{eq:composite_simpson_error}
    \int_{x_{2i}}^{x_{2i+2}} u(x) - \widehat u_3(x) \, \d x
    &= \int_{x_{2i}}^{x_{2i+2}} u(x) - \widehat u_3(x)  \, \d x + \int_{x_{2i}}^{x_{2i+1}} \widehat u_3(x) - \widehat u_2(x)  \, \d x.
\end{align}
The second term is zero,
because the integrand is a cubic polynomial with zeros at $x_{2i}$, $x_{2i+1}$ and $x_{2i+2}$,
and because
\[
    \int_{x_{2i}}^{x_{2i+2}} (x - x_{2i}) (x - x_{2i+1}) (x - x_{2i+2}) = 0.
\]
(Notice that the integrand is even around $x_{2i+1}$.)
The cancellation of the second term in~\eqref{eq:composite_simpson_error} also follows from the fact that
the degree of precision of the Simpson rule~\eqref{eq:simpsons} is equal to~3.
Using~\cref{theorem:interpolation_error},
we rewrite first term in~\eqref{eq:composite_simpson_error} from above as follows:
\[
    \abs*{\int_{x_{2i}}^{x_{2i+2}} u(x) - \widehat u_3(x)  \, \d x}
    \leq \int_{x_{2i}}^{x_{2i+2}} \frac{u^{(4)} \bigl(\xi(x)\bigr)}{24} (x-x_{2i})(x- x_{2i+1}) (x-x_{2i+2}) (x - x_{\alpha})\, \d x.
\]
Since this formula is valid for all $\alpha \in [2i, 2i+2]$,
we are allowed to take $\alpha = x_{2i+1}$.
Given that
\[
    \int_{x_{2i}}^{x_{2i+2}} (x-x_{2i})(x- x_{2i+1})^2 (x-x_{2i+2}) \, \d x = \frac{4}{15} h^5,
\]
with an integrand everywhere positive in the interval $[x_{2i}, x_{2i+2}]$,
we conclude that
\[
    \abs*{\int_{x_{2i}}^{x_{2i+2}} u(x) - \widehat u_3(x)  \, \d x} \leq \frac{C_4}{90} h^5,
    \qquad C_4 = \sup_{\xi \in [a, b]} \abs*{u^{(4)}(\xi)}.
\]
Summing the contributions of all the subintervals,
we finally obtain
\begin{equation}
    \label{eq:scaling_error}
    \abs{I - \widehat I_h} \leq  \frac{n}{2} \times \frac{C_4 h^5}{8} = (b-a)\frac{C_4 h^4}{180}.
\end{equation}

\paragraph{Estimating the error.}
In practice,
it is useful to have an estimate the integration error so that,
if the error is deemed too large,
a better approximation of the integral can then be calculated by using a smaller value for the step size~$h$.
Calculating the exact error $I - \widehat I_h$ is impossible in general,
because this would require to know the exact value of the integral,
but it is possible to calculate a rough approximation of the error based on two numerical approximations of the integral,
as we illustrate formally hereafter for the composite Simpson rule.

Assume that $\widehat I_{2h}$ and $\widehat I_{h}$ are two approximations of the integral,
calculated using the composite Simpson rule with step size $2h$ and $h$, respectively.
If we assume that the error scales as~$\mathcal O(h^4)$ as~\eqref{eq:scaling_error} suggests,
then it holds approximately that
\begin{equation}
    \label{eq:relation_errors}
    I - \widehat I_{h} \approx \frac{1}{2^4} (I - \widehat I_{2h}).
\end{equation}
This implies that
\[
    I - \widehat I_{2h} = (I - I_h) + (I_h - I_{2h}) \approx \frac{1}{16}(I - \widehat I_{2h}) + (I_h - \widehat I_{2h}).
\]
Rearranging this equation gives an approximate error estimate for $\widehat I_{2h}$:
\[
    I - \widehat I_{2h} \approx \frac{16}{15} (\widehat I_h - \widehat I_{2h}).
\]
Using~\eqref{eq:relation_errors},
we can then derive an approximate error estimate for $\widehat I_h$:
\begin{equation}
    \label{eq:integration_error_approximation}
    \abs{I - \widehat I_{h}} \approx \frac{1}{15} \abs{\widehat I_h - \widehat I_{2h}}.
\end{equation}
The right-hand side can be calculated numerically,
because it does not depend on the exact value of the integral.
In practice,
this approach works very well,
and the two sides of~\eqref{eq:integration_error_approximation} are very close for small~$h$.
In the code example below,
we approximate the integral
\[
    I = \int_{0}^{1} \cos(x) \, \d x = 1
\]
for different step sizes
and compare the exact error with the approximate error obtained using~\eqref{eq:integration_error_approximation}.
The results obtained are summarized in~\cref{tab:discretization_error},
which shows a good match between the two quantities.
\begin{table}[ht!]
    \def\arraystretch{1.5}
    \centering
    \caption{Comparison between the exact integration error and the approximate integration error calculated using~\eqref{eq:integration_error_approximation}.}
    \label{tab:discretization_error}
    \begin{tabular}{|c|c|c|}
        \hline
        $h$ & Exact error $\abs{I - \widehat I_h}$ & Approximate error $\frac{1}{15}\abs{\widehat I_h - \widehat I_{2h}}$
        \\ \hline
        $2^{-4}$ & $5.166847063531321 \times 10^{-7}$ & $5.185892840930961 \times 10^{-7}$
        \\ \hline
            $2^{-5}$ & $3.226500089326123 \times 10^{-8}$ & $3.229464703065806 \times 10^{-8}$
        \\ \hline
                $2^{-6}$ & $2.0161285974040766 \times 10^{-9}$ & $2.016591486390477 \times 10^{-9}$
        \\ \hline
                    $2^{-7}$ & $1.2600120946615334 \times 10^{-10}$ & $1.260084925291949 \times 10^{-10}$
        \\ \hline
    \end{tabular}
\end{table}

\begin{minted}[fontsize=\footnotesize]{julia}
# Composite Simpson's rule
function composite_simpson(u, a, b, n)
    # Integration nodes
    x = LinRange(a, b, n + 1)
    # Evaluation of u at the nodes
    ux = u.(x)
    # Step size
    h = x[2] - x[1]
    # Approximation of the integral
    return (h/3) * sum([ux[1]; ux[end]; 4ux[2:2:end-1]; 2ux[3:2:end-2]])
end
# Function to integrate
u(x) = cos(x)
# Integration bounds
a, b = 0, π/2
# Exact integral
I = 1.0
# Number of subintervals
ns = [8; 16; 32; 64; 128]
# Approximate integrals
Î = zeros(length(ns))
# Approximation with n = 8
Î[1] = composite_simpson(u, a, b, ns[1])
for i in 2:length(ns)
    Î[i] = composite_simpson(u, a, b, ns[i])
    println("Exact error: $(I - Î[i]), ",
            "Approx error: $((Î[i] - Î[i-1])/15)")
end
\end{minted}

\section{Richardson extrapolation and Romberg's method}
In the previous section,
we showed how the integration error could be approximated based on two approximations of an integral with different step sizes.
The aim of this section is to show that,
by cleverly combining two approximations $\widehat I_h$ and $\widehat I_{2h}$ of an integral,
an approximation even better than $\widehat I_h$ can be constructed.

This approach is based on \emph{Richardson's extrapolation},
which is a general method for accelerating the convergence of sequences,
with applications beyond numerical integration.
The idea is the following:
assume that $J(h)$ is an approximation with step size~$h$ of some unknown quantity~$J_* = \lim_{h\to 0} J(h)$,
and that we have access to evaluations of $J$ at $h, h/2, h/4, h/8 \dotsc$.
Assuming that $J$ extends to a smooth function over $[0, H]$,
we have by Taylor expansion that
\[
    J(\eta) = J(0) + J'(0) \eta + J''(0) \frac{\eta^2}{2} + J^{(3)}(0) \frac{\eta^3}{3!} + \dotsb + J^{(k)}(0) \frac{\eta^k}{k!} + \mathcal O(\eta^{k+1}).
\]

\paragraph{Elimination of the linear error term.}
Let us assume that $J'(0) \neq 0$,
so that the leading order term after the constant $J(0)$ scales as~$\eta$.
Then we have
\begin{align*}
    J(h) &= J(0) + J'(0) h + \mathcal O(h^2) \\
    J(h/2) &= J(0) + J'(0) \frac{h}{2} + \mathcal O(h^2).
\end{align*}
We now ask the following question:
can we combine linearly $J(h)$ and $J(h/2)$ in order to approximate~$J(0)$ with an error scaling as $\mathcal O(h^2)$.
Using the ansatz
\(
    J_1(h/2) = \alpha J(h) + \beta J(h/2),
\)
we calculate
\begin{equation}
    \label{eq:expansion_richardson}
    J_1(h/2) = (\alpha + \beta) J(0) + J'(0) h \left(\alpha + \frac{1 - \alpha}{2}\right) + \mathcal O(h^2).
\end{equation}
Since we want this expression to approximate $J(0)$ for small~$h$,
we need to impose that $\alpha + \beta = 1$.
Then, in order for the term multiplying~$h$ to cancel out,
we require that
\[
    \alpha + \frac{1 - \alpha}{2} = 0
    \qquad \Leftrightarrow \qquad
    \alpha = - 1.
\]
This yields the formula
\begin{equation}
    \label{eq:richardson}
    J_1(h/2) = 2J(h/2) - J(h).
\end{equation}
Notice that, in the case where $J$ is a linear function,
$J_1(h/2)$ is exactly equal to~$J(0)$.
This reveals a geometric interpretation of~\eqref{eq:richardson}:
the approximation $J_1(h/2)$ is simply the $y$ intercept of the straight line passing through the points $\bigl(h/2, J(h/2)\bigr)$ and $\bigl(h, J(h)\bigr)$.

\paragraph{Elimination of the quadratic error term.}
If we had tracked the coefficient of $h^2$ in the previous paragraph,
we would have obtained instead of~\eqref{eq:expansion_richardson} the following equation:
\[
    J_1(h/2) = J(0) - J^{(3)}(0) \frac{h^2}{4} + \mathcal O(h^3).
\]
If we have access also to $J(h/4)$,
then we can also construct
\[
   J_1(h/4) = \frac{2J(h/4) - J(h/2)}{2} = J(0) - J^{(3)}(0) \frac{h^2}{16} + \mathcal O(h^3).
\]
At this point,
it is natural to wonder whether we can combine $J_1(h)$ and $J_1(h/2)$ in order to produce an even better approximation of $J(0)$.
Applying the same reasoning as in the previous section leads to
\[
    J_2(h/4) = \frac{4 J_1(h/2) - J_2(h/4)}{4 - 1} = J(0) + \mathcal O(h^3).
\]
This is an exact approximation of $J(0)$ if $J$ is a quadratic polynomial,
implying that $J_2(h/4)$ is simply the $y$ intercept of the quadratic polynomial interpolation through the points
$\bigl(h/4, J(h/4)\bigr)$, $\bigl(h/2, J(h/2)\bigr)$ and $\bigl(h, J(h)\bigr)$.

\paragraph{Elimination of higher order terms.}
The procedure above can be repeated in order to eliminate terms of higher and higher order.
The following schematic illustrates,
for example, the calculation of an approximation $J_3(h/8) = J(0) + \mathcal O(h^4)$.
\[
    \begin{tikzcd}[cramped, sep=small]
        J(h) \arrow[dr]  \\
        J(h/2) \arrow[r] \arrow[dr] & J_1(h/2) \arrow[dr]  \\
        J(h/4) \arrow[r] \arrow[dr] & J_1(h/4) \arrow[r] \arrow[dr] & J_2(h/4) \arrow[dr]  \\
        J(h/8) \arrow[r] & J_1(h/8) \arrow[r] & J_2(h/8) \arrow[r] & J_3(h/8) \\
        \mathcal O(h) & \mathcal O(h^2) & \mathcal O(h^3) & \mathcal O(h^4).
    \end{tikzcd}
\]

\paragraph{Generalization}

\section{Methods with non-equidistant nodes}
The Newton--Cotes method relies on equidistant integration nodes,
and the only degrees of freedom are the integration weights.
If the nodes are not fixed,
then additional degrees of freedom are available,
and these can be leveraged in order to construct a better integration formula.
The total number of degrees of freedom for a general integration rule of the form~\eqref{eq:deterministic_integration} is $2n + 2$,
which enable to construct an integration rule with degree of precision equal to~$2n+1$.

A necessary and sufficient condition for an integration rule of the form~\eqref{eq:deterministic_integration} to have a degree of precision
equal to $2n +1$ is that it integrates exactly all the monomials of degree $0$ to $2n+1$.
Indeed, since $\widehat I$ is a linear function of~$u$,
we have
\begin{align*}
    \widehat I(\alpha_0 + \alpha_1 x + \dotsb + \alpha_{2n+1})
    &= \alpha_0 \widehat I(1) + \alpha_1 \widehat I(x) + \dotsb + \alpha_{2n+1} \widehat I(x^{2n+1}) \\
    &= \alpha_0 I(1) + \alpha_1 I(x) + \dotsb + \alpha_{2n+1} I(x^{2n+1}) \\
    &= I(\alpha_0 + \alpha_1 x + \dotsb + \alpha_{2n+1} x^{2n+1}),
\end{align*}
where $I(u)$ and $\widehat I(u)$ denote respectively the exact integral of~$u$ and the approximate integral using a rule of the form~\eqref{eq:deterministic_integration}.
Finding the nodes and weights of the integration rule,
we can therefore solve the nonlinear system of equations:
\begin{align}
    \label{eq:system_gauss_legendre}
    \sum_{i=0}^{m} w_i x_i^{j} &= \int_{-1}^{1} x^j \, \d x, \qquad j = 0, \dotsc, 2n+1.
\end{align}
The quadrature rule obtained by solving this system of equations is called the \emph{Gauss--Legendre quadrature}.
Let us derive the Gauss--Legendre quadrature with $n + 1 = 2$ nodes.
The system of equation that we need to solve in this case is the following:
\begin{align*}
    w_0 + w_1 = 2,
    \qquad w_0 x_0 + w_1 x_1 = 0,
    \qquad w_0 x_0^2 + w_1 x_1^2 = \frac{2}{3},
    \qquad w_0 x_0^3 + w_1 x_1^3 = 0.
\end{align*}
The solution to these equations is given by
\[
    - x_0 = x_1 = \frac{\sqrt{3}}{3}, \qquad w_1 = - w_2 = 1.
\]

Gauss--Legendre integration is ubiquitous in numerical methods for partial differential equations,
in particular the \emph{finite element method}.
Its generalization to higher dimensions is immediate:
for a function $u\colon [-1, 1] \times [-1, 1] \to \real$,
we have
\[
    \int_{0}^{1} \int_{0}^{1} u(x, y) \, \d y \d x \approx \sum_{i=0}^{n} \sum_{j=0}^{n} w_i w_j u(x_i, y_i).
\]
The degree of precision of this integration rule is the same as
that of the corresponding one-dimensional rule.

\section{Exercises}
\begin{exercise}
    Derive the Simpson's integration rule~\eqref{eq:simpsons}.
\end{exercise}

\begin{exercise}
    \label{exercise:composite_simpson}
    Derive the composite Simpson integration rule~\eqref{eq:composite_simpson}.
\end{exercise}

\begin{exercise}
    Show that the integration error for the composite Simpson rule satisfies the following bound:
    \[
        \abs{I - \widehat I_h}
        \leq C (b-a) \left( \sup_{\xi \in [a, b]} \abs*{u^{(4)}(\xi)} \right) h^4,
    \]
    for an appropriate constant~$C$.
\end{exercise}

\begin{exercise}
    The Gauss--Hermite quadrature rule with~$n$ nodes is an approximation of the form
    \[
        \int_{-\infty}^{\infty} u(x) \, \e^{- \frac{x^2}{2}} \, \d x = \sum_{i=0}^{n} w_i u(x_i),
    \]
    such that the rule is exact for all polynomials of degree less than or equal to $2n+1$.
    Find the Gauss--Hermite rule with two nodes.
\end{exercise}

\section{Discussion and bibliography}
In this chapter,
we covered mainly \emph{deterministic} integration formulas.
The presentation of part of the material follows that in~\cite{Legat}.
Much of the research around the calculation of high-dimensional integrals today is concerned with~\emph{probabilistic} integration methods using Monte Carlo approaches.
These methods are based on the connection between integrals and expectations.
For example, the integral
\[
    I = \int_{0}^{1} x^2 \, \d x
\]
may be expressed as the expectation $\expect [X^2]$,
where~$\expect$ is the expectation operator and~$X \sim \mathcal U(0, 1)$ is a uniformly distributed random variable over the interval~$[0, 1]$.
Therefore, in practice, $I$ may be approximated by generating a large number of samples $X_1, X_2,\dotsc$ from the distribution~$\mathcal U(0, 1)$
and averaging $f(X_i)$ over all these samples.
\begin{minted}{julia}
    n = 1000
    f(x) = x^2
    X = rand(n)
    I = (1/n) * sum(f.(X))
\end{minted}
The main advantage of this approach is that it generalizes very easily to high-dimensional and infinite-dimensional settings.
