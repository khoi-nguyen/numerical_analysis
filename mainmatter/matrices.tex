\setlength{\OuterFrameSep}{0pt}

\chapter{Vectors and matrices}%
\label{cha:vectors_and_matrices}

In this chapter,
we collect basic results on vectors and matrices that are useful for this course.
Throughout these lecture notes,
we use lower case and bold font to denote vectors, e.g.\ $\vect x \in \real^n$,
and upper case to denote matrices, e.g.\ $\mat A \in \real^{m \times n}$.
The entries of a vector $\vect x \in \real^n$ are denoted by~$(x_i)$,
and those of a matrix $\mat A \in \real^{m \times n}$ are denoted by~$(a_{ij})$.

\section{Inner products and norms}%
\label{sec:inner_product_and_norm}

We begin by recalling the definitions of the fundamental concepts of \emph{inner product} and \emph{norm}.
For simplicity,
we consider only the case of a \emph{real} vector space,
i.e.\ a vector space for which the scalar field is $\real$,
but the concepts can be extended without conceptual obstructions to the case of a \emph{complex} vector space.
\begin{definition}
    An inner product on a \emph{real} vector space $\mathcal X$ is a function $\ip{\placeholder,\placeholder}: \mathcal X \times \mathcal X \to \real$ satisfying the following axioms:
    \begin{itemize}
        \item
            \textbf{Symmetry}:
            \[
                \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
                \ip{\vect x, \vect y} = \ip{\vect y, \vect x}.
            \]

        \item
            \textbf{Linearity}:
            For all $(\alpha, \beta) \in \real^2$ and all $(\vect x, \vect y, \vect z) \in \mathcal X^3$,
            it holds that
            \[
                \ip{\alpha \vect x + \beta \vect y, \vect z}
                = \alpha \ip{\vect x, \vect z} + \beta \ip{\vect y, \vect z}.
            \]

        \item
            \textbf{Positive-definiteness}:
            \[
                \forall \vect x \in \mathcal X \backslash \{0\}, \qquad
                \ip{\vect x, \vect x} > 0.
            \]
    \end{itemize}
\end{definition}
For example, the familiar Euclidean inner product on $\real^n$ is given by
\[
    \ip{\vect x, \vect y} := \sum_{i=1}^{n} x_i y_i.
\]
A vector space with an inner product is called an \emph{inner product space}.

\begin{definition}
A norm on a real vector space $\mathcal X$ is a function $\norm{\placeholder}: \mathcal X \to \real$ satisfying the following axioms:
\begin{itemize}
    \item
        \textbf{Positivity}:
        \(
            \forall \vect x \in \mathcal X \backslash \{\vect 0\}, \quad
            \norm{\vect x} > 0.
        \)

    \item
        \textbf{Homogeneity}:
        \(
            \forall (c, \vect x) \in \real \times \mathcal X, \quad
            \norm{c \vect x} = \abs{c} \, \norm{\vect x} .
        \)

    \item
        \textbf{Triangular inequality}:
        \(
            \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \quad
            \norm{\vect x + \vect y} \leq \norm{\vect x} + \norm{\vect y}.
        \)
\end{itemize}
\end{definition}
Any inner product on $\mathcal X$ induces a norm via the formula
\begin{equation}
    \label{eq:induced_norm}%
    \norm{\vect x} = \sqrt{\ip{\vect x, \vect x}}.
\end{equation}
The Cauchy--Schwarz inequality enables to bound inner products using norms.
\begin{proposition}
    [Cauchy--Schwarz inequality]
    \label{proposition:cauchy_shwartz}
    Let $\mathcal X$ be an inner product space.
    It holds that
    \begin{equation}
        \label{eq:cauchy_shwartz}
        \forall  (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
        \abs[big]{\ip{\vect x, \vect y}} \leq \norm{\vect x} \norm{\vect y}.
    \end{equation}
\end{proposition}
\begin{proof}
    Let us define $p(t) = \norm{\vect x + t \vect y}^2$.
    Using the bilinearity of the inner product,
    we have
    \[
        p(t) = \norm{\vect x}^2 + 2 t \ip{\vect x, \vect y} + t^2 \norm{\vect y}^2.
    \]
    This shows that $p$ is a convex second-order polynomial with a minimum at $t_* = - \ip{\vect x, \vect y} / \norm{\vect y}^2$.
    Substituting this value in the expression of $p$,
    we obtain
    \[
        p(t_*) = \norm{\vect x}^2 - 2 \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2} +  \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}
        = \norm{\vect x}^2 - \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}.
    \]
    Since $p(t_*) \geq 0$ by definition of $p$,
    we obtain~\eqref{eq:cauchy_shwartz}.
\end{proof}

Several norms can be defined on the same vector space $\mathcal X$.
Two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\mathcal X$ are said to be equivalent if
there exists positive real numbers~$c_{\ell}$ and $c_u$ such that
\begin{equation}
    \label{eq:norm_equilavence}
    \forall \vect x \in \mathcal X,
    \qquad c_{\ell} \norm{\vect x}_{\alpha}
    \leq \norm{\vect x}_{\beta}
    \leq c_u \norm{\vect x}_{\alpha}.
\end{equation}
When working with norms on finite-dimensional vector spaces,
it is important to keep in mind the following result.
The proof is provided for information purposes.
\begin{proposition}
    Assume that $\mathcal X$ is a finite-dimensional vector space.
    Then all the norms defined on~$\mathcal X$ are pairwise equivalent.
\end{proposition}
\begin{proof}
    Let $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ be two norms on $\mathcal X$,
    and let $(\vect e_1, \dotsc, \vect e_n)$ be a basis of $\mathcal X$,
    where $n$ is the dimension of the vector space.
    % Assume for contradiction that there exists a sequence $(x_i)_{i \in \nat}$ in the limit as $i \to \infty$ such that $\norm{x_i}_{\alpha} \to 0$ but $\norm{x_n}_{\beta} > \varepsilon > 0$.
    % Upon replacing $x_i$ by $x_i/\norm{x_i}_{\beta}$, we may assume without loss of generality that $\norm{x_n}_{\beta} = 1$.
    Any $\vect x \in \mathcal X$ can be represented as $x = \lambda_1 \vect e_1 + \dotsb + \lambda_n \vect e_n$.
    By the triangle inequality,
    it holds that
    \begin{equation}
        \label{eq:bound1}
        \norm{\vect x}_{\alpha} \leq \abs{\lambda_1} \norm{\vect e_1}_{\alpha} + \dotsb + \abs{\lambda_n} \norm{\vect e_n}_{\alpha} \leq \Bigl(\abs{\lambda_1} + \dotsb + \abs{\lambda_n}\Bigr) \, \max \Bigl\{\norm{\vect e_1}_{\alpha}, \dotsc, \norm{\vect e_n}_{\alpha}\Bigr\}.
    \end{equation}
    On the other hand, as we prove below,
    there exists a constant $\ell$ such that
    \begin{equation}
        \label{eq:bound2}
        \forall \vect x \in \mathcal X, \qquad
        \norm{\vect x}_{\beta}
        \geq \ell \Bigl( \abs{\lambda_1} + \dotsb + \abs{\lambda_n} \Bigr).
    \end{equation}
    Combining~\eqref{eq:bound1} and~\eqref{eq:bound2},
    we conclude that
    \[
        \norm{\vect x}_{\alpha} \leq \frac{1}{\ell} \max \Bigl\{\norm{\vect e_1}_{\alpha}, \dotsc, \norm{\vect e_n}_{\alpha}\Bigr\} \norm{\vect x}_{\beta}.
    \]
    This proves the first inequality in~\eqref{eq:norm_equilavence},
    and reversing the roles of $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ yields the second inequality.

    We now prove~\eqref{eq:bound2} by contradiction.
    If this inequality were not true,
    then there would exist a sequence $(\vect x^{(i)})_{i \in \nat}$ such
    that~$\norm{\vect x^{(i)}}_{\beta} \to 0$ as $i \to \infty$ but $\abs{\lambda_1^{(i)}} + \dotsb + \abs{\lambda_n^{(i)}} = 1$ for all $i \in \nat$.
    Since $\lambda_1^{(i)} \in [-1, 1]$ for all $i \in \nat$,
    we can extract a subsequence, still denoted by $(\vect x^{(i)})_{i \in \nat}$ for simplicity,
    such that the corresponding coefficient $\lambda_1^{(i)}$ satisfies $\lambda_1^{(i)} \to \lambda_1^* \in [-1, 1]$,
    by compactness of the interval~$[-1, 1]$.
    Repeating this procedure for $\lambda_2, \lambda_3, \dots$,
    by taking a new subsequence every time,
    we obtain a subsequence such that $\lambda^{(i)}_j \to \lambda_j^*$ for all $j \in \{1, \dotsc, n\}$.
    Therefore, it holds that $\vect x^{(i)} \to \vect x^* := \lambda_1^* \vect e_1 + \dotsb \lambda_n^* \vect e_n$ in the $\norm{\placeholder}_{\beta}$ norm.
    Since $\vect x^{(i)} \to 0$ by assumption and the vectors $\vect e_1, \dotsc, \vect e_n$ are linearly independent,
    this implies that $\lambda_1^* = \dots = \lambda_n^* = 0$,
    which is a contradiction because we also have that
    \[
        \abs{\lambda_1^*} + \dotsb + \abs{\lambda_n^*} = \lim_{i \to \infty} \abs{\lambda_1^{(i)}} + \dotsb + \abs{\lambda_n^{(i)}} = 1.
    \]
    This concludes the proof of~\eqref{eq:bound2}.
\end{proof}

\begin{exercise}
    Using~\cref{proposition:cauchy_shwartz},
    show that the function $\norm{\placeholder}$ defined by~\eqref{eq:induced_norm} satisfies the triangle inequality.
\end{exercise}

\section{Vector norms}%
\label{sub:vector_norms}

In the vector space $\real^n$,
the most commonly used norms ones are particular cases of the $p$-norm, also called H\"older norm.
\begin{definition}
    \label{definition:pnorm_vector}
    Given $p \in [1, \infty]$,
    the $p$-norm of a vector $\vect x \in \real^n$ is defined by
    \[
        \norm{\vect x}_p :=
        \begin{cases}
            \left( \sum_{i=1}^{n} \abs{x^i}^p \right)^{\frac{1}{p}} & \text{if $p < \infty$}, \\
            \max \Bigl\{ \abs{x_1}, \dotsc, \abs{x_n} \Bigr\} & \text{if $p = \infty$}.
        \end{cases}
    \]
\end{definition}
The values of $p$ most commonly encountered in applications are $1$, $2$ and $\infty$.
The $1$-norm is sometimes called the \emph{taxicab} or \emph{Manhattan} norm,
and the $2$-norm is usually called the \emph{Eulidean norm}.
The explicit expressions of these norms are
\[
    \norm{\vect x}_1 = \sum_{i=1}^{n} \abs{x_i},
    \qquad
    \norm{\vect x}_2 = \sqrt{\sum_{i=1}^{n} \abs{x_i}^2}.
\]
Notice that the infinity norm $\norm{\placeholder}_{\infty}$ may be defined as the limit of the $p$-norm as $p \to \infty$:
\[
    \norm{\vect x}_{\infty}
    := \lim_{p \to \infty} \norm{\vect x}_p.
\]

In the rest of this chapter,
the notations $\ip{\placeholder,\placeholder}$ and $\norm{\placeholder}$ without subscript always refer to the Euclidean inner product~\eqref{eq:induced_norm} and induced norm,
unless specified otherwise.

\section{Matrix norms}%
\label{sec:matrix_norms}

Given two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\real^m$ and $\real^n$, respectively,
we define the \emph{operator norm} induced by $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ of the matrix $\mat A$ as
\begin{equation}
    \label{eq:subordinate_norm}
    \norm{\mat A}_{\alpha,\beta} = \sup \bigl\{ \norm{\mat A \vect x}_{\alpha} : \vect x \in \real^n, \norm{\vect x}_{\beta} \leq 1 \bigr\}.
\end{equation}
The term \emph{operator norm} is motivated by the fact that,
to any matrix $\mat A \in \real^{m \times n}$, there naturally corresponds the linear operator from $\real^n$ to $\real^m$ with action~$\vect x \mapsto \mat A \vect x$.
\Cref{eq:subordinate_norm} comes from the general definition of the norm of a bounded linear operator between normed spaces.
Matrix norms of the type~\eqref{eq:subordinate_norm} are also called \emph{subordinate} matrix norms.
An immediate corollary of the definition~\eqref{eq:subordinate_norm} is that,
for all $\vect x \in \real^n$,
\begin{equation}
    \label{eq:submultiplicative_mat_vec}%
    \norm{\mat A \vect x}_{\alpha}
    = \norm{\mat A \widehat{\vect x}}_{\alpha} \norm{\vect x}_{\beta}
    \leq \sup \bigl\{ \norm{\mat A \vect y}_{\alpha}: \norm{\vect y}_{\beta} \leq 1 \bigr\} \norm{\vect x}_{\beta}
    = \norm{\mat A}_{\alpha,\beta} \norm{\vect x}_{\beta},
    \qquad \widehat {\vect x} = \frac{\vect x}{\norm{\vect x}_{\beta}}.
\end{equation}
\begin{proposition}
    Equation~\eqref{eq:subordinate_norm} defines a norm on $\real^{m \times n}$.
\end{proposition}
\begin{proof}
    We need to verify that~\eqref{eq:subordinate_norm} satisfies the properties of positivity and homogeneity,
    together with the triangle inequality.
    \begin{itemize}
        \item
            Checking \textbf{positivity} is simple and left as an exercise.
            % \textbf{positivity}
            % . If $A \neq 0$,
            % then there is a column of $A$, say the $j$-th one, that is nonzero.
            % Let~$\vect y$ be the vector with all entries equal to zero except the $j$-th one which is equal to 1,
            % and let~$\vect x = \vect y / \norm{\vect y}_{\beta}$ so that $\norm{\vect x}_{\beta} = 1$.
            % It is clear that $A \vect x \neq \vect 0$,
            % so $\norm{A \vect x}_{\alpha} > 0$ by positivity of the norm $\norm{\placeholder}_{\alpha}$,
            % implying that $\norm{A}_{\alpha,\beta} > 0$.

        \item
            \textbf{Homogeneity} follows trivially from the definition~\eqref{eq:subordinate_norm}
            and the homogeneity of $\norm{\placeholder}_{\alpha}$.

        \item
            \textbf{Triangle inequality.}
            Let $\mat A$ and $\mat B$ be two elements of $\real^{m \times n}$.
            Employing the triangle inequality for the norm $\norm{\placeholder}_{\alpha}$,
            we have
            \begin{align*}
                \forall \vect x \in \real^{n} ~ \text{with} ~ \norm{\vect x}_{\beta}\leq 1, \qquad
                \norm{(\mat A + \mat B) \vect x}_{\alpha}
                &= \norm{\mat A \vect x + \mat B \vect x}_{\alpha} \\
                &\leq \norm{\mat A \vect x}_{\alpha} + \norm{\mat B \vect x}_{\alpha}
                \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}.
            \end{align*}
            Taking the supremum as in~\eqref{eq:subordinate_norm},
            we obtain $\norm{\mat A + \mat B}_{\alpha,\beta} \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}$.
    \end{itemize}
    Since the three properties are satisfied, $\norm{\placeholder}_{\alpha,\beta}$ is indeed a norm.
\end{proof}

The matrix $p$-norm is defined as the operator norm~\eqref{eq:subordinate_norm} in the particular case
where $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ are both H\"older norms with the same value of $p$.
\begin{definition}
Given $p \in [1, \infty]$,
the $p$-norm of a matrix $\mat A \in \real^{m\times n}$ is given by
\begin{equation}
    \label{eq:matrix_p_norm}
    \norm{\mat A}_{p} := \sup \bigl\{ \norm{\mat A \vect x}_{p} : \vect x \in \real^n, \norm{\vect x}_{p} \leq 1 \bigr\}.
\end{equation}
\end{definition}
Not all matrix norms are induced by vector norms.
For example, the Frobenius norm,
which is widely used in applications,
is not induced by a vector norm.
It is, however, induced by an inner product on~$\real^{m \times n}$.
\begin{definition}
    The Frobenius norm of $\mat A \in \real^{m\times n}$ is defined by
    \begin{equation}
        \label{eq:frobenius_norm}
        \norm{\mat A}_{\rm F} = \sum_{i=1}^{m} \sum_{j=1}^{m} \abs{a_{ij}}^2.
    \end{equation}
\end{definition}

A matrix norm $\norm{\placeholder}$ is said to be submultiplicative if,
for any two matrices $\mat A \in \real^{m \times n}$ and $\mat B \in \real^{n \times \ell}$,
it holds that
\[
    \norm{\mat A \mat B} \leq \norm{\mat A} \norm{\mat B}.
\]
All matrix subordinate matrix norms,
for example the $p$-norms, are submultiplicative,
and so is the Frobenius norm.

\begin{exercise}
    Write down the inner product on $\real^{m \times n}$ corresponding to~\eqref{eq:frobenius_norm}.
\end{exercise}
\begin{exercise}
    Show that the matrix $p$-norm is submultiplicative.
\end{exercise}

\section{Diagonalization}%
\label{sec:diagonalization}

\begin{definition}
    \label{definition:diagonalizable}
    A square matrix $\mat A \in \real^{n \times n}$ is said to be diagonalizable if there exists an invertible matrix $\mat P \in \real^{n \times n}$
    and a diagonal matrix $\mat D \in \real^{n \times n}$ such that
    \begin{equation}
        \label{eq:eigen_decomposition}
        \mat A \mat P = \mat P \mat D.
    \end{equation}
    In this case,
    the diagonal elements of $\mat D$ are called the eigenvalues of~$\mat A$,
    and the columns of $\mat P$ are called the eigenvectors of~$\mat A$.
\end{definition}
Denoting by $\vect e_i$ the $i$-th column of $\mat P$ and by $\lambda_i$ the $i$-th diagonal element of $\mat D$,
we have by~\eqref{eq:eigen_decomposition} that $\mat A \vect e_i = \lambda_i \vect e_i$ or,
equivalently, $(\mat A - \lambda_i \mat I_n) \vect e_i = \vect 0$.
Here $\mat I_n$ is the~$\real^{n \times n}$ identity matrix.
Therefore, $\lambda$ is an eigenvalue of $\mat A$ if and only if $\det(\mat A - \lambda \mat I_n) = 0$.
In other words, the eigenvalues of $\mat A$ are the roots of $\det(\mat A - \lambda \mat I_n)$,
called the \emph{characteristic polynomial}.

\begin{remark}
There is a definition parallel to \cref{definition:diagonalizable} for matrices in $\mat A \in \complex^{n \times n}$.
A matrix with real entries may not be diagonalizable in the sense of \cref{definition:diagonalizable} (over $\real$),
but still be diagonalizable over $\complex$,
when viewed as an element of $\complex^{n \times n}$.
\end{remark}

\subsection*{Symmetric matrices and spectral theorem}%
The transpose of a matrix $\mat A \in \real^{m \times n}$ is denoted by $\mat A^\t \in \real^{n \times m}$
and defined as the matrix with entries~$a^\t_{ij} = a_{ji}$.
A matrix equal to its transpose is necessarily square and called \emph{symmetric}.
Symmetric matrices enjoy many nice properties,
the main one being that they are diagonalizable with a matrix $\mat P$ that is orthogonal,
i.e. such that $\mat P^{-1} = \mat P^\t$.
This is the content of the \emph{spectral theorem},
a pillar of linear algebra with important generalizations to infinite-dimensional operators.

Before stating the spectral theorem,
we note that if $\vect e_1$ and $\vect e_2$ are eigenvectors of a symmetric matrix associated with different eigenvalues,
then they are necessarily orthogonal for the Euclidean inner product.
Indeed, since $\mat A = \mat A^\t$ it holds that
\[
    (\lambda_1 - \lambda_2) \ip{\vect e_1, \vect e_2}
    = \ip{\lambda_1 \vect e_1, \vect e_2} - \ip{\vect e_1, \lambda_2 \vect e_2}
    = \ip{\mat A \vect e_1, \vect e_2} - \ip{\vect e_1, \mat A \vect e_2}
    = \ip{\mat A \vect e_1, \vect e_2} - \ip{\mat A^\t \vect e_1, \vect e_2} = 0.
\]
\begin{theorem}
    [Spectral theorem for real symmetric matrices]
    If $\mat A \in \real^{n \times n}$ is symmetric,
    then there exists an orthogonal matrix $\mat Q \in \real^{n \times n}$ and a diagonal matrix $\mat D \in \real^{n \times n}$ such that
    \[
        \mat A \mat Q = \mat Q \mat D.
    \]
\end{theorem}
\begin{proof}
    [Sketch of the proof]
    The result is trivial for $n = 1$.
    Reasoning by induction,
    we assume that the result is true for symmetric matrices in $\real^{n-1 \times n-1}$
    and prove that it then also holds for~$A \in \real^{n \times n}$.
    % \begin{itemize}
        % \item

            \vspace{.3cm}
            \textbf{Step 1. Existence of a real eigenvalue}.
            By the fundamental theorem of algebra,
            there exists at least one solution $\lambda_1 \in \complex$ to the equation $\det(A - \lambda I_n) = 0$,
            to which there corresponds a solution $\vect e_1 \in \complex^n$ to the equation $(A - \lambda_1 I_n) \vect e = 0$.
            The eigenvalue $\lambda_1$ is necessarily real because
            \[
                \overline \lambda \vect e_1^* \vect e_1
                = (\lambda \vect e_1)^* \vect e_1
                = (\mat A \vect e_1)^* \vect e_1
                = \vect e_1^* \mat A^* \vect e_1
                = \vect e_1^* (\mat A \vect e_1)
                = \vect e_1^* (\lambda \vect e_1)
                = \lambda \vect e_1^* \vect e_1.
            \]
            Here $\overline \placeholder$ denotes the complex conjugate,
            and $\placeholder^*$ denotes the conjugate transpose.
            Since $\lambda_1$ is real, there exists a solution $\vect q_1 \in \real^n$,
            which we may multiply by an appropriate factor so that
            it is of norm 1,
            % $\norm{\vect q_1}^2 = 1$,
            to the equation $(\mat A - \lambda_1 \mat I_n) \vect e = 0$.

        % \item
            \vspace{.3cm}
            \textbf{Step 2. Using the induction hypothesis}.
            Next,
            take an orthonormal basis $(\vect e_2, \dotsc, \vect e_n)$ of the orthogonal complement $\Span\{\vect q_1\}^\perp$
            and construct the orthogonal matrix
            \[
                V = \begin{pmatrix} \vect q_1 & \vect e_2 & \dots & \vect e_n \end{pmatrix},
            \]
            i.e.\ the matrix with columns $\vect q_1$, $\vect e_2$, etc.
            A calculation gives
            \[
                \mat V^T \mat A \mat V =
                \begin{pmatrix}
                    \ip{\vect q_1, \mat A \vect q_1} & \ip{\vect q_1, \mat A \vect e_2} & \dots & \ip{\vect q_1, \mat A \vect e_n} \\
                    \ip{\vect e_2, \mat A \vect q_1} & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ip{\vect e_n, \mat A \vect q_1} & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \lambda_1 & 0 & \dots & 0 \\
                    0 & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}.
            \]
            Let us denote the $n-1 \times n-1$ lower right block of this matrix by $\mat V_{n-1}$.
            This is a symmetric matrix of size $n-1$ so,
            using the induction hypothesis,
            we deduce that $\mat V_{n-1} = \mat Q_{n-1} \mat D_{n-1} \mat Q_{n-1}^\t$
            for appropriate matrices $\mat Q_{n-1} \in \real^{n-1 \times n-1}$ and $\mat D_{n-1} \in \real^{n-1 \times n-1}$
            which are orthogonal and diagonal, respectively.

        % \item
            \vspace{.3cm}
            \textbf{Step 3. Constructing $\mat Q$ and $\mat D$}.
            Define now
            \[
                \mat Q =
                \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            It is not difficult to verify that $\mat Q$ is an orthogonal matrix,
            and we have
            \[
                \mat Q^\t \mat A \mat Q =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^\t
                \end{pmatrix}
                \mat V^\t \mat A \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^\t
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda_1 & \vect 0^\t \\
                    \vect 0 & \mat V_{n-1}
                \end{pmatrix}
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            Developing the last expression, we obtain
            \[
                \mat Q^\t \mat A \mat Q
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat D_{n-1}
                \end{pmatrix},
            \]
            which concludes the proof.
    % \end{itemize}
\end{proof}

\begin{exercise}
    Prove that if $\mat A \in \real^{n \times n}$ is diagonalizable as in~\eqref{eq:eigen_decomposition},
    then $\mat A^n = \mat P \mat D^n \mat P^{-1}$.
\end{exercise}

The largest eigenvalue of a matrix, in absolute value,
is called the \emph{spectral radius} and denoted by~$\rho$.
The 2-norm of a general matrix $\mat A \in \real^{n \times n}$
coincides with the spectral radius $\mat A^\t \mat A$.
Indeed, by the spectral theorem it holds that $\mat A^\t \mat A = \mat Q \mat D \mat Q^\t$ with an orthogonal matrix $\mat Q$.
Therefore, denoting by $(\lambda_i^2)_{1 \leq i \leq n}$ the (positive) diagonal elements of $\mat D$,
we have
\begin{equation}
    \label{eq:matrices_link_norm_spectral_radius}
    \norm{\mat A \vect x}
    = \sqrt{\vect x^\t \mat A^\t \mat A \vect x}
    = \sqrt{\vect x^\t \mat Q \mat D \mat Q^\t \vect x}
    = \sqrt{\sum_{i=1}^{n} \lambda_i^2 y_i^2}
    \leq \sqrt{\rho(\mat A^\t \mat A)} \sqrt{\sum_{i=1}^{n} y_i^2}
    =  \sqrt{\rho(\mat A^\t \mat A)} \norm{\vect y},
\end{equation}
where $\vect y := \mat Q^\t \vect x$ has the same norm as $\vect x$ since $\mat Q$ is orthogonal.
It follows from this inequality that $\norm{\mat A} \leq \rho(\mat A^\t \mat A)$,
and the converse inequality also holds true since $\norm{\mat A \vect x} = \rho(\mat A^\t \mat A) \norm{\vect x}$
if $\vect x$ is the eigenvector of $\mat A^\t \mat A$ associated with the largest eigenvalue $\rho(\mat A^\t \mat A)$.
