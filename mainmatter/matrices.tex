\setlength{\OuterFrameSep}{0pt}

\chapter{Linear algebra}%
\label{cha:vectors_and_matrices}

In this chapter,
we collect basic results on vectors and matrices that are useful for this course.
Throughout these lecture notes,
we use lower case and bold font to denote vectors, e.g.\ $\vect x \in \complex^n$,
and upper case to denote matrices, e.g.\ $\mat A \in \complex^{m \times n}$.
The entries of a vector $\vect x \in \complex^n$ are denoted by~$(x_i)$,
and those of a matrix $\mat A \in \complex^{m \times n}$ are denoted by~$(a_{ij})$ or~$(a_{i,j})$.

\section{Inner products and norms}%
\label{sec:inner_product_and_norm}

We begin by recalling the definitions of the fundamental concepts of \emph{inner product} and \emph{norm}.
For generality,
we consider the case of a \emph{complex} vector space,
i.e.\ a vector space for which the scalar field is $\complex$.
\begin{definition}
    An inner product on a \emph{real} vector space $\mathcal X$ is a function $\ip{\placeholder,\placeholder}: \mathcal X \times \mathcal X \to \complex$ satisfying the following axioms:
    \begin{itemize}
        \item
            \textbf{Conjugate symmetry}:
            Here $\overline \placeholder$ denotes the complex conjugate.
            \[
                \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
                \ip{\vect x, \vect y} = \overline{\ip{\vect y, \vect x}}.
            \]
        \item
            \textbf{Linearity}:
            For all $(\alpha, \beta) \in \real^2$ and all $(\vect x, \vect y, \vect z) \in \mathcal X^3$,
            it holds that
            \[
                \ip{\alpha \vect x + \beta \vect y, \vect z}
                = \alpha \ip{\vect x, \vect z} + \beta \ip{\vect y, \vect z}.
            \]

        \item
            \textbf{Positive-definiteness}:
            \[
                \forall \vect x \in \mathcal X \backslash \{0\}, \qquad
                \ip{\vect x, \vect x} > 0.
            \]
    \end{itemize}
\end{definition}
For example, the familiar Euclidean inner product on $\complex^n$ is given by
\[
    \ip{\vect x, \vect y} := \sum_{i=1}^{n} x_i \overline y_i.
\]
A vector space with an inner product is called an \emph{inner product space}.

\begin{definition}
A norm on a real vector space $\mathcal X$ is a function $\norm{\placeholder}: \mathcal X \to \real$ satisfying the following axioms:
\begin{itemize}
    \item
        \textbf{Positivity}:
        \(
            \forall \vect x \in \mathcal X \backslash \{\vect 0\}, \quad
            \norm{\vect x} > 0.
        \)

    \item
        \textbf{Homogeneity}:
        \(
            \forall (c, \vect x) \in \complex \times \mathcal X, \quad
            \norm{c \vect x} = \abs{c} \, \norm{\vect x} .
        \)

    \item
        \textbf{Triangular inequality}:
        \(
            \forall (\vect x, \vect y) \in \mathcal X \times \mathcal X, \quad
            \norm{\vect x + \vect y} \leq \norm{\vect x} + \norm{\vect y}.
        \)
\end{itemize}
\end{definition}
Any inner product on $\mathcal X$ induces a norm via the formula
\begin{equation}
    \label{eq:induced_norm}%
    \norm{\vect x} = \sqrt{\ip{\vect x, \vect x}}.
\end{equation}
The Cauchy--Schwarz inequality enables to bound inner products using norms.
It is also useful for showing that the functional defined in~\eqref{eq:induced_norm} satisfies the triangle inequality.
\begin{proposition}
    [Cauchy--Schwarz inequality]
    \label{proposition:cauchy_shwartz}
    Let $\mathcal X$ be an inner product space.
    It holds that
    \begin{equation}
        \label{eq:cauchy_shwartz}
        \forall  (\vect x, \vect y) \in \mathcal X \times \mathcal X, \qquad
        \abs[big]{\ip{\vect x, \vect y}} \leq \norm{\vect x} \norm{\vect y}.
    \end{equation}
\end{proposition}
\begin{proof}
    Let us define $p(t) = \norm{\vect x + t \vect y}^2$.
    Using the bilinearity of the inner product,
    we have
    \[
        p(t) = \norm{\vect x}^2 + 2 t \ip{\vect x, \vect y} + t^2 \norm{\vect y}^2.
    \]
    This shows that $p$ is a convex second-order polynomial with a minimum at $t_* = - \ip{\vect x, \vect y} / \norm{\vect y}^2$.
    Substituting this value in the expression of $p$,
    we obtain
    \[
        p(t_*) = \norm{\vect x}^2 - 2 \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2} +  \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}
        = \norm{\vect x}^2 - \frac{\abs[big]{\ip{\vect x, \vect y}}^2}{\norm{\vect y}^2}.
    \]
    Since $p(t_*) \geq 0$ by definition of $p$,
    we obtain~\eqref{eq:cauchy_shwartz}.
\end{proof}

Several norms can be defined on the same vector space $\mathcal X$.
Two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\mathcal X$ are said to be equivalent if
there exist positive real numbers~$c_{\ell}$ and $c_u$ such that
\begin{equation}
    \label{eq:norm_equilavence}
    \forall \vect x \in \mathcal X,
    \qquad c_{\ell} \norm{\vect x}_{\alpha}
    \leq \norm{\vect x}_{\beta}
    \leq c_u \norm{\vect x}_{\alpha}.
\end{equation}
When working with norms on finite-dimensional vector spaces,
it is important to keep in mind the following result.
The proof is provided for information purposes.
\begin{proposition}
    Assume that $\mathcal X$ is a finite-dimensional vector space.
    Then all the norms defined on~$\mathcal X$ are pairwise equivalent.
\end{proposition}
\begin{proof}
    Let $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ be two norms on $\mathcal X$,
    and let $(\vect e_1, \dotsc, \vect e_n)$ be a basis of $\mathcal X$,
    where $n$ is the dimension of the vector space.
    % Assume for contradiction that there exists a sequence $(x_i)_{i \in \nat}$ in the limit as $i \to \infty$ such that $\norm{x_i}_{\alpha} \to 0$ but $\norm{x_n}_{\beta} > \varepsilon > 0$.
    % Upon replacing $x_i$ by $x_i/\norm{x_i}_{\beta}$, we may assume without loss of generality that $\norm{x_n}_{\beta} = 1$.
    Any $\vect x \in \mathcal X$ can be represented as $x = \lambda_1 \vect e_1 + \dotsb + \lambda_n \vect e_n$.
    By the triangle inequality,
    it holds that
    \begin{equation}
        \label{eq:bound1}
        \norm{\vect x}_{\alpha} \leq \abs{\lambda_1} \norm{\vect e_1}_{\alpha} + \dotsb + \abs{\lambda_n} \norm{\vect e_n}_{\alpha} \leq \Bigl(\abs{\lambda_1} + \dotsb + \abs{\lambda_n}\Bigr) \, \max \Bigl\{\norm{\vect e_1}_{\alpha}, \dotsc, \norm{\vect e_n}_{\alpha}\Bigr\}.
    \end{equation}
    On the other hand, as we prove below,
    there exists a positive constant $\ell$ such that
    \begin{equation}
        \label{eq:bound2}
        \forall \vect x \in \mathcal X, \qquad
        \norm{\vect x}_{\beta}
        \geq \ell \Bigl( \abs{\lambda_1} + \dotsb + \abs{\lambda_n} \Bigr).
    \end{equation}
    Combining~\eqref{eq:bound1} and~\eqref{eq:bound2},
    we conclude that
    \[
        \norm{\vect x}_{\alpha} \leq \frac{1}{\ell} \max \Bigl\{\norm{\vect e_1}_{\alpha}, \dotsc, \norm{\vect e_n}_{\alpha}\Bigr\} \norm{\vect x}_{\beta}.
    \]
    This proves the first inequality in~\eqref{eq:norm_equilavence},
    and reversing the roles of $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ yields the second inequality.

    We now prove~\eqref{eq:bound2} by contradiction.
    If this inequality were not true,
    then there would exist a sequence $(\vect x^{(i)})_{i \in \nat}$ such
    that~$\norm{\vect x^{(i)}}_{\beta} \to 0$ as $i \to \infty$ but $\abs{\lambda_1^{(i)}} + \dotsb + \abs{\lambda_n^{(i)}} = 1$ for all $i \in \nat$.
    Since $\lambda_1^{(i)} \in [-1, 1]$ for all $i \in \nat$,
    we can extract a subsequence, still denoted by $(\vect x^{(i)})_{i \in \nat}$ for simplicity,
    such that the corresponding coefficient $\lambda_1^{(i)}$ satisfies $\lambda_1^{(i)} \to \lambda_1^* \in [-1, 1]$,
    by compactness of the interval~$[-1, 1]$.
    Repeating this procedure for $\lambda_2, \lambda_3, \dots$,
    taking a new subsequence every time,
    we obtain a subsequence $(\vect x^{(i)})_{i \in \nat}$ such that $\lambda^{(i)}_j \to \lambda_j^*$ in the limit as~$i \to \infty$, for all $j \in \{1, \dotsc, n\}$.
    Therefore, it holds that $\vect x^{(i)} \to \vect x^* := \lambda_1^* \vect e_1 + \dotsb \lambda_n^* \vect e_n$ in the $\norm{\placeholder}_{\beta}$ norm.
    Since $\vect x^{(i)} \to 0$ by assumption and the vectors $\vect e_1, \dotsc, \vect e_n$ are linearly independent,
    this implies that $\lambda_1^* = \dots = \lambda_n^* = 0$,
    which is a contradiction because we also have that
    \[
        \abs{\lambda_1^*} + \dotsb + \abs{\lambda_n^*} = \lim_{i \to \infty} \abs{\lambda_1^{(i)}} + \dotsb + \abs{\lambda_n^{(i)}} = 1.
    \]
    This concludes the proof of~\eqref{eq:bound2}.
\end{proof}

\begin{exercise}
    Using~\cref{proposition:cauchy_shwartz},
    show that the function $\norm{\placeholder}$ defined by~\eqref{eq:induced_norm} satisfies the triangle inequality.
\end{exercise}

\section{Vector norms}%
\label{sub:vector_norms}

In the vector space $\complex^n$,
the most commonly used norms are particular cases of the $p$-norm, also called H\"older norm.
\begin{definition}
    \label{definition:pnorm_vector}
    Given $p \in [1, \infty]$,
    the $p$-norm of a vector $\vect x \in \complex^n$ is defined by
    \[
        \norm{\vect x}_p :=
        \begin{cases}
            \left( \sum_{i=1}^{n} \abs{x^i}^p \right)^{\frac{1}{p}} & \text{if $p < \infty$}, \\
            \max \Bigl\{ \abs{x_1}, \dotsc, \abs{x_n} \Bigr\} & \text{if $p = \infty$}.
        \end{cases}
    \]
\end{definition}
The values of $p$ most commonly encountered in applications are $1$, $2$ and $\infty$.
The $1$-norm is sometimes called the \emph{taxicab} or \emph{Manhattan} norm,
and the $2$-norm is usually called the \emph{Euclidean norm}.
The explicit expressions of these norms are
\[
    \norm{\vect x}_1 = \sum_{i=1}^{n} \abs{x_i},
    \qquad
    \norm{\vect x}_2 = \sqrt{\sum_{i=1}^{n} \abs{x_i}^2}.
\]
Notice that the infinity norm $\norm{\placeholder}_{\infty}$ may be defined as the limit of the $p$-norm as $p \to \infty$:
\[
    \norm{\vect x}_{\infty}
    := \lim_{p \to \infty} \norm{\vect x}_p.
\]

In the rest of this chapter,
the notations $\ip{\placeholder,\placeholder}$ and $\norm{\placeholder}$ without subscript always refer to the Euclidean inner product~\eqref{eq:induced_norm} and induced norm,
unless specified otherwise.

\section{Matrix norms}%
\label{sec:matrix_norms}

Given two norms $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ on $\complex^m$ and $\complex^n$, respectively,
we define the \emph{operator norm} induced by $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ of the matrix $\mat A$ as
\begin{equation}
    \label{eq:subordinate_norm}
    \norm{\mat A}_{\alpha,\beta} = \sup \bigl\{ \norm{\mat A \vect x}_{\alpha} : \vect x \in \complex^n, \norm{\vect x}_{\beta} \leq 1 \bigr\}.
\end{equation}
The term \emph{operator norm} is motivated by the fact that,
to any matrix $\mat A \in \complex^{m \times n}$, there naturally corresponds the linear operator from $\complex^n$ to $\complex^m$ with action~$\vect x \mapsto \mat A \vect x$.
\Cref{eq:subordinate_norm} comes from the general definition of the norm of a bounded linear operator between normed spaces.
Matrix norms of the type~\eqref{eq:subordinate_norm} are also called \emph{subordinate} matrix norms.
An immediate corollary of the definition~\eqref{eq:subordinate_norm} is that,
for all $\vect x \in \complex^n$,
\begin{equation}
    \label{eq:submultiplicative_mat_vec}%
    \norm{\mat A \vect x}_{\alpha}
    = \norm{\mat A \widehat{\vect x}}_{\alpha} \norm{\vect x}_{\beta}
    \leq \sup \bigl\{ \norm{\mat A \vect y}_{\alpha}: \norm{\vect y}_{\beta} \leq 1 \bigr\} \norm{\vect x}_{\beta}
    = \norm{\mat A}_{\alpha,\beta} \norm{\vect x}_{\beta},
    \qquad \widehat {\vect x} = \frac{\vect x}{\norm{\vect x}_{\beta}}.
\end{equation}
\begin{proposition}
    Equation~\eqref{eq:subordinate_norm} defines a norm on $\complex^{m \times n}$.
\end{proposition}
\begin{proof}
    We need to verify that~\eqref{eq:subordinate_norm} satisfies the properties of positivity and homogeneity,
    together with the triangle inequality.
    \begin{itemize}
        \item
            Checking \textbf{positivity} is simple and left as an exercise.
            % \textbf{positivity}
            % . If $A \neq 0$,
            % then there is a column of $A$, say the $j$-th one, that is nonzero.
            % Let~$\vect y$ be the vector with all entries equal to zero except the $j$-th one which is equal to 1,
            % and let~$\vect x = \vect y / \norm{\vect y}_{\beta}$ so that $\norm{\vect x}_{\beta} = 1$.
            % It is clear that $A \vect x \neq \vect 0$,
            % so $\norm{A \vect x}_{\alpha} > 0$ by positivity of the norm $\norm{\placeholder}_{\alpha}$,
            % implying that $\norm{A}_{\alpha,\beta} > 0$.

        \item
            \textbf{Homogeneity} follows trivially from the definition~\eqref{eq:subordinate_norm}
            and the homogeneity of $\norm{\placeholder}_{\alpha}$.

        \item
            \textbf{Triangle inequality.}
            Let $\mat A$ and $\mat B$ be two elements of $\complex^{m \times n}$.
            Employing the triangle inequality for the norm $\norm{\placeholder}_{\alpha}$,
            we have
            \begin{align*}
                \forall \vect x \in \complex^{n} ~ \text{with} ~ \norm{\vect x}_{\beta}\leq 1, \qquad
                \norm{(\mat A + \mat B) \vect x}_{\alpha}
                &= \norm{\mat A \vect x + \mat B \vect x}_{\alpha} \\
                &\leq \norm{\mat A \vect x}_{\alpha} + \norm{\mat B \vect x}_{\alpha}
                \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}.
            \end{align*}
            Taking the supremum as in~\eqref{eq:subordinate_norm},
            we obtain $\norm{\mat A + \mat B}_{\alpha,\beta} \leq \norm{\mat A}_{\alpha,\beta} + \norm{\mat B}_{\alpha,\beta}$.
    \end{itemize}
    Since the three properties are satisfied, $\norm{\placeholder}_{\alpha,\beta}$ is indeed a norm.
\end{proof}

The matrix $p$-norm is defined as the operator norm~\eqref{eq:subordinate_norm} in the particular case
where $\norm{\placeholder}_{\alpha}$ and $\norm{\placeholder}_{\beta}$ are both H\"older norms with the same value of $p$.
\begin{definition}
Given $p \in [1, \infty]$,
the $p$-norm of a matrix $\mat A \in \complex^{m\times n}$ is given by
\begin{equation}
    \label{eq:matrix_p_norm}
    \norm{\mat A}_{p} := \sup \bigl\{ \norm{\mat A \vect x}_{p} : \vect x \in \complex^n, \norm{\vect x}_{p} \leq 1 \bigr\}.
\end{equation}
\end{definition}
Not all matrix norms are induced by vector norms.
For example, the Frobenius norm,
which is widely used in applications,
is not induced by a vector norm.
It is, however, induced by an inner product on~$\complex^{m \times n}$.
\begin{definition}
    The Frobenius norm of $\mat A \in \complex^{m\times n}$ is given by
    \begin{equation}
        \label{eq:frobenius_norm}
        \norm{\mat A}_{\rm F} = \left( \sum_{i=1}^{m} \sum_{j=1}^{n} \abs{a_{ij}}^2 \right)^{\frac{1}{2}}.
    \end{equation}
\end{definition}

A matrix norm $\norm{\placeholder}$ is said to be submultiplicative if,
for any two matrices $\mat A \in \complex^{m \times n}$ and $\mat B \in \complex^{n \times \ell}$,
it holds that
\[
    \norm{\mat A \mat B} \leq \norm{\mat A} \norm{\mat B}.
\]
All subordinate matrix norms,
for example the $p$-norms, are submultiplicative,
and so is the Frobenius norm.

\begin{exercise}
    Write down the inner product on $\complex^{m \times n}$ corresponding to~\eqref{eq:frobenius_norm}.
\end{exercise}
\begin{exercise}
    Show that the matrix $p$-norm is submultiplicative.
\end{exercise}

\section{Diagonalization}%
\label{sec:diagonalization}

\begin{definition}
    \label{definition:diagonalizable}
    A square matrix $\mat A \in \complex^{n \times n}$ is said to be diagonalizable if there exists an invertible matrix $\mat P \in \complex^{n \times n}$
    and a diagonal matrix $\mat D \in \complex^{n \times n}$ such that
    \begin{equation}
        \label{eq:eigen_decomposition}
        \mat A \mat P = \mat P \mat D.
    \end{equation}
    In this case,
    the diagonal elements of $\mat D$ are called the eigenvalues of~$\mat A$,
    and the columns of $\mat P$ are called the eigenvectors of~$\mat A$.
\end{definition}
Denoting by $\vect e_i$ the $i$-th column of $\mat P$ and by $\lambda_i$ the $i$-th diagonal element of $\mat D$,
we have by~\eqref{eq:eigen_decomposition} that $\mat A \vect e_i = \lambda_i \vect e_i$ or,
equivalently, $(\mat A - \lambda_i \mat I_n) \vect e_i = \vect 0$.
Here $\mat I_n$ is the~$\complex^{n \times n}$ identity matrix.
Therefore, $\lambda$ is an eigenvalue of $\mat A$ if and only if $\det(\mat A - \lambda \mat I_n) = 0$.
In other words, the eigenvalues of $\mat A$ are the roots of $\det(\mat A - \lambda \mat I_n)$,
which is called the \emph{characteristic polynomial}.

\subsection*{Symmetric matrices and spectral theorem}%
The transpose of a matrix $\mat A \in \complex^{m \times n}$ is denoted by $\mat A^\t \in \complex^{n \times m}$
and defined as the matrix with entries~$a^\t_{ij} = a_{ji}$.
The conjugate transpose of $\mat A$ is the matrix obtained by taking the transpose and taking the complex conjugate of all the entries.
A real matrix equal to its transpose is necessarily square and called \emph{symmetric},
and a complex matrix equal to its conjugate transpose is called \emph{Hermitian}.
Hermitian matrices, of which real symmetric matrices are a subset,
enjoy many nice properties,
the main one being that they are diagonalizable with a matrix $\mat P$ that is unitary,
i.e. such that $\mat P^{-1} = \mat P^*$.
This is the content of the \emph{spectral theorem},
a pillar of linear algebra with important generalizations to infinite-dimensional operators.

\begin{theorem}
    [Spectral theorem for Hermitian matrices]
    \label{theorem:spectral_theorem}
    If $\mat A \in \complex^{n \times n}$ is Hermitian,
    then there exists an unitary matrix $\mat Q \in \complex^{n \times n}$ and a diagonal matrix $\mat D \in \real^{n \times n}$ such that
    \[
        \mat A \mat Q = \mat Q \mat D.
    \]
\end{theorem}
\begin{proof}
    [Sketch of the proof]
    The result is trivial for $n = 1$.
    Reasoning by induction,
    we assume that the result is true for Hermitian matrices in $\complex^{n-1 \times n-1}$
    and prove that it then also holds for~$\mat A \in \complex^{n \times n}$.
    % \begin{itemize}
        % \item

            \vspace{.3cm}
            \textbf{Step 1. Existence of a real eigenvalue}.
            By the fundamental theorem of algebra,
            there exists at least one solution $\lambda_1 \in \complex$ to the equation $\det(\mat A - \lambda \mat I_n) = 0$,
            to which there corresponds a solution $\vect q_1 \in \complex^n$ of norm 1 to the equation $(\mat A - \lambda_1 \mat I_n) \vect q_{1} = 0$.
            The eigenvalue $\lambda_1$ is necessarily real because
            \[
                \lambda_1 \ip{\vect e_1, \vect e_1}
                = \ip{\lambda_1 \vect e_1, \vect e_1}
                = \ip{\mat A \vect e_1, \vect e_1}
                = \ip{\vect e_1, \mat A\vect e_1}
                = \ip{\vect e_1, \lambda_1 \vect e_1}
                = \ip{\vect e_1, \lambda_1 \vect e_1}
                = \overline \lambda_1 \ip{\vect e_1, \vect e_1}.
            \]

        % \item
            \vspace{.3cm}
            \textbf{Step 2. Using the induction hypothesis}.
            Next,
            take an orthonormal basis $(\vect e_2, \dotsc, \vect e_n)$ of the orthogonal complement $\Span\{\vect q_1\}^\perp$
            and construct the unitary matrix
            \[
                V = \begin{pmatrix} \vect q_1 & \vect e_2 & \dots & \vect e_n \end{pmatrix},
            \]
            i.e.\ the matrix with columns $\vect q_1$, $\vect e_2$, etc.
            A calculation gives,
            \[
                \mat V^* \mat A \mat V =
                \begin{pmatrix}
                    \ip{\vect q_1, \mat A \vect q_1} & \ip{\vect q_1, \mat A \vect e_2} & \dots & \ip{\vect q_1, \mat A \vect e_n} \\
                    \ip{\vect e_2, \mat A \vect q_1} & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    \ip{\vect e_n, \mat A \vect q_1} & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \lambda_1 & 0 & \dots & 0 \\
                    0 & \ip{\vect e_2, \mat A \vect e_2} & \dots & \ip{\vect e_2, \mat A \vect e_n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & \ip{\vect e_n, \mat A \vect e_2} & \dots & \ip{\vect e_n, \mat A \vect e_n}
                \end{pmatrix}.
            \]
            Let us denote the $n-1 \times n-1$ lower right block of this matrix by $\mat V_{n-1}$.
            This is a Hermitian matrix of size $n-1$ so,
            using the induction hypothesis,
            we deduce that $\mat V_{n-1} = \mat Q_{n-1} \mat D_{n-1} \mat Q_{n-1}^*$
            for appropriate matrices $\mat Q_{n-1} \in \complex^{n-1 \times n-1}$ and $\mat D_{n-1} \in \real^{n-1 \times n-1}$
            which are unitary and diagonal, respectively.

        % \item
            \vspace{.3cm}
            \textbf{Step 3. Constructing $\mat Q$ and $\mat D$}.
            Define now
            \[
                \mat Q =
                \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            It is not difficult to verify that $\mat Q$ is a unitary matrix,
            and we have
            \[
                \mat Q^* \mat A \mat Q =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^*
                \end{pmatrix}
                \mat V^* \mat A \mat V
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}^*
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda_1 & \vect 0^\t \\
                    \vect 0 & \mat V_{n-1}
                \end{pmatrix}
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat Q_{n-1}
                \end{pmatrix}.
            \]
            Developing the last expression, we obtain
            \[
                \mat Q^* \mat A \mat Q
                =
                \begin{pmatrix}
                    1 & \vect 0^\t \\
                    \vect 0 & \mat D_{n-1}
                \end{pmatrix},
            \]
            which concludes the proof.
    % \end{itemize}
\end{proof}

We deduce, as a corollary of the spectral theorem,
that if $\vect e_1$ and $\vect e_2$ are eigenvectors of a Hermitian matrix associated with different eigenvalues,
then they are necessarily orthogonal for the Euclidean inner product.
Indeed, since $\mat A = \mat A^*$ and the eigenvalues are real, it holds that
\[
    (\lambda_1 - \lambda_2) \ip{\vect e_1, \vect e_2}
    = \ip{\lambda_1 \vect e_1, \vect e_2} - \ip{\vect e_1, \lambda_2 \vect e_2}
    = \ip{\mat A \vect e_1, \vect e_2} - \ip{\vect e_1, \mat A \vect e_2}
    = \ip{\mat A \vect e_1, \vect e_2} - \ip{\mat A^* \vect e_1, \vect e_2} = 0.
\]

The largest eigenvalue of a matrix, in modulus,
is called the \emph{spectral radius} and denoted by~$\rho$.
The following result relates the 2-norm of a matrix to the spectral radius of $\mat A \mat A^*$.
\begin{proposition}
    It holds that $\norm{\mat A}_2 = \sqrt{\rho(\mat A^* \mat A)}$.
\end{proposition}
\begin{proof}
    Since $\mat A^* \mat A$ is Hermitian,
    it holds by the spectral theorem that $\mat A^* \mat A = \mat Q \mat D \mat Q^*$ for some unitary matrix $\mat Q$ and real diagonal matrix $\mat D$.
    Therefore, denoting by $(\mu_i)_{1 \leq i \leq n}$ the (positive) diagonal elements of $\mat D$
    and introducing $\vect y := \mat Q^* \vect x$,
    we have
    \begin{equation}
        \label{eq:matrices_link_norm_spectral_radius}
        \norm{\mat A \vect x}
        = \sqrt{\vect x^* \mat A^* \mat A \vect x}
        = \sqrt{\vect x^* \mat Q \mat D \mat Q^* \vect x}
        = \sqrt{\sum_{i=1}^{n} \mu_i y_i^2}
        \leq \sqrt{\rho(\mat A^* \mat A)} \sqrt{\sum_{i=1}^{n} y_i^2}
        =  \sqrt{\rho(\mat A^* \mat A)} \norm{\vect x},
    \end{equation}
    where we used in the last equality the fact that $\vect y$ has the same norm as $\vect x$,
    because $\mat Q$ is unitary.
    It follows from~\eqref{eq:matrices_link_norm_spectral_radius} that $\norm{\mat A} \leq \sqrt{\rho(\mat A^* \mat A)}$,
    and the converse inequality also holds true since $\norm{\mat A \vect x} = \sqrt{\rho(\mat A^* \mat A)} \norm{\vect x}$
    if $\vect x$ is the eigenvector of $\mat A^* \mat A$ corresponding to an eigenvalue of modulus~$\rho(\mat A^* \mat A)$.
\end{proof}

To conclude this section,
we recall and prove the Courant--Fisher theorem.
\begin{theorem}
    [Courant--Fisher Min-Max theorem]
    \label{theorem:courant-fisher}
    The eigenvalues $\lambda_1 \geq \lambda_2 \geq \dotsb \geq \lambda_n$ of a Hermitian matrix are characterized by the relation
    \[
        \lambda_k = \max_{\mathcal S, {\rm dim}(\mathcal S) = k} \left( \min_{\vect x \in \mathcal S \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} \right).
    \]
\end{theorem}
\begin{proof}
    Let $\vect v_1, \dotsc, \vect v_n$ be orthonormalized eigenvectors associated with the eigenvalues $\lambda_1, \dotsc, \lambda_n$.
    Let $\mathcal S_k = \Span \{ \vect v_1, \dotsc, \vect v_k \}$.
    Any $\vect x \in \mathcal S_k$ may be expressed as~$\vect x = \alpha_1 \vect v_1 + \dotsb + \alpha_k \vect v_k$,
    and so
    \[
        \forall \vect x \in \mathcal S_k, \qquad
        \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} =
        \frac{\sum_{i=1}^{k} \lambda_i \abs{\alpha_i}^2}{\sum_{i=1}^{k} \abs{\alpha_i}^2}
        \geq \lambda_k.
    \]
    Therefore,
    it holds that
    \[
         \min_{\vect x \in \mathcal S_k \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}  \geq \lambda_k,
    \]
    which proves the $\geq$ direction.
    For the $\leq$ direction,
    let $\mathcal U_k = \Span \{ \vect v_{k}, \dots \vect v_n \}$.
    Using a well-known result from linear algebra,
    we calculate that, for any subspace $\mathcal S \subset \complex^n$ of dimension~$k$,
    \begin{align*}
        \dim(\mathcal S \cap \mathcal U_k)
        &= \dim(\mathcal S) + \dim(\mathcal U_k) -\dim(\mathcal S + \mathcal U_k) \\
        &=  k + (n - k + 1) - n = 1.
    \end{align*}
    Therefore, any $\mathcal S \subset \complex^n$ of dimension $k$
    has a nonzero intersection with $\mathcal U_k$.
    But since any vector in $\mathcal U_k$ can be expanded as $\beta_1 \vect v_k + \dotsb + \beta_n \vect v_n$,
    we have
    \[
        \forall \vect x \in \mathcal U_k, \qquad
        \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x} =
        \frac{\sum_{i=k}^{n} \lambda_i \abs{\alpha_i}^2}{\sum_{i=k}^{n} \abs{\alpha_i}^2}
        \leq \lambda_k.
    \]
    This shows that
    \[
        \forall \mathcal S \subset \complex^n, \quad \dim(\mathcal S) = k,
        \qquad
         \min_{\vect x \in \mathcal S \backslash\{0\}} \frac{\vect x^* \mat A \vect x}{\vect x^* \vect x}  \leq \lambda_k,
    \]
    which enables to conclude the proof.
\end{proof}

\begin{exercise}
    Prove that if $\mat A \in \real^{n \times n}$ is diagonalizable as in~\eqref{eq:eigen_decomposition},
    then $\mat A^n = \mat P \mat D^n \mat P^{-1}$.
\end{exercise}

\section{Similarity transformation and Jordan normal form}%
\label{sec:similarity_transformation_and_jordan_normal_form}
In this section, we work with matrices in $\complex^{n \times n}$.
A \emph{similarity transformation} is a mapping of the type~$\complex^{n \times n} \ni \mat A \mapsto \mat P^{-1} \mat A \mat P \in \complex^{n \times n}$,
where $\mat P \in \complex^{n \times n}$ is a nonsingular matrix.
If two matrices are related by a similarity transformation,
they are called \emph{similar}.

\begin{definition}
    [Jordan block]
    A Jordan block with dimension $n$ is a matrix of the form
    \[
        \mat J_{n}(\lambda) =
        \begin{pmatrix}
            \lambda & 1  \\
          & \lambda & 1 \\
          &         & \ddots  & \ddots  \\
          &         &         & \lambda & 1  \\
          &         &         &         & \lambda
        \end{pmatrix}
    \]
    The parameter $\lambda \in \complex$ is called the eigenvalue of the Jordan block.
\end{definition}
A Jordan block is diagonalizable if and only if it is of dimension 1.
The only eigenvector of a Jordan block is $\begin{pmatrix} 1 & 0 & \hdots & 0 \end{pmatrix}^\t$.
The power of a Jordan block admits an explicit expression.
\begin{lemma}
    \label{lemma:matrices_power_jordan_block}
    It holds that
    \begin{equation}
        \label{eq:matrices_power_jordan_block}
        \mat J_n(\lambda)^k =
        \begin{pmatrix}
            \lambda^k & \binom{k}{1}\lambda^{k-1} & \binom{k}{2}\lambda^{k-2} & \cdots & \cdots & \binom{k}{n-1}\lambda^{k-n+1} \\
                      & \lambda^k & \binom{k}{1}\lambda^{k-1} & \cdots & \cdots & \binom{k}{n-2}\lambda^{k-n+2} \\
                      &  & \ddots & \ddots &  & \vdots\\
                      &  & & \ddots & \ddots & \vdots\\
                      &  & &  & \lambda^k & \binom{k}{1}\lambda^{k-1}\\
                      &  &  &  &  & \lambda^k
        \end{pmatrix}.
    \end{equation}
    % In addition, for any $\mu > \abs{\lambda}$,
    % there exists $C > 0$ such that
    % \[
    %     \forall k \in \nat, \qquad
    %     \norm{J_{n}(\lambda)^k} \leq C \mu^k.
    % \]
\end{lemma}
\begin{proof}
    The explicit expression of the Jordan block can be obtained by decomposing the block as $\mat J_{n}(\lambda) = \lambda \mat I + \mat N$
    and using the binomial formula:
    \[
        (\lambda \mat I + \mat N)^k =
        \sum_{i=1}^{k} \binom{k}{i} (\lambda \mat I)^{k-i} \mat N^{i}.
    \]
    To conclude the proof,
    we use the fact that $\mat N^{i}$ is a matrix with zeros everywhere except for $i$-th super-diagonal,
    which contains only ones.
    Moreover $\mat N^i = \mat 0_{n \times n}$ if $i \geq n$.
\end{proof}

A matrix is said to be of \emph{Jordan normal form} if it is block-diagonal with Jordan blocks on the diagonal.
In other words, a matrix~$\mat J \in \complex^{n \times n}$ is of Jordan normal form if
\[
    \mat J =
        \begin{pmatrix}
            J_{n_1}(\lambda_1)  \\
                              & J_{n_2}(\lambda_2)    \\
                              &                 & \ddots  \\
                              &                 &         & J_{n_{k-1}}(\lambda_{k-1})   &   \\
                              &                 &         &                              & J_{n_k}(\lambda_k)
    \end{pmatrix}
\]
with $n_1 + \dotsb + n_k = n$.
Note that $\lambda_1, \dotsc, \lambda_k$ are the eigenvalues of $\mat A$.
We state without proof the following important result.

\begin{proposition}
    [Jordan normal form]
    \label{proposition:matrices_jordan_normal_form}
    Any matrix $\mat A \in \complex^{n \times n}$ is \emph{similar} to a matrix in Jordan normal form.
    In other words, there exists an invertible matrix $\mat P \in \complex^{n \times n}$
    and a matrix in normal Jordan form $\mat J \in \complex^{n \times n}$ such that
    \[
        \mat A = \mat P \mat J \mat P^{-1}
    \]
\end{proposition}
% \begin{proof}
%     We prove the result by induction:
%     it is trivial in dimension one,
%     and we assume that it holds true up to dimension $n-1$.
%     By the fundamental theorem of algebra,
%     the matrix $\mat A$ has at least one eigenvalue~$\lambda$.
%     Considering $\mat A - \lambda \mat I$ instead of $\mat A$ if necessary,
%     we can assume without loss of generality that $\lambda = 0$.
%     Let $(\vect k_1, \dotsc, \vect k_{m})$ be a basis of the vector space generated by the generalized eigenvectors associated with $0$,
%     i.e.\ the vectors satisfying $\mat A^i \vect v = 0$ for some $i \geq 0$.
%     Let also~$\mat R$ denote the range of $\mat A - \lambda \mat I$,
%     i.e.\ the vector space generated by the columns of $\mat A - \lambda \mat I$,
%     and $(\vect e_{m+1}, \dotsc, \vect e_{n})$ be a basis of $\mat R$.
%     The subspace $\mat R$ is a stable subspace of $\mat A$,
%     which means that~$\mat A \vect r \in \real$ for all $\vect r \in \mat R$.
%     Therefore
%     \[
%         \mat B^{-1} (\mat A - \lambda \mat I) \mat B =
%         \begin{pmatrix}
%             \mat 0_{m \times m}
%         \end{pmatrix}
%     \]
% \end{proof}

\section{Oldenburger's theorem and Gelfand's formula}
The following result establishes a necessary and sufficient condition,
in terms of the spectral radius of $\mat A$ for the convergence of $\norm{\mat A^k}$ to 0,
for any matrix norm $\norm{\placeholder}$.
\begin{proposition}
    [Oldenburger]
    \label{proposition:matrices_convergence_power_of_matrix}
    Let $\rho(\mat A)$ denote the spectral radius of $\mat A \in \complex^{n \times n}$ and $\norm{\placeholder}$ be a matrix norm.
    Then $\norm{\mat A^k} \to 0$ in the limit as $k \to \infty$ if and only if $\rho(\mat A) < 1$.
    In addition, $\norm{\mat A^k} \to \infty$ in the limit as $k \to \infty$ if and only if $\rho(\mat A) > 1$.
\end{proposition}
\begin{proof}
    Since all matrix norms are equivalent,
    we can assume without loss of generality that~$\norm{\placeholder}$ is the 2-norm.
    We prove only the equivalence $\norm{\mat A^k} \to 0 \Leftrightarrow \rho(\mat A) < 1$.
    The other statement can be proved similarly.

    If $\rho(\mat A) \geq 1$,
    then denoting by $\vect v$ the eigenvector of $\mat A$ corresponding to the eigenvalue with modulus $\rho(\mat A)$,
    we have $\norm{\mat A^k \vect v} = \rho(\mat A)^k \norm{\vect v} \geq \norm{\vect v}$.
    Therefore, the sequence $(\mat A^k)_{k \in \nat}$ does not converge to the zero matrix in the $2$-norm.
    This shows the implication $\norm{\mat A^k} \to 0 \Rightarrow \rho(\mat A) < 1$.

    To show the converse implication,
    we employ~\cref{proposition:matrices_jordan_normal_form},
    which states that there exists a nonsingular matrix $\mat P$ such that $\mat A = \mat P \mat J \mat P^{-1}$,
    for a matrix $\mat J \in \complex^{n \times n}$ which is in normal Jordan form.
    It holds that $\mat A^k = \mat P \mat J^k \mat P^{-1}$,
    and so it is sufficient to show that $\norm{\mat J^k} \to 0$.
    The latter convergence follows from the expression of the power of a Jordan block given in~\cref{lemma:matrices_power_jordan_block}.
\end{proof}

With this result,
we can prove Gelfand's formula,
which relates the spectral radius to the asymptotic growth of $\norm{\mat A^k}$,
and is used in \cref{cha:solution_of_linear_systems}.
\begin{proposition}
    [Gelfand's formula]
    \label{proposition:matrices_gelfands}
    Let $\mat A \in \complex^{n \times n}$.
    It holds for any norm that
    \[
        \lim_{k \to \infty} \norm{\mat A^k}^{\frac{1}{k}} = \rho(\mat A)
    \]
\end{proposition}
\begin{proof}
    Let $0 < \varepsilon < \rho(\mat A)$
    and define $\mat A^+ = \frac{\mat A}{\rho(\mat A) + \varepsilon}$ and $\mat A^- = \frac{\mat A}{\rho(\mat A) - \varepsilon}$.
    It holds by construction that~$\rho(\mat A^+) < 1$ and $\rho(\mat A^-) > 1$.
    Using~\cref{proposition:matrices_convergence_power_of_matrix}, we deduce that
    \[
        \lim_{k \to \infty} \norm{(\mat A^+)^k} = 0, \qquad \lim_{k \to \infty} \norm{(\mat A^-)^k} = \infty.
    \]
    In particular,
    there exists $K(\varepsilon) \in \nat$ such that
    \[
        \forall k \geq K(\varepsilon), \qquad
        \norm{(\mat A^+)^k} = \frac{\norm{\mat A^k}}{(\rho(A) + \varepsilon)^k} \leq 1,
        \qquad \norm{(\mat A^-)^k} = \frac{\norm{\mat A^k}}{(\rho(A) - \varepsilon)^k}  \geq 1.
    \]
    Taking the $k$-th root and rearranging these equations,
    we obtain
    \[
        \forall k \geq K(\varepsilon), \qquad
        \rho(A) - \varepsilon \leq \norm{\mat A^k}^{\frac{1}{k}} \leq \rho(A) + \varepsilon.
    \]
    Since $\varepsilon$ was arbitrary,
    this implies the statement.
\end{proof}
