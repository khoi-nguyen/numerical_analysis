\chapter{Solution of linear systems of equation}
\label{cha:solution_of_linear_systems}
This chapter is devoted to the numerical solution of linear problems of the following form:
\begin{equation}
    \label{eq:linear_system}
    \text{Find $\vect x \in \real^n$ such that} \qquad
    \mat A \vect x = \vect b,
    \qquad \mat A \in \real^{n \times n},
    \qquad \vect b \in \real^n.
\end{equation}
Systems of this type appear in a variety of applications.
They naturally arise in the context of linear partial differential equations,
which we use as main motivating example.
Partial differential equations govern a wide range of physical phenomena including heat propagation, gravity, and electromagnetism,
to mention just a few.
Linear systems in this context often have a particular structure:
the matrix $\mat A$ is generally very sparse,
which means that most of the entries are equal to 0,
and it is often symmetric and positive definite,
provided that these properties are satisfied by the underlying operator.

There are two main approaches for solving linear systems:
\begin{itemize}
    \item
        Direct methods enable to calculate the exact solution to systems of linear equations,
        up to round-off errors.
        Although this is an attractive property,
        they are usually too computationally costly for large systems:
        The cost of inverting a general $n \times n$ matrix,
        measured in number of floating operations,
        scales as $n^3$!

    \item
        Iterative methods, on the other hand,
        enable to progressively calculate increasingly accurate approximations of the solution.
        Iterations may be stopped once the \emph{the residue} is sufficiently small.
        These methods are often preferable when the dimension $n$ of the linear system is very large.
\end{itemize}

This section is organized as follows.
\begin{itemize}
    \item
        In \cref{sec:conditioning},
        we introduce the concept of \emph{conditioning}.
        The condition number of a matrix provides information on the sensitivity of the solution to perturbations of the right-hand side $\vect b$ or matrix $\mat A$.
        It is useful, for example, in order to determine the potential impact of round-off errors.
\end{itemize}

\section{Conditioning}%
\label{sec:conditioning}

The condition number for a given problem measures the sensitivity of the solution to some input data.
In order to define this concept precisely,
we consider a general problem of the form~$F(x, d) = 0$,
with unknown $x$ and data $d$.
The linear system~\eqref{eq:linear_system} can be recast in this form,
with the input data equal to $\vect b$ or $\mat A$ or both.
We denote the solution corresponding to a perturbed input data $d + \Delta d$ by $x + \Delta x$.
For the general problem,
we define the absolute and relative condition numbers as follows.

\begin{definition}
    [Condition number for abstract problem]
    The absolute and relative condition numbers with respect to perturbations of $d$ are defined as
    \[
        K_{\rm abs}(d) = \lim_{\varepsilon \to 0} \left( \sup_{\norm{\Delta d} \leq \varepsilon} \frac{\norm{\Delta x}}{\norm{\Delta d}} \right),
        \qquad
        K(d) = \lim_{\varepsilon \to 0} \left( \sup_{\norm{\Delta d} \leq \varepsilon} \frac{\norm{\Delta x} / \norm{x}}{\norm{\Delta d} / \norm{d}} \right).
    \]
    The short notation $K$ is reserved for the relative condition number,
    which is often more useful in applications.
\end{definition}

In the rest of this section,
we obtain an upper bound on the relative condition number of the linear system~\eqref{eq:linear_system} with respect to perturbations first of $\vect b$,
and then of $\mat A$.
We use the notation~$\norm{\placeholder}$ to denote both a vector norm on $\real^n$ and the induced operator norm on matrices.

\begin{proposition}
    [Perturbation of the right-hand side]
    \label{proposition:linear_perturbation_rhs}
    Let $\vect x + \Delta \vect x$ denote the solution to the perturbed equation $\mat A (\vect x + \Delta \vect x) = \vect b + \Delta \vect b$.
    Then it holds
    \begin{equation}
        \label{eq:linear_perturbation_rhs}
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}} \leq \norm{\mat A} \norm{\mat A^{-1}} \, \frac{\norm{\Delta \vect b}}{\norm{\vect b}},
    \end{equation}
\end{proposition}
\begin{proof}
    It holds by definition that $\mat A \Delta \vect x = \Delta \vect b$.
    Therefore, we have
    \begin{equation}
        \label{eq:linear_perturbation_rhs_to_rearrange}
        \norm{\Delta \vect x}
        = \norm{\mat A^{-1} \Delta \vect b}
        \leq \norm{\mat A^{-1}} \norm{\Delta \vect b}
        = \frac{\norm{\mat A \vect x}}{\norm{\vect b}} \norm{\mat A^{-1}} \norm{\Delta \vect b}
        \leq \frac{\norm{\mat A} \norm{\vect x}}{\norm{\vect b}} \norm{\mat A^{-1}} \norm{\Delta \vect b}.
    \end{equation}
    Here we employed~\eqref{eq:submultiplicative_mat_vec},
    proved in \cref{cha:vectors_and_matrices},
    in the first and last inequalities.
    Rearranging the inequality~\eqref{eq:linear_perturbation_rhs_to_rearrange},
    we obtain~\eqref{eq:linear_perturbation_rhs}.
\end{proof}
\Cref{proposition:linear_perturbation_rhs} shows that
the relative condition number of~\eqref{eq:linear_system} with respect to perturbations of the right-hand side is bounded from above by $\norm{\mat A} \norm{\mat A^{-1}}$.
\Cref{exercise:linear_sharp_inequality} shows that there are values of $\vect x$ and $\Delta \vect b$ for which the inequality~\eqref{eq:linear_perturbation_rhs} is sharp.

Studying the impact of perturbations of the matrix~$\mat A$ is slightly more difficult,
because this time the variation of the solution~$\Delta \vect x$ does not depend linearly on the perturbation.
\begin{proposition}
    [Perturbation of the matrix]
    \label{proposition:linear_perturbation_matrix}
    Let $\vect x + \Delta \vect x$ denote the solution to the perturbed equation $(\mat A + \Delta \mat A) (\vect x + \Delta \vect x) = \vect b$.
    If $\mat A$ is invertible and $\norm{\Delta \mat A} < \norm{\mat A^{-1}}^{-1}$,
    then
    \begin{equation}
        \label{eq:linear_perturbation_matrix}
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}}
        \leq \norm{\mat A} \norm{\mat A^{-1}} \frac{\norm{\Delta \mat A}}{\norm{\mat A}}
        \left(\frac{1}{1 - \norm{\mat A^{-1} \Delta \mat A}} \right).
    \end{equation}
\end{proposition}
Before proving this result,
we show the following ancillary lemma.
\begin{lemma}
    \label{lemma:linear_inverse_neumann}
    Let $\mat B \in \real^{n \times n}$ be such that $\norm{\mat B} \leq 1$.
    Then $\mat I - \mat B$ is invertible and
    \begin{equation}
        \label{eq:linear_bound_inverse_perturbation_identity}
        \norm{(\mat I - \mat B)^{-1}}
        \leq \frac{1}{1 - \norm{\mat B}},
    \end{equation}
    where $\mat I \in \real^{n \times n}$ is the identity matrix.
\end{lemma}
\begin{proof}
    It holds for any matrix $\mat B \in \real^{n \times n}$ that
    \[
        \mat I - \mat B^{n+1} = (\mat I - \mat B)(\mat I + \mat B + \dotsb + \mat B^n)
    \]
    Since $\norm{\mat B} < 1$ in a submultiplicative matrix norm,
    both sides of the equation are convergent in the limit as $n \to \infty$,
    with the left-hand side converging to identity matrix $\mat I$.
    Equating the limits,
    we obtain
    \[
        \mat I = (\mat I - \mat B) \sum_{i=0}^{\infty} \mat B^i.
    \]
    This implies that $(\mat I - \mat B)$ is invertible with inverse
    given by the so-called \emph{Neumann} series
    \begin{equation*}
        (\mat I - \mat B)^{-1} = \sum_{i=0}^{\infty} \mat B^i.
    \end{equation*}
    Applying the triangle inequality repeatedly,
    and then using the submultiplicative property of the norm,
    we obtain
    \[
        \forall n \in \nat,
        \qquad
        \norm*{\sum_{i=0}^{n} \mat B^i}
        \leq \sum_{i=0}^{n} \norm{\mat B^i}
        \leq \sum_{i=0}^{n} \norm{\mat B}^i
        = \frac{1}{1 - \norm{\mat B}}.
    \]
    where we used the summation formula for geometric series in the last equality.
    Letting $n \to \infty$ in this equation and
    using the continuity of the norm enables to conclude the proof.
\end{proof}

\begin{proof}
    [Proof of \cref{proposition:linear_perturbation_matrix}]
    Left-multiplying both side by $\mat A^{-1}$,
    we obtain
    \begin{equation}
        \label{eq:linear_perturbation_matrix_initial}
        (\mat I + \mat A^{-1} \Delta \mat A) (\vect x + \Delta \vect x) = \vect x
        \quad \Leftrightarrow \quad
        (\mat I + \mat A^{-1} \Delta \mat A) \Delta \vect x = - \mat A^{-1} \Delta \mat A \vect x.
    \end{equation}
    Since $\norm{\mat A^{-1} \Delta \mat A} \leq \norm{\mat A^{-1}} \norm{\Delta \mat A} < 1$ by assumption,
    we deduce~\cref{lemma:linear_inverse_neumann} that the matrix on the left-hand side is invertible
    with a norm bounded as in~\eqref{eq:linear_bound_inverse_perturbation_identity}.
    Consequently,
    using in addition the assumed submultiplicative property of the norm,
    we deduce that
    \[
        \norm{\Delta \vect x}
        = \norm{(\mat I + \mat A^{-1} \Delta \mat A)^{-1} \mat A^{-1} \Delta \mat A \vect x}
        \leq \frac{\norm{\mat A^{-1} \Delta \mat A}}{1 - \norm{\mat A^{-1} \Delta \mat A}} \norm{\vect x}.
    \]
    which enables to conclude the proof.
\end{proof}
Using \cref{proposition:linear_perturbation_matrix},
we deduce that the relative condition number of~\eqref{eq:linear_system} with respect to perturbations of the matrix $\mat A$ is also bounded from above by $\norm{\mat A} \norm{\mat A^{-1}}$,
because the term between brackets on the right-hand side of~\eqref{eq:linear_perturbation_matrix} converges to 1 as $\norm{\Delta \mat A} \to 0$.


\Cref{proposition:linear_perturbation_rhs,proposition:linear_perturbation_matrix} show that
the condition number, with respect to perturbations of either~$\vect b$ or~$\mat A$,
depends only on $\mat A$.
This motivates the following definition.
\begin{definition}
    [Condition number of a matrix]
    The condition number of a matrix $\mat A$ associated to a vector norm $\norm{\placeholder}$ is defined as
    \[
        \kappa(\mat A) = \norm{\mat A} \norm{\mat A^{-1}}.
    \]
    The condition number for the vector $p$-norm,
    defined in \cref{definition:pnorm_vector},
    is denoted by $\kappa_p(\mat A)$.
\end{definition}
Since the 2-norm of an invertible matrix $\mat A \in \real^{n \times n}$ coincides with the spectral radius $\rho(\mat A^\t \mat A)$,
the condition number $\kappa_2$ corresponding to the $2$-norm is equal to
\[
    \kappa_2(\mat A) = \frac{\lambda_{\max}(\mat A^\t \mat A)}{\lambda_{\min}(\mat A^\t \mat A)},
\]
where $\lambda_{\max}$ and $\lambda_{\min}$ are the maximal and minimal (both real and positive) eigenvalues of $\mat A$.
\begin{example}
    [Perturbation of the matrix]
    Consider the following linear system
    with perturbed matrix
    \[
        (\mat A + \Delta \mat A)
        \begin{pmatrix}
            x_1 \\
            x_2
        \end{pmatrix}
        = \begin{pmatrix}
            0 \\
            .01
        \end{pmatrix},
        \qquad
        \mat A
        = \begin{pmatrix}
            1 & 0 \\
            0 & .01
        \end{pmatrix},
        \qquad
        \Delta \mat A =
        \begin{pmatrix}
            0 & 0 \\
            0 & \varepsilon
        \end{pmatrix},
    \]
    where $0 < \varepsilon \ll .01$.
    Here the eigenvalues of $\mat A$ are given by $\lambda_1 = 1$ and $\lambda_2 = 0.01$.
    The solution when $\varepsilon = 0$ is given by $(0, 1)^\t$,
    and the solution to the perturbed equation is
    \[
        \begin{pmatrix}
        x_1 + \Delta x_1 \\
        x_2 + \Delta x_2
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 \\
            \frac{1}{1 + 100 \varepsilon}
        \end{pmatrix}.
    \]
    Consequently, we deduce that
    \[
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}}
        = \abs*{\frac{100 \varepsilon}{1 + 100 \varepsilon}}
        \approx 100 \varepsilon
        = 100 \frac{\norm{\Delta \mat A}}{\norm{\mat A}}.
    \]
    In this case,
    the relative impact of perturbations of the matrix is close to $\kappa_2(\mat A) = 100$.
\end{example}

\begin{exercise}
    \label{exercise:linear_sharp_inequality}
    In the simple case where $\mat A$ is symmetric,
    find values of $\vect x$, $\vect b$ and $\Delta \vect b$ for which the inequality~\eqref{eq:linear_perturbation_rhs} is in fact an equality?
\end{exercise}

\section{Direct solution method}%
\label{sec:direct_solution_method}
In this section,
we present the \emph{direct method} for solving linear systems.
For the linear system~\eqref{eq:linear_system} with general invertible matrix~$\mat A \in \real^{n \times n}$,
the direct method for solving can be decomposed in three steps:
\begin{itemize}
    \item
        First calculate the so-called $\mat L \mat U$ decomposition of $\mat A$,
        i.e.\ find an lower triangular matrix $\mat U$ and a lower triangular matrix $\mat L$ such that
        \(
            \mat A = \mat L \mat U.
        \)

    \item
        Then solve
        \(
            \mat L \vect y = \vect b.
        \)
         using a method called \emph{backward substitution}.

    \item
        Finally, solve
        \(
            \mat U \vect x = \vect y.
        \)
         using a method called \emph{forward substitution}.
\end{itemize}

\subsection{LU decomposition}%
\label{sub:lu_decomposition}

We begin by introducing the concept of Gaussian transformation.
\begin{definition}
    A Gaussian transformation is a matrix of the form $\mat M_k = \mat I + \mat C_k$,
    where $\mat C_k$ is a rank one matrix with all columns equal to 0 except for the $k$-th one,
    which is required of the form
    \[
        \begin{pmatrix}
            0 & 0 & \dots & 0 & c_{k+1} & c_{k+2} & \dots & c_n
        \end{pmatrix}^\t.
    \]
\end{definition}

The action of a Gaussian transformation left-multiplying a matrix $\mat A \in \real^{n \times n}$ is
to replace the rows from index $k + 1$ to index $n$ by a linear combination involving themselves and the $k$-th row.
To see this, let us denote by $(\vect r_i)_{1 \leq i \leq n}$ the lines of $\mat A$.
Then, we have
\[
    \mat M_k \mat A
    = (\mat I + \mat R_k) \mat A
    =
    \begin{pmatrix}
        1   \\
      & 1  \\
         & &  \ddots \\
        & & & 1 & & & \\
        & & & c_{k+1} & 1  \\
        & & & \vdots & & \ddots \\
        & & & c_n & & & 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        \vect r_1 \\
        \vect r_2 \\
        \vdots \\
        \vect r_k \\
        \vect r_{k+1} \\
        \vdots \\
        \vect r_{n}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \vect r_1 \\
        \vect r_2 \\
        \vdots \\
        \vect r_k \\
        \vect r_{k+1} + c_{k+1} \vect r_k \\
        \vdots \\
        \vect r_{n} + c_{n} \vect r_n
    \end{pmatrix}
\]
We show in \cref{exercise:inverse_gaussian_transformation} that
the inverse of a Gaussian transformation matrix is given by
\begin{equation}
    \label{eq:inverse_gaussian_transformation}
    (\mat I + \mat R_k)^{-1} = \mat I - \mat R_k.
\end{equation}
The idea of the $\mat L \mat U$ decomposition algorithm is to successively left-multiply $\mat A$
by Gaussian transformation matrices $\mat M_1$, then $\mat M_2$, etc.\
appropriately chosen in such a way that all the under-diagonal entries in columns 1 to $k$ of
the matrix $\mat A_k$,
obtained after $k$ iterations,
are equal to zero.
The resulting matrix $\mat A_n$ after $n$ iterations is then upper triangular,
and by construction
\[
    \mat M_n \dotsc \mat M_1 \mat A = \mat A_n.
\]
Using~\eqref{eq:inverse_gaussian_transformation},
we deduce that
\[
    \mat A = (\mat M_n^{-1} \dots \mat M_1^{-1}) \mat A_n.
\]
Since the first factor is lower triangular by~\eqref{eq:inverse_gaussian_transformation} and \cref{exercise:linear_product_of_lower_triangular},
this completes the $\mat L \mat U$ factorization of the matrix $\mat A$.
Of course, the success of this strategy hinges on the existence of an appropriate Gaussian transformation at each iteration.
It is not difficult to show that,
provided that the diagonal entries of $A$ are all nonzero,
the transformation matrices exist and are uniquely defined.
We assume this for now.

We assume for simplicity that the diagonal elements of $A$ are nonzero.
The $\mat L \mat U$ decomposition procedure can be summarized as follows:
\begin{algorithmic}
\State $\mat A_0 \gets \mat A, \mat M \gets \mat I$
\For{$i \in \{1, \dotsc, n\}$}
    \State Construct $\mat M_i$ such that $\mat M_i \mat A_{i-1}$ has only zeros under the diagonal of the $i$-th column.
    \State $\mat A_i \gets \mat M_i \mat A_{i-1}, \mat M \gets  \mat M_i \mat M$
\EndFor
\State $\mat U \gets A_n, \mat L \gets M^\t$.
\end{algorithmic}

\begin{exercise}
    [Inverse of Gaussian transformation]
    \label{exercise:inverse_gaussian_transformation}
    Show the formula~\eqref{eq:inverse_gaussian_transformation}.
\end{exercise}

\begin{exercise}
    \label{exercise:linear_product_of_lower_triangular}
    Show that the product of lower triangular matrices is lower triangular.
\end{exercise}

\textcolor{red}{WORK IN PROGRESS}
