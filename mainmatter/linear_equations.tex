\chapter{Solution of linear systems of equation}
\label{cha:solution_of_linear_systems}
\minitoc

This chapter is devoted to the numerical solution of linear problems of the following form:
\begin{equation}
    \label{eq:linear_system}
    \text{Find $\vect x \in \real^n$ such that} \qquad
    \mat A \vect x = \vect b,
    \qquad \mat A \in \real^{n \times n},
    \qquad \vect b \in \real^n.
\end{equation}
Systems of this type appear in a variety of applications.
They naturally arise in the context of linear partial differential equations,
which we use as main motivating example.
Partial differential equations govern a wide range of physical phenomena including heat propagation, gravity, and electromagnetism,
to mention just a few.
Linear systems in this context often have a particular structure:
the matrix $\mat A$ is generally very sparse,
which means that most of the entries are equal to 0,
and it is often symmetric and positive definite,
provided that these properties are satisfied by the underlying operator.

There are two main approaches for solving linear systems:
\begin{itemize}
    \item
        Direct methods enable to calculate the exact solution to systems of linear equations,
        up to round-off errors.
        Although this is an attractive property,
        direct methods are usually too computationally costly for large systems:
        The cost of inverting a general $n \times n$ matrix,
        measured in number of floating operations,
        scales as $n^3$!

    \item
        Iterative methods, on the other hand,
        enable to progressively calculate increasingly accurate approximations of the solution.
        Iterations may be stopped once the \emph{the residue} is sufficiently small.
        These methods are often preferable when the dimension $n$ of the linear system is very large.
\end{itemize}

This chapter is organized as follows.
\begin{itemize}
    \item
        In \cref{sec:conditioning},
        we introduce the concept of \emph{conditioning}.
        The condition number of a matrix provides information on the sensitivity of the solution to perturbations of the right-hand side $\vect b$ or matrix $\mat A$.
        It is useful, for example, in order to determine the potential impact of round-off errors.

    \item
        In \cref{sec:direct_solution_method},
        we present the direct method for solving systems of linear equations.
        We study in particular the $\mat L \mat U$ decomposition for an invertible matrix,
        as well as its variant for symmetric positive definite matrices,
        which is called the Cholesky decomposition.
\end{itemize}

\section{Conditioning}%
\label{sec:conditioning}

The condition number for a given problem measures the sensitivity of the solution to the input data.
In order to define this concept precisely,
we consider a general problem of the form~$F(x, d) = 0$,
with unknown $x$ and data $d$.
The linear system~\eqref{eq:linear_system} can be recast in this form,
with the input data equal to $\vect b$ or $\mat A$ or both.
We denote the solution corresponding to perturbed input data $d + \Delta d$ by $x + \Delta x$.
The absolute and relative condition numbers are defined as follows.

\begin{definition}
    [Condition number for the problem $F(x, d) = 0$]
    The absolute and relative condition numbers with respect to perturbations of $d$ are defined as
    \[
        K_{\rm abs}(d) = \lim_{\varepsilon \to 0} \left( \sup_{\norm{\Delta d} \leq \varepsilon} \frac{\norm{\Delta x}}{\norm{\Delta d}} \right),
        \qquad
        K(d) = \lim_{\varepsilon \to 0} \left( \sup_{\norm{\Delta d} \leq \varepsilon} \frac{\norm{\Delta x} / \norm{x}}{\norm{\Delta d} / \norm{d}} \right).
    \]
    The short notation $K$ is reserved for the relative condition number,
    which is often more useful in applications.
\end{definition}

In the rest of this section,
we obtain an upper bound on the relative condition number for the linear system~\eqref{eq:linear_system} with respect to perturbations first of $\vect b$,
and then of $\mat A$.
We use the notation~$\norm{\placeholder}$ to denote both a vector norm on $\real^n$ and the induced operator norm on matrices.

\begin{proposition}
    [Perturbation of the right-hand side]
    \label{proposition:linear_perturbation_rhs}
    Let $\vect x + \Delta \vect x$ denote the solution to the perturbed equation $\mat A (\vect x + \Delta \vect x) = \vect b + \Delta \vect b$.
    Then it holds that
    \begin{equation}
        \label{eq:linear_perturbation_rhs}
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}} \leq \norm{\mat A} \norm{\mat A^{-1}} \, \frac{\norm{\Delta \vect b}}{\norm{\vect b}},
    \end{equation}
\end{proposition}
\begin{proof}
    It holds by definition of $\Delta \vect x$ that $\mat A \Delta \vect x = \Delta \vect b$.
    Therefore, we have
    \begin{equation}
        \label{eq:linear_perturbation_rhs_to_rearrange}
        \norm{\Delta \vect x}
        = \norm{\mat A^{-1} \Delta \vect b}
        \leq \norm{\mat A^{-1}} \norm{\Delta \vect b}
        = \frac{\norm{\mat A \vect x}}{\norm{\vect b}} \norm{\mat A^{-1}} \norm{\Delta \vect b}
        \leq \frac{\norm{\mat A} \norm{\vect x}}{\norm{\vect b}} \norm{\mat A^{-1}} \norm{\Delta \vect b}.
    \end{equation}
    Here we employed~\eqref{eq:submultiplicative_mat_vec},
    proved in \cref{cha:vectors_and_matrices},
    in the first and last inequalities.
    Rearranging the inequality~\eqref{eq:linear_perturbation_rhs_to_rearrange},
    we obtain~\eqref{eq:linear_perturbation_rhs}.
\end{proof}
\Cref{proposition:linear_perturbation_rhs} implies that
the relative condition number of~\eqref{eq:linear_system} with respect to perturbations of the right-hand side is bounded from above by $\norm{\mat A} \norm{\mat A^{-1}}$.
\Cref{exercise:linear_sharp_inequality} shows that there are values of $\vect x$ and $\Delta \vect b$ for which the inequality~\eqref{eq:linear_perturbation_rhs} is sharp.

Studying the impact of perturbations of the matrix~$\mat A$ is slightly more difficult,
because this time the variation~$\Delta \vect x$ of the solution does not depend linearly on the perturbation.
\begin{proposition}
    [Perturbation of the matrix]
    \label{proposition:linear_perturbation_matrix}
    Let $\vect x + \Delta \vect x$ denote the solution to the perturbed equation $(\mat A + \Delta \mat A) (\vect x + \Delta \vect x) = \vect b$.
    If $\mat A$ is invertible and $\norm{\Delta \mat A} < \norm{\mat A^{-1}}^{-1}$,
    then
    \begin{equation}
        \label{eq:linear_perturbation_matrix}
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}}
        \leq \norm{\mat A} \norm{\mat A^{-1}} \frac{\norm{\Delta \mat A}}{\norm{\mat A}}
        \left(\frac{1}{1 - \norm{\mat A^{-1} \Delta \mat A}} \right).
    \end{equation}
\end{proposition}
Before proving this result,
we show an ancillary lemma.
\begin{lemma}
    \label{lemma:linear_inverse_neumann}
    Let $\mat B \in \real^{n \times n}$ be such that $\norm{\mat B} \leq 1$.
    Then $\mat I - \mat B$ is invertible and
    \begin{equation}
        \label{eq:linear_bound_inverse_perturbation_identity}
        \norm{(\mat I - \mat B)^{-1}}
        \leq \frac{1}{1 - \norm{\mat B}},
    \end{equation}
    where $\mat I \in \real^{n \times n}$ is the identity matrix.
\end{lemma}
\begin{proof}
    It holds for any matrix $\mat B \in \real^{n \times n}$ that
    \[
        \mat I - \mat B^{n+1} = (\mat I - \mat B)(\mat I + \mat B + \dotsb + \mat B^n).
    \]
    Since $\norm{\mat B} < 1$ in a submultiplicative matrix norm,
    both sides of the equation are convergent in the limit as $n \to \infty$,
    with the left-hand side converging to identity matrix $\mat I$.
    Equating the limits,
    we obtain
    \[
        \mat I = (\mat I - \mat B) \sum_{i=0}^{\infty} \mat B^i.
    \]
    This implies that $(\mat I - \mat B)$ is invertible with inverse
    given by a so-called \emph{Neumann} series
    \begin{equation*}
        (\mat I - \mat B)^{-1} = \sum_{i=0}^{\infty} \mat B^i.
    \end{equation*}
    Applying the triangle inequality repeatedly,
    and then using the submultiplicative property of the norm,
    we obtain
    \[
        \forall n \in \nat,
        \qquad
        \norm*{\sum_{i=0}^{n} \mat B^i}
        \leq \sum_{i=0}^{n} \norm{\mat B^i}
        \leq \sum_{i=0}^{n} \norm{\mat B}^i
        = \frac{1}{1 - \norm{\mat B}}.
    \]
    where we used the summation formula for geometric series in the last equality.
    Letting $n \to \infty$ in this equation and
    using the continuity of the norm enables to conclude the proof.
\end{proof}

\begin{proof}
    [Proof of \cref{proposition:linear_perturbation_matrix}]
    Left-multiplying both side by $\mat A^{-1}$,
    we obtain
    \begin{equation}
        \label{eq:linear_perturbation_matrix_initial}
        (\mat I + \mat A^{-1} \Delta \mat A) (\vect x + \Delta \vect x) = \vect x
        \quad \Leftrightarrow \quad
        (\mat I + \mat A^{-1} \Delta \mat A) \Delta \vect x = - \mat A^{-1} \Delta \mat A \vect x.
    \end{equation}
    Since $\norm{\mat A^{-1} \Delta \mat A} \leq \norm{\mat A^{-1}} \norm{\Delta \mat A} < 1$ by assumption,
    we deduce from~\cref{lemma:linear_inverse_neumann} that the matrix on the left-hand side is invertible
    with a norm bounded as in~\eqref{eq:linear_bound_inverse_perturbation_identity}.
    Consequently,
    using in addition the assumed submultiplicative property of the norm,
    we obtain that
    \[
        \norm{\Delta \vect x}
        = \norm{(\mat I + \mat A^{-1} \Delta \mat A)^{-1} \mat A^{-1} \Delta \mat A \vect x}
        \leq \frac{\norm{\mat A^{-1} \Delta \mat A}}{1 - \norm{\mat A^{-1} \Delta \mat A}} \norm{\vect x}.
    \]
    which enables to conclude the proof.
\end{proof}
Using \cref{proposition:linear_perturbation_matrix},
we deduce that the relative condition number of~\eqref{eq:linear_system} with respect to perturbations of the matrix $\mat A$ is also bounded from above by $\norm{\mat A} \norm{\mat A^{-1}}$,
because the term between brackets on the right-hand side of~\eqref{eq:linear_perturbation_matrix} converges to 1 as $\norm{\Delta \mat A} \to 0$.


\Cref{proposition:linear_perturbation_rhs,proposition:linear_perturbation_matrix} show that
the condition number, with respect to perturbations of either~$\vect b$ or~$\mat A$,
depends only on $\mat A$.
This motivates the following definition.
\begin{definition}
    [Condition number of a matrix]
    The condition number of a matrix $\mat A$ associated to a vector norm $\norm{\placeholder}$ is defined as
    \[
        \kappa(\mat A) = \norm{\mat A} \norm{\mat A^{-1}}.
    \]
    The condition number for the $p$-norm,
    defined in \cref{definition:pnorm_vector},
    is denoted by $\kappa_p(\mat A)$.
\end{definition}
Since the 2-norm of an invertible matrix $\mat A \in \real^{n \times n}$ coincides with the spectral radius $\rho(\mat A^\t \mat A)$,
the condition number $\kappa_2$ corresponding to the $2$-norm is equal to
\[
    \kappa_2(\mat A) = \frac{\lambda_{\max}(\mat A^\t \mat A)}{\lambda_{\min}(\mat A^\t \mat A)},
\]
where $\lambda_{\max}$ and $\lambda_{\min}$ are the maximal and minimal (both real and positive) eigenvalues of $\mat A$.
\begin{example}
    [Perturbation of the matrix]
    Consider the following linear system
    with perturbed matrix
    \[
        (\mat A + \Delta \mat A)
        \begin{pmatrix}
            x_1 \\
            x_2
        \end{pmatrix}
        = \begin{pmatrix}
            0 \\
            .01
        \end{pmatrix},
        \qquad
        \mat A
        = \begin{pmatrix}
            1 & 0 \\
            0 & .01
        \end{pmatrix},
        \qquad
        \Delta \mat A =
        \begin{pmatrix}
            0 & 0 \\
            0 & \varepsilon
        \end{pmatrix},
    \]
    where $0 < \varepsilon \ll .01$.
    Here the eigenvalues of $\mat A$ are given by $\lambda_1 = 1$ and $\lambda_2 = 0.01$.
    The solution when $\varepsilon = 0$ is given by $(0, 1)^\t$,
    and the solution to the perturbed equation is
    \[
        \begin{pmatrix}
        x_1 + \Delta x_1 \\
        x_2 + \Delta x_2
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 \\
            \frac{1}{1 + 100 \varepsilon}
        \end{pmatrix}.
    \]
    Consequently, we deduce that, in the 2-norm,
    \[
        \frac{\norm{\Delta \vect x}}{\norm{\vect x}}
        = \abs*{\frac{100 \varepsilon}{1 + 100 \varepsilon}}
        \approx 100 \varepsilon
        = 100 \frac{\norm{\Delta \mat A}}{\norm{\mat A}}.
    \]
    In this case,
    the relative impact of perturbations of the matrix is close to $\kappa_2(\mat A) = 100$.
\end{example}

\begin{exercise}
    \label{exercise:linear_sharp_inequality}
    In the simple case where $\mat A$ is symmetric,
    find values of $\vect x$, $\vect b$ and $\Delta \vect b$ for which the inequality~\eqref{eq:linear_perturbation_rhs} is in fact an equality?
\end{exercise}

\section{Direct solution method}%
\label{sec:direct_solution_method}
In this section,
we present the \emph{direct method} for solving linear systems of the form~\eqref{eq:linear_system}
with a general invertible matrix~$\mat A \in \real^{n \times n}$.
The direct method can be decomposed into three steps:
\begin{itemize}
    \item
        First calculate the so-called $\mat L \mat U$ decomposition of $\mat A$,
        i.e.\ find an upper triangular matrix~$\mat U$ and a lower triangular matrix~$\mat L$ such that
        \(
            \mat A = \mat L \mat U.
        \)

    \item
        Then solve
        \(
            \mat L \vect y = \vect b.
        \)
         using a method called \emph{forward substitution}.

    \item
        Finally, solve
        \(
            \mat U \vect x = \vect y.
        \)
         using a method called \emph{backward substitution}.
         By construction, the solution $\vect x$ thus obtained is a solution to~\eqref{eq:linear_system}.
\end{itemize}

\subsection{LU decomposition}%
\label{sub:lu_decomposition}

We begin by introducing the concept of \emph{Gaussian transformation}.
\begin{definition}
    A Gaussian transformation is a matrix of the form $\mat M_k = \mat I - \vect c^{(k)} \vect e_k$,
    where~$\vect e_k$ is the column vector with entry at index $k$ equal to 1 and all the other entries equal to zero,
    and $\vect c^{(k)}$ is a column vector of the following form:
    \[
        \vect c^{(k)} =
        \begin{pmatrix}
            0 & 0 & \dots & 0 & c^{(k)}_{k+1} & c^{(k)}_{k+2} & \dots & c^{(k)}_n
        \end{pmatrix}^\t.
    \]
\end{definition}

The action of a Gaussian transformation left-multiplying a matrix $\mat A \in \real^{n \times n}$ is
to replace the rows from index $k + 1$ to index $n$ by a linear combination involving themselves and the $k$-th row.
To see this, let us denote by $(\vect r^{(i)})_{1 \leq i \leq n}$ the lines of a matrix $\mat T \in \real^{n \times n}$.
Then, we have
\[
    \mat M_k \mat T
    = \bigl(\mat I - \vect c^{(k)} \vect e_k\bigr) \mat T
    =
    \begin{pmatrix}
        1   \\
      & 1  \\
         & &  \ddots \\
        & & & 1 & & & \\
        & & & - c^{(k)}_{k+1} & 1  \\
        & & & \vdots & & \ddots \\
        & & & - c^{(k)}_n & & & 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        \vect r^{(1)} \\
        \vect r^{(2)} \\
        \vdots \\
        \vect r^{(k)} \\
        \vect r^{(k+1)} \\
        \vdots \\
        \vect r^{(n)}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \vect r^{(1)} \\
        \vect r^{(2)} \\
        \vdots \\
        \vect r^{(k)} \\
        \vect r^{(k+1)} - c^{(k)}_{k+1} \vect r^{(k)} \\
        \vdots \\
        \vect r^{(n)} - c^{(k)}_{n} \vect r^{(k)}
    \end{pmatrix}
\]
We show in \cref{exercise:inverse_gaussian_transformation} that
the inverse of a Gaussian transformation matrix is given by
\begin{equation}
    \label{eq:inverse_gaussian_transformation}
    (\mat I - \vect c^{(k)} \vect e_k)^{-1} = \mat I + \vect c^{(k)} \vect e_k.
\end{equation}
The idea of the $\mat L \mat U$ decomposition algorithm is to successively left-multiply $\mat A$
by Gaussian transformation matrices $\mat M_1$, then $\mat M_2$, etc.\
appropriately chosen in such a way that the matrix~$\mat A^{(k)}$,
obtained after $k$ iterations,
is upper triangular up to column $k$.
That is to say, the Gaussian transformations are constructed so that
all the entries in columns~1 to~$k$ and under the diagonal of the matrix $\mat A^{(k)}$ are equal to zero.
The resulting matrix $\mat A^{(n-1)}$ after $n-1$ iterations is then upper triangular,
and satisfies
\[
    \mat M_{n-1} \dotsc \mat M_1 \mat A = \mat A^{(n-1)}.
\]
We deduce that
\[
    \mat A = (\mat M_1^{-1} \dots \mat M_{n-1}^{-1}) \mat A^{(n-1)}.
\]
Since the first factor is lower triangular by~\eqref{eq:inverse_gaussian_transformation} and \cref{exercise:linear_product_of_lower_triangular},
this completes the $\mat L \mat U$ factorization of the matrix $\mat A$.
The product in the definition of the matrix $\mat L$ admits a simple explicit expression.
\begin{lemma}
    \label{lemma:linear_inverse_product_gaussian_transformations}
    It holds that
    \begin{equation*}
        % \label{eq:product_inverse}
        \mat M_1^{-1} \dotsb \mat M_{n-1}^{-1}
        = (\mat I + \vect c^{(1)} \vect e_1^\t) \dotsb (\mat I + \vect c^{(n-1)} \vect e_{n-1}^\t)
        = \mat I + \sum_{i=1}^{n-1}  \vect c^{(i)} \vect e_i^\t.
    \end{equation*}
\end{lemma}
\begin{proof}
    Notice that, for $i < j$,
    \[
        \vect c^{(i)} \vect e_i^\t \vect c^{(j)} \vect e_j^\t
        = \vect c^{(i)} (\vect e_i^\t \vect c^{(j)}) \vect e_j^\t
        = \vect c^{(i)} 0 \vect e_j^\t = 0.
    \]
    The statement then follows easily by expanding the product.
\end{proof}
A byproduct of~\cref{lemma:linear_inverse_product_gaussian_transformations} is all the diagonal entries of the lower triangular matrix $\mat L$ are equal to 1;
the matrix $\mat L$ is \emph{unit lower triangular}.
In practical implementations, therefore,
these diagonal entries can be omitted and the matrices $\mat L$ and $\mat U$ can be stored within the same matrix.
The full expression of the matrix $\mat L$ given the Gaussian transformations is
\begin{equation}
    \label{eq:linear_matrix_L}
    \mat L
    = \mat I +
    \begin{pmatrix}
        \vect c^{(1)} & \hdots & \vect c^{(n)} & \vect 0_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & \\
        c^{(1)}_2 & 1 \\
        c^{(1)}_3 & c^{(2)}_3 & 1 \\
        c^{(1)}_4 & c^{(2)}_4 & c^{(3)}_4 &  1 \\
        \vdots & \vdots & \vdots & & \ddots \\
        c^{(1)}_n & c^{(2)}_n & c^{(3)}_n & \hdots & c^{(n-1)}_n & 1 \\
    \end{pmatrix}
\end{equation}

Of course, the success of the strategy outlined above for the calculation of the $\mat L \mat U$ factorization hinges on
the existence of an appropriate Gaussian transformation at each iteration.
It is not difficult to show that,
if the $(k+1)$-th diagonal entry of the matrix $\mat A^{(k)}$ is nonzero for all~$k \in \{1, \dotsc, n-2\}$,
then the Gaussian transformation matrices exist and are uniquely defined.
\begin{lemma}
    \label{lemma:linear_expression_gaussian_transformations}
    Assume that $\mat A^{(k)}$ is an upper triangular up to columns $k$ included,
    with $k \leq n-2$.
    If $a^{(k)}_{k+1,k+1} > 0$,
    then there is a unique Gaussian transformation matrix $\mat M_{k+1}$ such that $\mat M_{k+1} \mat A^{(k)}$ is upper triangular up to column $k + 1$.
    It is given by $\mat I - \vect c^{(k+1)} \vect e_{k+1}^\t$ where
    \[
        \vect c^{(k+1)} =
        \begin{pmatrix}
            0 & 0 & \dots & 0 & \frac{a^{(k)}_{k+2,k+1}}{a^{(k)}_{k+1,k+1}} & \frac{a^{(k)}_{k+3,k+1}}{a^{(k)}_{k+1,k+1}} & \dots & \frac{a^{(k)}_{n,k+1}}{a^{(k)}_{k+1,k+1}}
        \end{pmatrix}^\t.
    \]
\end{lemma}
The diagonal elements $a^{(k)}_{k+1,k+1}$, where $k \in \{0, \dots, n-2 \}$, are called the pivots.
We assume for now that these are all nonzero,
and comment in \cref{sub:pivoting} on the case where a pivot is zero.
In the setting of nonzero pivots,
the $\mat L \mat U$ factorization procedure is summarized as follows.

% \begin{algorithm}
% \caption{$\mat L \mat U$ factorization algorithm}%
% \label{algo:factorization_algorithm}%
\begin{algorithmic}
\State $\mat A^{(0)} \gets \mat A, \mat L \gets \mat I$
\For{$i \in \{1, \dotsc, n-1\}$}
    \State Construct $\mat M_{i}$ as in~\cref{lemma:linear_expression_gaussian_transformations}.
    \State $\mat A^{(i)} \gets \mat M_{i} \mat A^{(i-1)}, \mat L \gets \mat \mat \mat L \mat M_i^{-1}$
\EndFor
\State $\mat U \gets \mat A^{(n-1)}$.
\end{algorithmic}
% \end{algorithm}

Of course,
in practice it is not necessary to explicitly create the Gaussian transformation matrices,
or to perform full matrix multiplications.
A more realistic, but still very simplified, version of the algorithm in Julia is given below.
\begin{minted}[xleftmargin=\parindent, linenos, mathescape]{julia}
 # A is an invertible matrix of size n x n
 L = [i == j ? 1.0 : 0.0 for i in 1:n, j in 1:n]
 U = copy(A)
 for i in 1:n-1
     for r in i+1:n
         U[i, i] == 0 && error("Pivotal entry is zero!")
         ratio = U[r, i] / U[i, i]
         L[r, i] = ratio
         U[r, i:end] -= U[i, i:end] * ratio
     end
 end
 # L is lower triangular and U is upper triangular
\end{minted}

\subsubsection*{Computational cost}%
\label{ssub:computational_cost}
The cost of the algorithm is dominated by the Gaussian transformations,
in line 9 in the above code.
All the operations amount to a computational cost scaling as $\mathcal O(n^2)$.
We denote by ``$\mathrm{flops}$'' the number of \emph{floating point operations}.
The Gaussian transformations require
\[
    \overbrace{2\times}^{\text{\julia{-} and \julia{*}}} \underbrace{\sum_{i=1}^{n-1}}_{\julia{for i in 1:n-1}} \overbrace{(n - i)}^{\julia{for r in i+1:n}} \underbrace{(n - i + 1)}_{\text{indices \julia{[i:end]}}} \quad \mathrm{flops}
    = \frac{2}{3} n^3 + \mathcal O(n^2) \quad \mathrm{flops}.
\]

\subsubsection*{Uniqueness of the decomposition}%
It is not difficult to show that,
if there exists a factorization of an invertible matrix $\mat A \in \real^{n \times n}$
into a unit triangular matrix $\mat L$ and an upper triangular matrix $\mat U$,
then the factors are necessarily unique,
and they can be obtained by using the procedure described in the previous section.
\begin{proposition}
    \label{proposition:linear_uniqueness_lu}
    The $\mat L \mat U$ decomposition of a nonsingular matrix $\mat A$,
    if it exists,
    is unique.
\end{proposition}
\begin{proof}
    We denote by $\vect c^{(1)}, \dots \vect c^{(n-1)},$ the columns of the matrix $\mat L - \mat I$.
    Then the matrices given by $\mat M_k = \mat I - \vect c^{(k)} \vect e_{k}^\t$,
    for $k \in \{1, \dotsc, n-1 \}$,
    are Gaussian transformations and it holds that
    \[
        \mat L = \mat M_1^{-1} \dotsb  \mat M_{n-1}^{-1}
    \]
    in view of \cref{lemma:linear_inverse_product_gaussian_transformations}.
    Since $\mat A = \mat L \mat U$ by assumption,
    the result of the product
    \[
        \mat M_{n-1} \dotsb \mat M_1 \mat A = \mat U
    \]
    is upper triangular.
    Let us use the notation $\mat A^{(k)} = \mat M_{k} \dotsb \mat M_1 \mat A$.
    Of all the Gaussian transformations,
    only $\mat M_1$ acts on the second line of the matrix it multiplies.
    Therefore, the entry $(2, 1)$ of~$\mat U$ coincides with the entry $(2, 1)$ of $\mat A^{(1)}$,
    which implies that $a^{(1)}_{2,1} = 0$.
    % and since $u_{2,1} = 0$ we deduce that $a_{21} - c^{(1)}_2 a_{11} = 0$.
    Then notice that $a^{(k)}_{3,1} = a^{(1)}_{3, 1}$ for all $k \geq 1$,
    because the entry $(3, 1)$ of the matrix $\mat M_2 \mat A^{(1)}$ is given by $a^{(1)}_{3,1} - c^{(2)}_3 a^{(1)}_{2,1} = a^{(1)}_{3,1}$,
    and the other transformation matrices $\mat M_3, \dotsc, \mat M_{n-1}$ leave the third line invariant.
    Consequently, it holds that $a^{(1)}_{3,1} = u_{3,1} = 0$.
    Continuing in this manner,
    we deduce that $\mat A^{(1)}$ is upper triangular in the first column and that,
    since $\mat A$ is invertible by assumption,
    the first pivot~$a_{11}$ is nonzero.

    The reasoning can then be repeated with other columns,
    in order to deduce that $\mat A^{(k)}$ is upper triangular up to column $k$ and that all the pivots $a^{(k-1)}_{kk}$ are nonzero.
    Since this uniquely defines the Gaussian transformations by \cref{lemma:linear_expression_gaussian_transformations},
    we conclude that $\mat L$ is unique, and therefore $\mat U = \mat L^{-1} \mat A$ is also unique.
\end{proof}

\subsubsection*{Existence of the decomposition}%
We now present a necessary and sufficient condition for the existence of the $\mat L \mat U$ decomposition of a matrix.
To this end, we define the principal submatrix of order $i$ of a matrix $\mat A \in \real^{n \times n}$
as the matrix $\mat A_i = \mat A[1:i, 1:i]$, in Julia notation.

\begin{proposition}
    \label{proposition:linear_existence_lu}
    The $\mat L \mat U$ factorization of a matrix $\mat A \in \real^{n \times n}$ exists if and only if
    the principal submatrices of $\mat A$ of all orders are nonsingular.
\end{proposition}
\begin{proof}
    We prove only the ``if'' direction; see~\cite[Theorem 3.4]{MR2265914} for the ``only if'' implication.

    The statement is clear if $n = 1$.
    Reasoning by induction,
    we assume that the result is proved up to $n - 1$.
    Since the matrix $\mat A_{n-1}$ is nonsingular by assumption,
    it holds that $\mat A_{n-1} = \mat L_{n-1} \mat U_{n-1}$,
    for a unit lower triangular matrix $\mat L_{n-1}$ and an upper triangular matrix $\mat U_{n-1}$.
    Let us decompose~$\mat A$ as follows:
    \[
        \mat A =
        \begin{pmatrix}
            \mat A_{n-1} & \vect c \\
            \vect d^\t & a_{nn}
        \end{pmatrix}.
    \]
    Let $\vect \ell$ and $\vect u$ denote the solutions to $\mat L_{n-1} \vect u = \vect c$ and $\mat U_{n-1}^\t \vect \ell = \vect d$.
    These solutions exists, because the matrices $\mat L_{n-1}$ and $\mat U_{n-1}$ are nonsingular.
    Letting $u_{nn} = (\vect \ell^\t \vect u)^{-1} a_{nn}$,
    we check that $\mat A$ factorizes as
    \[
        \begin{pmatrix}
            \mat A_{n-1} & \vect c \\
            \vect d^\t & a_{nn}
        \end{pmatrix}
        =
        \begin{pmatrix}
            \mat L_{n-1} & \vect 0_{n-1} \\
            \vect \ell^\t & 1
        \end{pmatrix}
        \begin{pmatrix}
            \mat U_{n-1} & \vect u \\
            \vect 0_{n-1}^\t & u_{nn}
        \end{pmatrix},
    \]
    which completes the proof.
\end{proof}

\begin{remark}
    In fact,
    the reasoning in the proof of~\cref{proposition:linear_existence_lu} can also be employed to show uniqueness of the $\mat L \mat U$ decomposition.
    The advantage of our approach in the proof of \cref{proposition:linear_uniqueness_lu} is that it also implies,
    as a byproduct, the iterative procedure for calculating the $\mat L \mat U$ decomposition is well-defined.
\end{remark}

\cref{proposition:linear_existence_lu} raises the following question:
are there classes of matrices whose principal matrices are all nonsingular?
The answer is positive, and we mention,
as an important example, the class of positive definite matrices.
Proving this is the aim of \cref{exercise:linear_positive_definite_matrix_nonsingular_principal_components}.

\subsection{Backward and forward substitution}%
\label{sub:backward_and_forward_substitution}
Once the $\mat L \mat U$ factorization has been completed,
the solution to the linear system can be obtained by first using forward, and then backward substitution,
which are just bespoke methods for solving linear systems with lower and upper triangular matrices, respectively.
Let us consider the case of a lower triangular system:
\[
    \mat L \vect y = \vect b
\]
Notice that the unknown $y_1$ may be obtained from the first line of the system.
Then, since $y_1$ is known, the value of $y_2$ can be obtained from the second line, etc.
A simple implementation of this algorithm is as follows:
\begin{minted}{julia}
    # L is unit lower triangular
    y = copy(b)
    for i in 2:n
        for j in 1:i-1
            y[i] -= L[i, j] * y[j]
        end
    end
\end{minted}

\subsection{LU factorization with pivoting}%
\label{sub:pivoting}

The $\mat L \mat U$ factorization algorithm that
we presented in \cref{sub:lu_decomposition} relies on the assumption that the pivotal entries $A^{(k)}_{k+1,k+1}$ are nonzero.
In practice,
this assumption may not be satisfied,
and in this case a modified algorithm,
called $\mat L \mat U$ factorization \emph{with pivoting},
is required.
In fact, pivoting is useful even if the pivots are nonzero,
as it enables to reduce the condition number of the algorithm,
i.e.\ the sensitivity of the output matrices~$\mat L$ and~$\mat U$ to perturbations of the input.
There are two types of pivoting:
partial pivoting, where only the rows are rearranged through a permutation at each iteration,
and complete pivoting, where both the rows and the columns are rearranged at each iteration.

Showing rigorously why pivoting is useful requires a detailed analysis and is beyond the scope of this course.
In this section, we only present the partial pivoting method.
Its influence on the condition number of the factors $\mat L$ and $\mat U$ is studied empirically in \cref{exercise:linear_lu_with_partial_pivoting}.
It is useful at this point to introduce the concept of a row permutation matrix.

\subsubsection*{Row permutation matrix}%
\begin{definition}
    \label{definition:row_permutation_matrix}%
    Let $\sigma : \{1, \dotsc, n\} \to \{1, \dotsc, n\}$ be a permutation,\
    i.e.\ a bijection on the set~$\{1, \dotsc, n\}$.
    The row permutation matrix associated with $\sigma$ is the matrix with entries
    \[
        p_{ij} =
        \begin{cases}
            1 & \text{if $i = \sigma(j)$,} \\
            0 & \text{otherwise.}
        \end{cases}
    \]
\end{definition}
When a row permutation $\mat P$ left-multiplies a matrix $\mat B \in \real^{n \times n}$,
row $i$ of matrix $\mat B$ is moved to row index $\sigma(i)$ in the resulting matrix,
for all $i \in \{1, \dots, n\}$.
A permutation matrix has a single entry equal to~1 per row and per column,
and its inverse coincides with its transpose:~$\mat P^{-1} = \mat P^\t$.

\subsubsection*{Partial pivoting}%
The idea of partial pivoting is to rearrange the lines at each iteration of the $\mat L \mat U$ factorization in such a manner that
the pivotal entry is as large as possible in absolute value.
One step of the procedure reads
\begin{equation}
    \label{eq:step_partial_pivoting}
    \mat A^{(k+1)} = \mat M_{k+1} \mat P_{k+1} \mat A^{(k)}.
\end{equation}
Here $\mat P_{k+1}$ is simple row permutation matrix which,
when acting on $\mat A^{(k)}$,
interchanges row~$k+1$ and row $\ell$,
for some index $\ell \geq k+1$.
The row index $\ell$ is selected in such a way that the absolute value of the pivotal entry in position $(k+1, k+1)$ of the product~$\mat P_{k+1} \mat A^{(k)}$ is maximum.
The matrix $\mat M_{k+1}$ is then the unique Gaussian transformation matrix ensuring that~%
$\mat A^{(k+1)}$ is upper triangular up to column~$k+1$.
The resulting matrix $\mat A^{(n-1)}$ after $n-1$ steps of the form~\eqref{eq:step_partial_pivoting} is upper triangular and satisfies
\[
    \mat A^{(n-1)} = \mat M_{n-1} \mat P_{n-1} \dotsb \mat M_{1} \mat P_{1} \mat A
    \quad \Leftrightarrow \quad
     \mat A = (\mat M_{n-1} \mat P_{n-1} \dotsb \mat M_{1} \mat P_{1})^{-1} \mat A^{(n-1)}.
\]
The first factor in the decomposition of $\mat A$ is not necessarily lower triangular.
However, using the notation $\mat M = \mat M_{n-1} \mat P_{n-1} \dotsb \mat M_{1} \mat P_{1}$ and $\mat P = \mat P_{n-1} \dotsb \mat P_{1}$,
we have
\begin{equation}
    \label{eq:plu_decomposition}
    \mat P \mat A = \mat P \mat M^{-1} \mat U = (\mat P \mat M^{-1}) \mat U =: \mat L \mat U.
\end{equation}
\Cref{lemma:linear_matrix_l_pivoting} below shows that,
as the notation suggests,
the matrix $\mat L = (\mat P \mat M^{-1})$ on the right-hand side is indeed lower triangular.
Before stating and proving the lemma,
we note that $\mat P$ is a row permutation matrix,
and so the solution to the linear system $\mat A \vect x = \mat b$ can be obtained by solving $\mat L \mat U \vect x = \mat P^\t \vect b$
by forward and backward substitution.
Since $\mat P$ is a very sparse matrix,
the right-hand side~$\mat P^\t \vect b$ can be calculated very efficiently.
\begin{lemma}
    \label{lemma:linear_matrix_l_pivoting}
    The matrix $\mat L = \mat P \mat M^{-1}$ is unit lower triangular
    with all entries bounded in absolute value from above by 1.
    It admits the expression
    \[
        \mat L
        = \mat I
        + (\mat P_{n-1} \dotsb \mat P_{2} \vect c^{(1)}) \vect e_{1}^\t
        + (\mat P_{n-1} \dotsb \mat P_{3} \vect c^{(2)}) \vect e_{2}^\t
        + \dotsb
        + (\mat P_{n-1} \vect c^{(n-2)}) \vect e_{n-2}^\t
        + \vect c^{(n-1)} \vect e_{n-1}^\t.
    \]
\end{lemma}
\begin{proof}
    Let $\mat M^{(k)} = \mat M_{k} \mat P_{k} \dotsb \mat M_{1} \mat P_{1}$ and $\mat P^{(k)} = \mat P_{k} \dotsb \mat P_{1}$.
    It is sufficient to show that
    \begin{equation}
        \label{eq:linear_lower_triangular_l}
        \mat P^{(k)} \bigl( \mat M^{(k)} \bigr)^{-1}
        = \mat I
        + (\mat P_{k} \dotsb \mat P_{2} \vect c^{(1)}) \vect e_{1}^\t
        + (\mat P_{k} \dotsb \mat P_{3} \vect c^{(2)}) \vect e_{2}^\t
        + \dotsb
        + (\mat P_{k} \vect c^{(k-1)}) \vect e_{k-1}^\t
        + \vect c^{(k)} \vect e_{k}^\t
    \end{equation}
    for all $k \in \{1, \dotsc, n-1 \}$.
    The statement is clear for $k = 1$,
    and we assume by induction that it is true up to~$k-1$.
    Then notice that
    \begin{align*}
        \mat P^{(k)} \bigl( \mat M^{(k)} \bigr)^{-1}
        &= \mat P_{k} \left( \mat P^{(k-1)} \bigl( \mat M^{(k-1)} \bigr)^{-1} \right) \mat P_k^{-1} \mat M_k^{-1} \\
        &= \mat P_{k} \Bigl( \mat I
        + (\mat P_{k-1} \dotsb \mat P_{2} \vect c^{(1)}) \vect e_{1}^\t
        + \dotsb
        + (\mat P_{k-1} \vect c^{(k-2)}) \vect e_{k-2}^\t
        + \vect c^{(k-1)} \vect e_{k-1}^\t \Bigr) \mat P_k^{-1} \mat M_k^{-1} \\
        &= \Bigl( \mat I
        + (\mat P_k \mat P_{k-1} \dotsb \mat P_{2} \vect c^{(1)}) \vect e_{1}^\t
        + \dotsb
        + (\mat P_k \mat P_{k-1} \vect c^{(k-2)}) \vect e_{k-2}^\t
        + (\mat P_k \vect c^{(k-1)}) \vect e_{k-1}^\t \Bigr) \mat M_k^{-1}.
    \end{align*}
    In the last equality,
    we used that $\vect e_{i}^\t \mat P_k^{-1} = (\mat P_k \vect e_i)^\t = \vect e_i^\t$ for all $i \in \{1, \dotsc, k-1\}$,
    because the row permutation $\mat P_k$ does not affect lines~$1$ to~$k-1$.
    Using the expression $\mat M_k^{-1} = \mat I + \vect c^{(k)} \vect e_k^\t$,
    expanding the product and noting that $\vect e_j^\t \vect c^{(k)} = 0$ if $j \leq k$,
    we obtain~\eqref{eq:linear_lower_triangular_l}.
    The statement that the entries are bounded in absolute value from above by 1 follows from the choice of the pivot at each iteration.
\end{proof}
The expression of $\mat L$ in \cref{lemma:linear_matrix_l_pivoting} suggests the iterative procedure given in~\cref{algo:LU_decomposition_with_partial_pivoting} for performing the $\mat L \mat U$ factorization with partial pivoting.
A Julia implementation of this algorithm is presented in~\cref{julia:lu_factorization_with_pivoting}.
\begin{algorithm}
\caption{$\mat L \mat U$ decomposition with partial pivoting}%
\label{algo:LU_decomposition_with_partial_pivoting}%
\begin{algorithmic}
\State Assign $\mat A^{(0)} \gets \mat A$ and $\mat P \gets \mat I$
\For{$i \in \{1, \dotsc, n-1\}$}
    \State Find the row index $k \geq i$ such that $\mat A^{(i-1)}_{k,i}$ is maximum in absolute value.
    \State Interchange the rows $i$ and $k$ of matrices $\mat A^{(i-1)}$ and $\mat P$, and of vectors $\vect c^{(1)}, \dotsc, \vect c^{(i-1)}$.
    \State Construct $\mat M_{i}$ with corresponding column vector $\vect c^{(i)}$ as in~\cref{lemma:linear_expression_gaussian_transformations}.
    \State Assign $\mat A^{(i)} \gets \mat M_{i} \mat A^{(i-1)}$
\EndFor
\State Assign $\mat U \gets \mat A^{(n-1)}$.
\State Assign $\mat L \gets \mat I + \begin{pmatrix} \vect c^{(1)} & \hdots & \vect c^{(n-1)} & \vect 0_n \end{pmatrix}$.
\end{algorithmic}
\end{algorithm}

\begin{code}
\begin{minted}[fontsize=\footnotesize]{julia}
    # Auxiliary function
    function swap_rows!(i, j, matrices...)
        for M in matrices
            M_row_i = M[i, :]
            M[i, :] = M[j, :]
            M[j, :] = M_row_i
        end
    end

    n = size(A)[1]
    L, U = zeros(n, 0), copy(A)
    P = [i == j ? 1.0 : 0.0 for i in 1:n, j in 1:n]
    for i in 1:n-1
        # Pivoting
        index_row_pivot = i - 1 + argmax(abs.(U[i:end, i]))
        swap_rows!(i, index_row_pivot, U, L, P)

        # Usual Gaussian transformation
        c = [zeros(i-1); 1.0; zeros(n-i)]
        for r in i+1:n
            ratio = U[r, i] / U[i, i]
            c[r] = ratio
            U[r, i:end] -= U[i, i:end] * ratio
        end
        L = [L c]
    end
    L = [L [zeros(n-1); 1.0]]
    # It holds that P*A = L*U
\end{minted}
\caption{%
    $\mat L \mat U$ factorization with partial pivoting.
}%
\label{julia:lu_factorization_with_pivoting}
\end{code}

\begin{remark}
    It is possible to show that,
    if the matrix $\mat A$ is column diagonally dominant in the sense that
    \[
        \forall j \in \{1, \dotsc, n\}, \qquad
        \lvert a_{jj} \rvert \geq \sum_{i=1, i\neq j}^{n} \lvert a_{ij} \rvert,
    \]
    then pivoting does not have an effect:
    at each iteration,
    the best pivot is already on the diagonal.
\end{remark}

\subsection{Direct method for symmetric positive definite matrices}%
\label{sub:direct_method_for_symmetric_positive_definite_matrices}
The $\mat L \mat U$ factorization with partial pivoting applies to any matrix $\mat A \in \real^{n \times n}$ that is invertible.
If~$\mat A$ is symmetric positive definite,
however, it is possible to compute a factorization into lower and upper triangular matrices at half the computational cost,
using the so-called \emph{Cholesky decomposition}.
\begin{lemma}
    [Cholesky decomposition]
    If~$\mat A$ is symmetric positive definite,
    then there exists a lower-triangular matrix $\mat C \in \real^{n \times n}$ such that
    \begin{equation}
        \label{eq:linear_cholesky_decomposition}
        \mat A = \mat C \mat C^\t.
    \end{equation}
    Equation~\eqref{eq:linear_cholesky_decomposition} is called the Cholesky factorization of $\mat A$.
    The matrix $\mat C$ is unique if we require that all its diagonal entries are positive.
\end{lemma}
\begin{proof}
    Since $\mat A$ is positive definite,
    its $\mat L \mat U$ decomposition exists and is unique by~\cref{proposition:linear_uniqueness_lu,proposition:linear_existence_lu}.
    Let $\mat D$ denote the diagonal matrix with the same diagonal as that of $\mat U$.
    Then
    \[
        \mat A = \mat L \mat D \mat (\mat D^{-1} \mat U).
    \]
    Note that the matrix $\mat D^{-1} \mat U$ is unit upper triangular.
    Since $\mat A$ is symmetric,
    we have
    \[
        \mat A = \mat A^\t = (\mat D^{-1} \mat U)^\t (\mat L \mat D)^\t.
    \]
    The first and second factors on the right-hand side are respectively unit lower triangular and upper triangular,
    and so we deduce, by uniqueness of the $\mat L \mat U$ decomposition,
    that $\mat L = (\mat D^{-1} \mat U)^\t$ and $\mat U = (\mat L \mat D)^\t$.
    But then
    \[
        \mat A = \mat L \mat U = \mat L \mat D \mat L^\t = (\mat L \sqrt{\mat D}) (\sqrt{\mat D} \mat L)^\t.
    \]
    Here $\sqrt{\mat D}$ denotes the diagonal matrix whose diagonal entries are obtained by taking the square root of those of $\mat D$,
    which are necessarily positive because $\mat A$ is positive definite.
    This implies the existence of a Cholesky factorization with $\mat C = \mat L \sqrt{\mat D}$.
\end{proof}

\subsubsection*{Calculation of the Choleski factor}%

The matrix $\mat C$ can be calculated from~\eqref{eq:linear_cholesky_decomposition}.
For example, developing the matrix product gives that $a_{1,1} = c_{1, 1}^2$ and so $c_{1, 1} = \sqrt{a_{1,1}}$.
It is then possible to calculate $c_{2,1}$ from the equation~$a_{2, 1} = c_{2, 1} c_{1, 1}$, and so on.
Implementing the Cholesky factorization is the goal of \cref{exercise:linear_cholesky}.

\subsection{Banded matrices}%


\begin{exercise}
    [Inverse of Gaussian transformation]
    \label{exercise:inverse_gaussian_transformation}
    Prove the formula~\eqref{eq:inverse_gaussian_transformation}.
\end{exercise}

\begin{exercise}
    \label{exercise:linear_product_of_lower_triangular}
    Prove that the product of two lower triangular matrices is lower triangular.
\end{exercise}

\begin{exercise}
    \label{exercise:linear_positive_definite_matrix_nonsingular_principal_components}
    Assume that $\mat A \in \real^{n \times n}$ is positive definite,
    i.e.\ that $\vect x^\t \mat A \vect x > 0$ for all $\vect x \in \real^n \backslash \{ \vect 0_n \}$.
    Show that all the principal submatrices of $\mat A$ are nonsingular.
\end{exercise}

\begin{compexercise}
    Implement the backward substitution algorithm for solving $\mat U x = y$.
    What is the computational cost of the algorithm?
\end{compexercise}

\begin{compexercise}
    \label{exercise:linear_lu_with_partial_pivoting}
    Compare the condition number of the matrices $\mat L$ and $\mat U$ with and without partial pivoting.
    For testing, use a matrix with pseudo-random entries generated as follows
    \begin{minted}{julia}
    import Random
    # Set the seed so that the code is deterministic
    Random.seed!(0)
    n = 1000 # You can change this parameter
    A = randn(n, n)
    \end{minted}
\end{compexercise}

\begin{compexercise}
    \label{exercise:linear_cholesky}
    Write a code for calculating the Cholesky factorization of a matrix by comparing the entries of the product $\mat C \mat C^\t$ with those of the matrix $\mat A$.
    What is the associated computational cost,
    and how does it compare with that of the $\mat L \mat U$ factorization?
\end{compexercise}
