\chapter{Floating point arithmetic}%
\label{cha:rounding_errors}
\minitoc

\section{Introduction}%
\label{sec:introduction}
When we study numerical algorithms in the next chapters,
we assume implicitly that the operations involved are performed exactly.
On a computer, however, only a subset of the real numbers can be stored and,
consequently, many arithmetic operations are performed only approximately.
This is the source of the so-called \emph{round-off errors}.
% Whether or not these inevitable errors are sufficiently small to be neglected in a numerical method
% depends in general on the \emph{numerical stability} of the method.
% A numerical method is said to be \emph{numerically unstable} is small errors are magnified,
% and \emph{numerically stable} otherwise.
% these errors generally need not be a cause worry when they appear in methods that are numerically stables.
The rest of this chapter is organized as follows.
\begin{itemize}
    \item
        In \cref{sec:set_of_values},
        we describe the set of floating point numbers that can be represented in the usual floating point formats;
    \item
        In \cref{sec:arithmetic_operations_between_floating_point_formats}
        we seek to understand how arithmetic operations between floating point numbers behave.
        We insist in particular on the fact that,
        in a calculation involving several successive arithmetic operations,
        the result of each intermediate operation is stored as a floating point number,
        with a possible error.
        % This is why \texttt{1.0 + 1e-100 - 1.0} returns \texttt{0.0},
        % in the usual double-precision format.
    \item
        In \cref{sec:encoding_of_floating_point_numbers},
        we briefly present how these numbers are encoded
        according to the IEEE754 standard, widely accepted today.
        We discuss in particular the encoding of special values such as \texttt{Inf}, \texttt{-Inf} and \texttt{NaN}.
\end{itemize}
In order to completely describe a floating-point system,
one would in principle need to also specify the storage of integer numbers
and the conversion mechanisms between different number formats,
as well as a number of edge cases.
For instance, the reference IEEE754 standard specifies that \texttt{NaN == NaN} must evaluate to \texttt{false},
which is somewhat arbitrary but may be useful in some situations.
Needless to say,
a comprehensive discussion of the subject is beyond the scope of this course;
our aim in this chapter is only to introduce the key concepts.

\section{Set of values representable in floating point formats}%
\label{sec:set_of_values}
As mentioned in the introduction,
only a subset of the real numbers can be stored exactly in a computer.
In all the programming languages and software that follow the IEEE754 standard,
the set of representable numbers is of the form
\begin{align*}
    \floating (p, E_{\min}, E_{\max})
    &= \Bigl\{ (-1)^s 2^E (b_0. b_1 b_2 \dots b_{p-1})_2 \colon s \in \{0, 1\}, b_i \in \{0, 1\} \, \text{and} \, E_{\min} \leq E \leq E_{\max} \Bigr\}.
\end{align*}
In addition to these, floating number formats provide the special entities \texttt{Inf}, \texttt{-Inf} and \texttt{NaN}.
Three parameters appear in the definition of this set.
The parameter $p$ is the number of significant bits (also called the precision),
and $E_{\min}, E_{\max}$ are respectively the minimum and maximum exponents.
The subscript $_2$ is employed to indicate that the number $b_0. b_1 b_2 \dots b_{p-1}$ is expressed in base 2:
\[
    (b_0. b_1 b_2 \dots b_{p-1})_2 := \sum_{i=0}^{p-1} b_i 2^{-i}.
\]
For a number $x \in \floating(p, E_{\min}, E_{\max})$,
$s$ is called the \emph{sign}, $E$ is the \emph{exponent} and
$b_0. b_1 b_2 \dots b_{p-1}$ is the \emph{significand}.
The latter can be divided into a \emph{leading bit} $b_0$ and the fraction $b_1 b_2 \dots b_{p-1}$,
to the right of the binary point.
The most widely used floating point formats are the \emph{single} and \emph{double precision} formats,
which are called respectively \texttt{Float32} and \texttt{Float64} in Julia.
Their parameters are summarized in~\cref{table:floating_point_formats},
and in the rest of this section we use the shorthand notation $\floating_{32}$ and $\floating_{64}$.
\begin{table}[hb]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        & Single precision & Double precision
        \\ \hline
        $p$ & 24 & 53
        \\ \hline
        $E_{\min}$ & -126 & -1022
        \\ \hline
        $E_{\max}$ & 127 & 1023
        \\ \hline
    \end{tabular}
    \caption{Floating point formats}%
    \label{table:floating_point_formats}
\end{table}

At this point, the exponent $E$ and the significand of some number $x \in \floating(p, E_{\min}, E_{\max})$ are generally not uniquely defined.
For example, the number $2.0 \in \floating(53, -1022, 1023)$ may be expressed as $(-1)^0 2^1 (1.00\dots00)_2$ or as $(-1)^0 2^{2} (0.100\dots00)_2$.
In a computer, however, each number in a floating point format is always represented in the same manner,
which is useful in order to perform comparisons:
since the representation is unique,
two numbers are equal if only if their sign, exponent and significand are equal.
The exponent and significand are made unique by requiring that
\begin{itemize}
    \item either $E > E_{\min}$ and $b_0 = 1$;
    \item or $E = E_{\min}$, in which case the leading bit may be 0.
\end{itemize}
Floating point numbers with a leading bit equal to 1 are called \emph{normalized},
whereas those with a leading bit equal to zero are called \emph{subnormal} or \emph{denormalized}.
We give more information on this in~\cref{sec:encoding_of_floating_point_numbers},
which is devoted specifically to the encoding of floating point numbers.

\begin{remark}
    [Nonuniqueness of the floating point represention of 0.0]
    The sign $s$ is clearly unique for any number in a floating point format,
    except for $0.0$,
    which could in principle be represented as
    \[
        (-1)^0 2^{E_{\min}} (0.00\dots00)_2
        \qquad \text{or} \qquad
        (-1)^1 2^{E_{\min}} (0.00\dots00)_2.
    \]
    In practice, both representations of 0.0 are available on most machines,
    and these behave slightly differently.
    For example $\texttt{1/(0.0) = Inf}$ but $\texttt{1/(-0.0) = -Inf}$.
\end{remark}

In \texttt{Julia}, non-integer numbers are interpreted as \texttt{Float64} by default,
which you may verify by using the \texttt{typeof} function.
In order to define a number of type \texttt{Float32},
the suffix $\texttt{f0}$ must be appended to the decimal expansion.
For instance, the command $\texttt{a = 4.0f0}$ defines a number $a$ of type \texttt{Float64}.

\section{Arithmetic operations between floating point formats}%
\label{sec:arithmetic_operations_between_floating_point_formats}

\section{Encoding of floating point numbers}%
\label{sec:encoding_of_floating_point_numbers}

We already presented, in~\cref{sec:set_of_values},
how the representation (sign, exponent and significand) of a nonzero number in a floating point format $\floating(p, E_{\min}, E_{\max})$ can be made unique.
In this section,
we discuss how these numbers are stored in practice.

\section{An example: numerical differentiation}%
To illustrate this point,
consider the following program for calculating the derivative of the function $f(x) = \log(x)$ at $x = 1$.
\[
    f'(x) \approx \frac{\log(x + \varepsilon) - \log(x)}{\varepsilon}
\]
\begin{minted}{julia}
f(x) = log(x)
x + δ
for i in 1:10
end
abcdefghijklmnopqr
\end{minted}

\begin{exercise}
    [Avoiding overflow]
    Write a code to calculate the weighted average
    \[
        S := \frac
        {\sum_{x=0}^{L} \e^x i}
        {\sum_{x=0}^{L} \e^x},
        \qquad L = 1000.
    \]
\end{exercise}



% \begin{exercise}
%     Show that $31_{8} = 25_{10}$,
%     an equation sometimes written as~${\rm Oct} 31 = {\rm Dec} 25$.
% \end{exercise}

% \begin{example}
%     In the HTML language,
%     colors are represented by strings of the type $\texttt{\#rrggbb}$.
% \end{example}

% \begin{example}
%     [Computation of the standard deviation]
% \end{example}

% \begin{example}
%     [Calculation of the derivative]
%     A classic π example of cancellation is when calculating derivatives.
%     Consider the following approximation of $\frac{\d}{\d x}\log(x) \vert_{x=1}$:
%     \[
%         	\frac{\log(x + \varepsilon) - \log(x)}{\varepsilon}
%     \]
% \end{example}

% \begin{example}
%     [Second-degree equation]
%     Consider the equation
%     \[
%         x^2 - \varepsilon x + 1 = 0
%     \]
% \end{example}

% \begin{theorem}
%     {Title of theorem}
%     \label{thm:test}
%     hello
% \end{theorem}

% \begin{example}
%     [A test example]
%     hello
% \end{example}

% \begin{lemma}
%     [Title of the lemma]
%     {Title of theorem}
%     \label{lemma:test}
%     hello
% \end{lemma}

% \begin{remark}
%     [Hello]
%     test
% \end{remark}

% \begin{theorem}
%     {Title of theorem}
%     test
% \end{theorem}

% \cref{lemma:test}

\section{Discussion and bibliography}%
\label{sec:discussion_and_bibliograhpy}
\Cref{sec:set_of_values} is based on the IEEE754 standard~\cite{ieee754} and~\cite{MR2265914}.
The interested reader may refer
% In this chapters,
% we covered the fundamentals of floating point arithmetics,
