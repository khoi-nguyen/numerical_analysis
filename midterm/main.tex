\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage[colorlinks=true]{hyperref}
\usepackage[outputdir=build,newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\theoremstyle{definition}
\newtheorem{question}{Question}
\usepackage{enumitem}
\setlist[enumerate]{font=\bfseries}
\input{../macros.tex}
\begin{document}

\title{Numerical Analysis: Midterm \\
\small{(\textbf{30 marks}, only the 3 best questions count)}}
\author{Urbain Vaes}
\maketitle

\begin{question}
    [Floating point arithmetic, 10 marks]
    True or false? (+1/0/-1)
    \begin{enumerate}
        \item
            Let $(\placeholder)_2$ denote binary representation.
            It holds that
            \(
                (0.1011)_2 + (0.0101)_2 = 1.
            \)

        \item
            Let $(\placeholder)_3$ denote base 3 representation.
            It holds that
            \(
                (1000)_3 \times (0.002)_3 = 2.
            \)

        \item
            A natural number with binary representation $(b_4 b_3 b_2 b_1 b_0)_2$ is even if and only if $b_0 = 0$.

        \item
            In Julia, \julia{Float64(.4) == Float32(.4)} evaluates to \julia{true}.

        \item
            Machine addition~$\madd$ is a commutative operation.
            More precisely, given any two double-precision floating point numbers $x \in \floating_{64}$ and $y \in \floating_{64}$,
            it holds that
            \(
                x \madd y = y \madd x.
            \)

        \item
            Let $\floating_{32}$ and $\floating_{64}$ denote respectively the sets of single and double precision floating point numbers.
            It holds that $\floating_{32} \subset \floating_{64}$.

        \item
            The machine epsilon of a floating point format is the smallest strictly positive number that can be represented exactly in the format.

        \item
            Let $\floating_{64}$ denote the set of double precision floating point numbers.
            For any $x \in \real$ such that~$x \in \floating_{64}$,
            it holds that $x + 1 \in \floating_{64}$.

        \item
            Let $a_i \in \{0, 1\}$ for $i \in \{1, 2, 3\}$.
            If $(a_1 a_2 a_3)_2$ is a multiple of 3,
            then $(a_1 a_2 a_3)_4$ is a multiple of 6.
            Here $(\placeholder)_4$ denotes base 4 representation.

        \item
            Let $f\colon \real \to \real$ denote the function that maps $x \in \real$
            to the number of double precision floating point numbers contained in the interval $[x-1, x+1]$.
            Then $f$ is a decreasing function of $x$.

        \item
            Let $n \in \nat$.
            The number of bits in the binary representation of $n$ is less than or equal to 4 times the number of digits in the decimal representation of~$n$.

        \item
            It holds that $(0.\overline{2200})_3 = (0.9)_{10}$.

        \item
            Let $p \in \nat$.
            The set
            \(
                \bigl\{ (b_0. b_1 b_2 \dots b_{p-1})_2 \colon b_i \in \{0, 1\} \bigr\}
            \)
            contains $2^{p}$ distinct real numbers.
        % \item
        %     Let~$\madd$ and~$\msub$ denote machine addition and machine subtraction in the $\floating_{64}$ format.
        %     The result of $(10^{10} \madd 0.1) \msub 10^{10}$ is closer to 0.1 than $0.1 \madd (10^{10} \msub 10^{10})$.
    \end{enumerate}
    % A correct (resp.~incorrect) answer leads to +1 mark (resp. -1 mark).
\end{question}

\newpage
\begin{question}
    [Interpolation and approximation, 10 marks]
    Throughout this exercise, we assume
    that $x_0 < \dotsc < x_n$ are distinct values and
    that $u \colon \real \to \real$ is a smooth function.
    The notation $\poly(n)$ denotes the set of polynomials of degree less than or equal to $n$.

    \begin{enumerate}
        \item
            \mymarks{4}
            Are the following statements true or false? (+1/0/-1)
            \begin{itemize}
                \item
                    There exists a unique polynomial~$p \in \poly(n)$ such that
                    \begin{equation}
                        \label{eq:interpolation}
                        \forall i \in \{0, \dotsc, n\}, \qquad
                        p(x_i) = u (x_i).
                    \end{equation}

                \item
                    Assume that $p \in \poly(n)$ is such that~\eqref{eq:interpolation} is satisfied.
                    Then there is a constant~$K \in \real$ independent of~$x$ such that
                    \[
                        \forall x \in \real,
                        \qquad u(x) - p(x) = K (x - x_0) \dotsc (x - x_n).
                    \]

                \item
                    Assume that $p \in \poly(n)$ is such that~\eqref{eq:interpolation} is satisfied.
                    Then $p$ is of degree exactly~$n$.

                \item
                    If $x_0, \dotsc, x_n$ are the roots of the Chebyshev polynomial of degree~$n$,
                    then
                    \[
                        \sup_{x \in \real} \Bigl\lvert (x - x_0) \dotsc (x - x_n) \Bigr\rvert \leq \frac{\pi}{2^n}.
                    \]

                \item
                    The function $S\colon \nat \to \real$ given by
                    \[
                        S(n) = \sum_{i=1}^{n} \left(i + i^2 + i^3 + i^4\right)
                    \]
                    is a polynomial of degree 5.
                    (More precisely, there exists a polynomial of degree~5,
                    say~$q$, such that $S(n) = q(n)$ for all $n \in \nat$.)

                % \item
                %     Assume that $p \in \poly(n)$ is such that~\eqref{eq:interpolation} is satisfied.
                %     It holds that
                %     \[
                %         \sup_{x \in \real} \bigl\lvert u(x) - p(x) \bigr\rvert \leq \pi^2 /n.
                %     \]
            \end{itemize}
    \item
        For $i \in \{0, \dotsc, n\}$,
        let $u_i = u(x_i)$,
        and let $m \leq n$ be a given natural number.
        We wish to fit the data $(x_0, u_0), \dotsc, (x_n, u_n)$ with a function $\widehat u\colon \real \to \real$ of the form
        \[
            \widehat u(x) = \alpha_0 + \alpha_1 x + \dotsc + \alpha_m x^m.
        \]
        Specifically, we wish to find coefficients $\vect \alpha = (\alpha_0, \dotsc, \alpha_m)^\t$
        such that the error
        \[
            J(\vect \alpha) := \frac{1}{2}\sum_{i=0}^{n} \abs[big]{u_i - \widehat u(x_i)}^2
        \]
        is minimized.
        Throughout this exercise,
        we use the notations
        \[
            \mat A
            \begin{pmatrix}
                1 & x_0 & \hdots & x_0^m \\
                \vdots & \vdots & & \vdots \\
                1 & x_{n} & \hdots & x_n^m
            \end{pmatrix},
            \qquad
            \vect b :=
            \begin{pmatrix}
                u_0 \\
                \vdots \\
                u_n
            \end{pmatrix}
        \]
        \begin{itemize}
            \item
            \mymarks{3}
                Show that $J(\vect \alpha)$ may be rewritten as
                \[
                    J(\vect \alpha) = \frac{1}{2} (\mat A \vect \alpha  - \vect b)^\t (\mat A \vect \alpha  - \vect b).
                \]

            \item
            \mymarks{2}
                Prove that if $\vect \alpha_* \in \real^{m+1}$ is a minimizer of $J$,
                then
                \begin{equation}
                    \label{eq:normal_equations}
                    \mat A^\t \mat A \vect \alpha_* = \mat A^\t \vect b.
                \end{equation}

            % \item
            %     \mymark
            %     Show that the matrix $\mat A^\t \mat A$ is positive definite.
            %     You can take for granted that the columns of~$\mat A$ are linearly independent.

            \item
                \mymark
                Find a solution to~\eqref{eq:normal_equations} in terms of $u_0, \dotsc, u_n$ and $n$ when $m = 0$.
                Explain.
        \end{itemize}
\end{enumerate}
\end{question}

\newpage
\begin{question}
    [Numerical integration, 10 marks]
    The Gauss--Legendre quadrature formula with $n$ nodes is an approximate integration formula of the form
    \begin{equation}
        \label{gauss_legendre}
        I(u) := \int_{-1}^{1} u(x) \, \d x \approx \sum_{i=1}^{n} w_i \, u(x_i) =: \widehat I_n(u),
    \end{equation}
    which is exact when $u$ is a polynomial of degree less than or equal to $2n-1$.
    (Note that the nodes are here numbered starting from 1.)

    \begin{enumerate}
        \item
            \mymarks{5}
            Find the nodes and weights of the Gauss--Legendre rule with $n=3$ nodes.

        \item
            \mymarks{2}
            Let $\{L_0, L_1, \dotsc\}$ denote orthogonal polynomials for the inner product
            \[
                \ip{f, g} := \int_{-1}^{1} f(x) g(x) \, \d x
            \]
            which, in addition, satisfy the following two conditions:
            \begin{itemize}
                \item
                    For all $i \in \nat$,
                    the polynomial $L_i$ is of degree $i$.

                \item
                    The leading coefficient of $L_i$,
                    which multiplies $x^i$,
                    is equal to 1.
            \end{itemize}
            Calculate $L_0$, $L_1$, $L_2$ and $L_3$.
            What is the connection between $L_3$ and the rule found in the first item?

        \item
            Assume that $x_1, \dotsc, x_n$ and $w_1, \dotsc, w_n$ are such that~\eqref{gauss_legendre} is satisfied for all $u \in \poly(2n-1)$.
            % denote the nodes and weights of the Gauss--Legendre quadrature with~$n$ nodes.
            \begin{itemize}

                \item
                   \mymarks{2}
                    Show that the weights are given by
                    \[
                        \forall i \in \{1, \dotsc, n\}, \qquad
                        w_i = \int_{-1}^{1} \ell_i(x) \, \d x,
                    \]
                    where $\ell_i$ is the Lagrange polynomial
                    \[
                        \ell_i(x) = \prod_{j \neq i} \frac{x - x_j}{x_i - x_j}.
                    \]

                \item
                   \mymarks{1}
                    Show that the weights are all positive: $w_i > 0$ for all $i$.
            \end{itemize}

                \newpage
                \item
                    \mybonus{2} Prove the following error estimate:
                    if $u$ is a smooth function,
                    then
                    \[
                         \bigl\lvert I(u) - \widehat I_n(u) \bigr\rvert
                         \leq \frac{C_{2n}}{(2n)!}\int_{-1}^1 \bigl(L_n(x)\bigr)^2 \, \d x,
                         \qquad C_{2n} := \sup_{\xi \in [-1, 1]} \left\lvert u^{(2n)}(\xi) \right\rvert.
                    \]
                    \textbf{Hint}: You may find it useful to proceed as follows:
                    \begin{itemize}
                        \item
                            First show that
                            \begin{equation}
                                \label{eq:hermite_interpolation}
                                 I(u) - \widehat I_n(u)
                                = \int_{-1}^{1} u(x) - p(x) \, \d x,
                            \end{equation}
                            for \emph{any} polynomial $p \in \poly(2n-1)$ such that
                            \[
                                \forall i \in \{1, \dotsc, n\}, \qquad
                                p(x_i) = u(x_i).
                            \]

                        \item
                            Notice that equation~\eqref{eq:hermite_interpolation} is true in particular when $p$ is the Hermite interpolation of~$u$ at the nodes~$x_1, \dotsc, x_n$.
                            Finally, conclude by using the formula for the interpolation error proved in class:
                            if $p$ is the Hermite interpolant of $u$ at the nodes $x_1, \dotsc, x_n$,
                        then
                        \[
                            \forall x \in \real, \qquad
                            u(x) - p(x) = \frac{u^{(2n)}\bigl(\xi(x)\bigr)}{(2n)!} (x-x_1)^2 \dotsc (x - x_n)^2.
                        \]
                    \end{itemize}
    \end{enumerate}

\end{question}

\newpage
\begin{question}
    [Vector and matrix norms, 10 marks]
    The 1-norm and the $\infty$-norm of a vector~$\vect x \in \real^n$ are defined as follows:
    \[
        \norm{\vect x}_1 = \lvert x_1 \rvert + \dotsb + \lvert x_n \rvert
        \qquad \text{ and } \qquad
        \norm{\vect x}_{\infty} = \max \Bigl\{ \lvert x_1 \rvert, \dotsc, \lvert x_n \rvert \Bigr\}.
    \]
    These norms both induce a matrix norm through the formula
    \[
        \norm{\mat A}_p
        := \sup \Bigl\{ \norm{\mat A \vect x}_p : \norm{\vect x}_p = 1 \Bigr\}.
    \]
    Prove, for $\mat A \in \real^{n \times n}$, that
    \begin{enumerate}
        \item
            \mymarks{10}
            $\norm{\mat A}_1$ is given by the maximum absolute column sum:
            \begin{equation}
                \label{eq:matrix_1_norm}
               \norm{\mat A}_1 = \max_{1 \leq j \leq n}  \sum_{i=1}^{n} \abs{a_{ij}}.
            \end{equation}

        \item
            \mybonus{2}
            $\norm{\mat A}_{\infty}$ is given by the maximum absolute row sum:
            \[
                \norm{\mat A}_{\infty} = \max_{1 \leq i \leq n}  \sum_{j=1}^{n} \abs{a_{ij}}.
            \]
    \end{enumerate}

    \textbf{Hint:} In order to prove~\eqref{eq:matrix_1_norm},
    you may find it useful to proceed as follows:
    \begin{itemize}
        \item
            Introduce $j_*$ as the index of the column with maximum absolute sum:
            \[
                j_* = \argmax_{1 \leq j \leq n}  \sum_{i=1}^{n} \abs{a_{ij}}.
            \]
        \item
            Prove the direction $\geq$ in~\eqref{eq:matrix_1_norm} by finding a vector $\vect x$ with $\norm{\vect x}_1 = 1$ such that
            \[
                \norm{\mat A \vect x}_1 =  \sum_{i=1}^{n} \abs{a_{ij_*}}.
            \]

        \item
            Prove the direction $\leq$ in~\eqref{eq:matrix_1_norm} by showing that,
            for any $\vect x \in \real^n$ with $\norm{\vect x}_1 = 1$,
            \[
               \norm{\mat A \vect x}_1 \leq  \sum_{i=1}^{n} \abs{a_{ij_*}}.
           \]
    \end{itemize}
\end{question}
\end{document}
