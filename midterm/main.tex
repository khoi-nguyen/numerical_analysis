\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage[outputdir=build,newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm}
\usepackage{mathtools}
\theoremstyle{definition}
\newtheorem{question}{Question}
\input{../macros.tex}
\begin{document}

\title{Numerical Analysis: Practice Midterm (30 marks)}
\author{Urbain Vaes}
\maketitle

\begin{question}
    [8 marks]
    True or false?
    \begin{enumerate}
        \item Let $(\placeholder)_2$ denote binary representation.
            It holds that
            \(
                (0.1111)_2 + (0.0001)_2 = 1.
            \)
        \item It holds that
            \(
                (1000)_2 \times (0.001)_2 = 1.
            \)
        \item It holds that
            \[
                (0.\overline{1})_3 = \frac{1}{2}.
            \]
        \item In base 16, all the natural numbers from 1 to 200 can be represented using 2 digits.
        \item In Julia, \julia{Float64(.1) == Float32(.1)} evaluates to \julia{true}.
        \item The spacing (in absolute value) between successive double-precision (\julia{Float64}) floating point numbers is constant.
        \item It holds that $(0.\overline{10101})_2 = (1.2345)_{10}$.
        \item Machine addition~$\madd$ is an associative operation.
            More precisely, given any three double-precision floating point numbers $x$, $y$ and $z$,
            the following equality holds:
            \[
                (x \madd y) \madd z = x \madd (y \madd z).
            \]
        \item
            The machine epsilon is the smallest strictly positive number that can be represented in a floating point format.

        \item
            Let $\varepsilon$ denote the machine epsilon for the double-precision format.
            Let also~$\madd$ and~$\mdiv$ denote respectively the machine addition and the machine division operators for the double-precision format.
            It holds that $1 \madd (\varepsilon \mdiv 64) = 1$ and that $\varepsilon \mdiv 64 \neq 0$.

        \item
            Assume that $x \in \real$ belongs to the double-precision floating point format (that is,
            assume that $x \in \floating_{64}$).
            Then $-x \in \floating_{64}$.
    \end{enumerate}
    A correct (resp.~incorrect) answer leads to +1 mark (resp. -1 mark).
\end{question}

\newpage
\begin{question}
    [8 marks]
    Assume that $\mat A \in \real^{n \times n}$ is an invertible matrix
    and that $\vect b \in \real^n$ and~$\vect \beta \in \real^n$ are two nonzero vectors in $\real^n$.
    We denote by $\vect x$ and $\vect \xi$ the solutions to the linear equations~$\mat A \vect x = \vect b$ and $\mat A \vect \xi = \vect \beta$,
    respectively.
    Show that
    \[
        \frac{\norm{\vect x - \vect \xi}}{\norm{\vect x}} \leq \norm{\mat A} \norm{\mat A^{-1}} \frac{\norm{\vect b - \vect \beta}}{\norm{\vect b}}.
    \]
    Here $\norm{\placeholder}$ denotes both the Euclidean vector norm and the induced matrix norm.

    \noindent \textbf{Bonus question} (1 mark): Let $\kappa := \norm{\mat A} \norm{\mat A^{-1}}$. Prove that $\kappa \geq 1$ .
\end{question}

\newpage
\begin{question}
    [8 marks]
    Let $\mat A \in \real^{n \times n}$ be a symmetric positive definite matrix and let~$\vect b \in \real^{n}$.
    The steepest descent algorithm for solving $\mat A \vect x = \vect b$ is given hereafter:

    \begin{center}
    \begin{algorithmic}
    \State Pick $\varepsilon > 0$ and initial $\vect x$%
    \State $\vect r \gets \mat A \vect x - \vect b$%
    \While{$\norm{\vect r} \geq \varepsilon \norm{\vect b}$}
        \State $\omega \gets \vect r^\t \vect r/\vect r^\t \mat A \vect r$
        \State $\vect x \gets \vect x - \omega \vect r$
        \State $\vect r \gets \mat A \vect x - \vect b$
    \EndWhile
    \end{algorithmic}
    \end{center}

    \noindent
    \begin{itemize}
        \item
            Why is this method called the \emph{steepest descent} algorithm? (1 mark)

        \item
            How many floating point operations does an iteration of this algorithm require? (5~marks)

        \item Are the following statements true of false? (2 marks)
        \begin{enumerate}
            \item
                There exists a unique solution~$\vect x_*$ to the linear system~\( \mat A \vect x = \vect b \).

            \item
                The iterates converge to $\vect x_*$ in at most $n$ iterations.

            \item
                We consider the following modification of the algorithm:
                \begin{center}
                \begin{algorithmic}
                \State Pick $\varepsilon > 0$, $\omega > 0$ and initial $\vect x$%
                \State $\vect r \gets \mat A \vect x - \vect b$%
                \While{$\norm{\vect r} \geq \varepsilon \norm{\vect b}$}
                    \State $\vect x \gets \vect x - \omega \vect r$
                    \State $\vect r \gets \mat A \vect x - \vect b$
                \EndWhile
                \end{algorithmic}
                \end{center}
                If $\omega$ is sufficiently small, then this algorithm converges.

            \item
                Here we no longer assume that $\mat A$ is positive definite.
                Instead, we consider that
                \[
                    \mat A =
                    \begin{pmatrix}
                        -1 & 0 \\ 0 & -2
                    \end{pmatrix}.
                \]
                In this case, the steepest descent algorithm is convergent for any initial $\vect x$.
        \end{enumerate}
    \end{itemize}
\end{question}

\newpage
\begin{question}
    [6 marks]
    We proved in class the quadratic convergence of the Newton--Raphson method for a smooth function with a simple root.
    The aim of this exercise is to study the convergence of the method in the case of a function with a double root.
    To this end, we consider the simple one-dimensional equation
    \begin{equation}
        \label{eq:nonlinear_equation}
        f(x) := (x-1)^2 = 0.
    \end{equation}
    \begin{enumerate}
        \item
            Write down one iteration of the Newton--Raphson method for~\eqref{eq:nonlinear_equation} in the form:
            \[
                x_{k+1} = F(x_k).
            \]

        \item
            Let $e_k = x_k - x_*$, where $x_*$ is the exact solution to~\eqref{eq:nonlinear_equation}.
            Find a recurrence relation for the error and,
            assuming that the initial guess is $x_0 = 2$,
            write down an explicit expression for $e_k$.

        \item
            What is the order of convergence of the method in this case?

        \item
            \textbf{Bonus question} (1 mark):
            Repeat the previous exercises for the equation $(x-1)^3 = 0$.
            What is the order of convergence in this case,
            and what is the rate of convergence?
    \end{enumerate}
\end{question}

\end{document}
