\documentclass[11pt]{article}
\usepackage{setspace}
\onehalfspacing
\usepackage[outputdir=build,newfloat]{minted}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\theoremstyle{definition}
\newtheorem{question}{Question}
\input{../macros.tex}
\begin{document}

\title{Numerical Analysis: Practice Midterm (30 marks)}
\author{Urbain Vaes}
\maketitle

\begin{question}
    [8 marks]
    True or false?
    \begin{enumerate}

        \item
            Let $(\placeholder)_2$ denote binary representation.
            It holds that
            \(
                (0.1011)_2 + (0.0101)_2 = 1.
            \)
        \item It holds that
            \(
                (1000)_3 \times (0.002)_3 = 2.
            \)

        \item
            A natural number with binary representation $(b_4 b_3 b_2 b_1 b_0)_2$ is even if and only if $b_0 = 0$.

        \item
            In Julia, \julia{Float64(.4) == Float32(.4)} evaluates to \julia{true}.

        \item
            Let $(\placeholder)_3$ denote base 3 representation.
            It holds that $(0.\overline{2200})_3 = (0.9)_{10}$.

        \item
            Machine addition~$\madd$ is a commutative operation.
            More precisely, given any two double-precision floating point numbers $x \in \floating_{64}$ and $y \in \floating_{64}$,
            it holds that
            \(
                x \madd y = y \madd x.
            \)

        \item
            Let $\floating_{32}$ and $\floating_{64}$ denote respectively the sets of single and double precision floating point numbers.
            It holds that $\floating_{32} \subset \floating_{64}$.

        \item
            The machine epsilon of a floating point format is the smallest strictly positive number that $\emph{(i)}$ is a power of 2 and \emph{(ii)} can be represented exactly in the format.

        \item
            Let $\floating_{64}$ denote the set of double precision floating point numbers.
            For any $x \in \real$ such that~$x \in \floating_{64}$,
            it holds that $x + 1 \in \floating_{64}$.

        \item
            Let $f\colon \real \to \real$ denote the function that maps $x \in \real$
            to the number of double precision floating point numbers contained in the interval $[x-1, x+1]$.
            Then $f$ is a decreasing function of $x$.

        \item
            Let $n \in \nat$.
            The number of bits in the binary representation of $n$ is less than or equal to 4 times the number of digits in the decimal representation of~$n$.

        % \item
        %     Let $a_i \in \{0, 1\}$ for $i \in \{1, 2, 3\}$.
        %     If $(a_1 a_2 a_3)_2$ is a multiple of 3,
        %     then $(a_1 a_2 a_3)_4$ is a multiple of 6.

        % \item
        %     Let~$\madd$ and~$\msub$ denote machine addition and machine subtraction in the $\floating_{64}$ format.
        %     The result of $(10^{10} \madd 0.1) \msub 10^{10}$ is closer to 0.1 than $0.1 \madd (10^{10} \msub 10^{10})$.
    \end{enumerate}
    A correct (resp.~incorrect) answer leads to +1 mark (resp. -1 mark).
\end{question}

\newpage
\begin{question}
    [Interpolation and approximation, 10 marks]
    Throughout this exercise, we assume
    that $x_0 < \dotsc < x_n$ are distinct values and
    that $u \colon \real \to \real$ is a smooth function.

    \begin{enumerate}
        \item 
            \marks{3}
            Are the following statements true or false?
            \begin{itemize}
                \item
                    There exists a unique polynomial~$p$ of degree less than or equal $n$ such that
                    \begin{equation}
                        \label{eq:interpolation}
                        \forall i \in \{0, \dotsc, n\}, \qquad
                        p(x_i) = u (x_i).
                    \end{equation}

                \item
                    Assume that $p \in \poly(n)$ is such that~\eqref{eq:interpolation} is satisfied.
                    Then there is a constant~$K \in \real$ independent of~$x$ such that
                    \[
                        \forall x \in \real,
                        \qquad u(x) - p(x) = K (x - x_0) \dotsc (x - x_n).
                    \]

                \item
                    Assume that $p \in \poly(n)$ is such that~\eqref{eq:interpolation} is satisfied.
                    Then $p$ is necessarily of degree~$n$.
            \end{itemize}
    \item
        For $i \in \{0, \dotsc, n\}$, 
        let $u_i = u(x_i)$,
        and let $m \leq n$ be a given natural number.
        We wish to fit the data $(x_0, u_0), \dotsc, (x_n, u_n)$ with a function $\widehat u\colon \real \to \real$ of the form
        \[
            \widehat u(x) = \alpha_0 + \alpha_1 x + \dotsc + \alpha_m x^m.
        \]
        Specifically, we wish to find the coefficients $\vect \alpha = (\alpha_0, \dotsc, \alpha_m)^\t$
        such that the error
        \[
            J(\vect \alpha) := \frac{1}{2}\sum_{i=0}^{n} \abs[big]{u_i - \widehat u(x_i)}^2
        \]
        is minimized.
        Throughout this exercise,
        we use the notations
        \[
            \mat A
            \begin{pmatrix}
                1 & x_0 & \hdots & x_0^m \\
                \vdots & \vdots & & \vdots \\
                1 & x_{n} & \hdots & x_n^m
            \end{pmatrix},
            \qquad
            \vect b :=
            \begin{pmatrix}
                u_0 \\
                \vdots \\
                u_n
            \end{pmatrix}
        \]
        \begin{itemize}
            \item
            \marks{3}
                Show that $J(\vect \alpha)$ may be rewritten as
                \[
                    J(\vect \alpha) = \frac{1}{2} (\mat A \vect \alpha  - \vect b)^\t (\mat A \vect \alpha  - \vect b).
                \]

            \item
            \marks{4}
                Prove that if $\vect \alpha_* \in \real^{m+1}$ is a minimizer of $J$,
                then
                \[
                    \mat A^\t \mat A \vect \alpha_* = \mat A^\t \vect b.
                \]

            \item
                \mark
                Show that the matrix $\mat A^\t \mat A$ is positive definite.
                You can take for granted that the columns of~$\mat A$ are linearly independent.
        \end{itemize}
\end{enumerate}

\end{question}

% \newpage
% \begin{question}
%     [8 marks]
%     Assume that $\mat A \in \real^{n \times n}$ is an invertible matrix
%     and that $\vect b \in \real^n$ and~$\vect \beta \in \real^n$ are two nonzero vectors in $\real^n$.
%     We denote by $\vect x$ and $\vect \xi$ the solutions to the linear equations~$\mat A \vect x = \vect b$ and $\mat A \vect \xi = \vect \beta$,
%     respectively.
%     Show that
%     \[
%         \frac{\norm{\vect x - \vect \xi}}{\norm{\vect x}} \leq \norm{\mat A} \norm{\mat A^{-1}} \frac{\norm{\vect b - \vect \beta}}{\norm{\vect b}}.
%     \]
%     Here $\norm{\placeholder}$ denotes both the Euclidean vector norm and the induced matrix norm.

%     \noindent \textbf{Bonus question} (1 mark): Let $\kappa := \norm{\mat A} \norm{\mat A^{-1}}$. Prove that $\kappa \geq 1$ .
% \end{question}

% \newpage
% \begin{question}
%     [8 marks]
%     Let $\mat A \in \real^{n \times n}$ be a symmetric positive definite matrix and let~$\vect b \in \real^{n}$.
%     The steepest descent algorithm for solving $\mat A \vect x = \vect b$ is given hereafter:

%     \begin{center}
%     \begin{algorithmic}
%     \State Pick $\varepsilon > 0$ and initial $\vect x$%
%     \State $\vect r \gets \mat A \vect x - \vect b$%
%     \While{$\norm{\vect r} \geq \varepsilon \norm{\vect b}$}
%         \State $\omega \gets \vect r^\t \vect r/\vect r^\t \mat A \vect r$
%         \State $\vect x \gets \vect x - \omega \vect r$
%         \State $\vect r \gets \mat A \vect x - \vect b$
%     \EndWhile
%     \end{algorithmic}
%     \end{center}

%     \noindent
%     \begin{itemize}
%         \item
%             Why is this method called the \emph{steepest descent} algorithm? (1 mark)

%         \item
%             How many floating point operations does an iteration of this algorithm require? (5~marks)

%         \item Are the following statements true of false? (2 marks)
%         \begin{enumerate}
%             \item
%                 There exists a unique solution~$\vect x_*$ to the linear system~\( \mat A \vect x = \vect b \).

%             \item
%                 The iterates converge to $\vect x_*$ in at most $n$ iterations.

%             \item
%                 We consider the following modification of the algorithm:
%                 \begin{center}
%                 \begin{algorithmic}
%                 \State Pick $\varepsilon > 0$, $\omega > 0$ and initial $\vect x$%
%                 \State $\vect r \gets \mat A \vect x - \vect b$%
%                 \While{$\norm{\vect r} \geq \varepsilon \norm{\vect b}$}
%                     \State $\vect x \gets \vect x - \omega \vect r$
%                     \State $\vect r \gets \mat A \vect x - \vect b$
%                 \EndWhile
%                 \end{algorithmic}
%                 \end{center}
%                 If $\omega$ is sufficiently small, then this algorithm converges.

%             \item
%                 Here we no longer assume that $\mat A$ is positive definite.
%                 Instead, we consider that
%                 \[
%                     \mat A =
%                     \begin{pmatrix}
%                         -1 & 0 \\ 0 & -2
%                     \end{pmatrix}.
%                 \]
%                 In this case, the steepest descent algorithm is convergent for any initial $\vect x$.
%         \end{enumerate}
%     \end{itemize}
% \end{question}

% \newpage
% \begin{question}
%     [6 marks]
%     We proved in class the quadratic convergence of the Newton--Raphson method for a smooth function with a simple root.
%     The aim of this exercise is to study the convergence of the method in the case of a function with a double root.
%     To this end, we consider the simple one-dimensional equation
%     \begin{equation}
%         \label{eq:nonlinear_equation}
%         f(x) := (x-1)^2 = 0.
%     \end{equation}
%     \begin{enumerate}
%         \item
%             Write down one iteration of the Newton--Raphson method for~\eqref{eq:nonlinear_equation} in the form:
%             \[
%                 x_{k+1} = F(x_k).
%             \]

%         \item
%             Let $e_k = x_k - x_*$, where $x_*$ is the exact solution to~\eqref{eq:nonlinear_equation}.
%             Find a recurrence relation for the error and,
%             assuming that the initial guess is $x_0 = 2$,
%             write down an explicit expression for $e_k$.

%         \item
%             What is the order of convergence of the method in this case?

%         \item
%             \textbf{Bonus question} (1 mark):
%             Repeat the previous exercises for the equation $(x-1)^3 = 0$.
%             What is the order of convergence in this case,
%             and what is the rate of convergence?
%     \end{enumerate}
% \end{question}

\begin{question}
    [Numerical integration]
\end{question}

\newpage
\begin{question}
    [Vector and matrix norms, 6 marks]
    The 1-norm and the $\infty$-norm of a vector~$\vect x \in \real^n$ are defined as follows:
    \[
        \norm{\vect x}_1 = \lvert x_1 \rvert + \dotsb + \lvert x_n \rvert
        \qquad \text{ and } \qquad
        \norm{\vect x}_{\infty} = \max \Bigl\{ \lvert x_1, \dotsc, \lvert x_n \rvert \Bigr\}.
    \]
    These norms both induce a matrix norm through the formula
    \[
        \norm{\mat A}_p
        := \sup \Bigl\{ \norm{\mat A \vect x}_p : \norm{\vect x}_p = 1 \Bigr\}.
    \]
    Prove that, for $\mat A \in \real^{n \times n}$,
    \begin{itemize}
        \item
            \marks{6}
            $\norm{\mat A}_1$ is given by the maximum absolute column sum:
            \begin{equation}
                \label{eq:matrix_1_norm}
               \norm{\mat A}_1 = \max_{1 \leq j \leq n}  \sum_{i=1}^{n} \abs{a_{ij}}.
            \end{equation}

        \item
            \mark
            $\norm{\mat A}_{\infty}$ is given by the maximum absolute row sum:
            \[
                \norm{\mat A}_{\infty} = \max_{1 \leq i \leq n}  \sum_{j=1}^{n} \abs{a_{ij}}.
            \]
    \end{itemize}

    \textbf{Hint:} In order to prove~\eqref{eq:matrix_1_norm},
    you may find it useful to proceed as follows:
    \begin{itemize}
        \item
            Introduce $j_*$ as the index of the column with maximum absolute sum:
            \[
                j_* = \argmax_{1 \leq j \leq n}  \sum_{i=1}^{n} \abs{a_{ij}}.
            \]
        \item
            Prove the direction $\geq$ in~\eqref{eq:matrix_1_norm} by finding a vector $\vect x$ with $\norm{\vect x}_1 = 1$ such that
            \[
                \norm{\mat A \vect x}_1 =  \sum_{i=1}^{n} \abs{a_{ij_*}}.
            \]

        \item
            Prove the direction $\leq$ in~\eqref{eq:matrix_1_norm} by showing that,
            for a general $\vect x \in \real^n$ with $\norm{\vect x}_1 = 1$,
            \[
               \norm{\mat A \vect x} \leq  \sum_{i=1}^{n} \abs{a_{ij_*}}.
           \]
    \end{itemize}
\end{question}
\end{document}
